Introduction by Enrico Ronchi

So I think we can start, because it's sharp at my clock. So welcome everyone. My name is Enrico Ronchi and I am an associate professor at University Sweden and I'm here to introduce you to the sixth webinar of our human behavior in fire webinar series arranged by the permanent group of the International Association for Fire Safety Science. So just a couple of information about our payment group. 
This group has been started along with the last ifss conference That was supposed to be in 2020 but then ended up in 2021 due to the pandemic, and we had a kickoff Workshop there, and this is coordinated by myself and also Dr Erica koligowski. Eric, I see you there, so maybe you can briefly into the use yourself. Hi, everyone, I'm glad that you could make it. I'm Erica kuligowski from rmit University. 
Thanks, Erica, and as mentioned. this is the sixth event of our webinar series and we have also other ongoing activities in the UMB bring fire domain. So if you want to be informed and join our permanent group, this is a free group. It's a basically a People Like Us researchers for teachers and so on that are just interested in this topic and we want to network and share opportunities to coordinate efforts. 
So I put this link here to join the group. I will also post it in the chat later on and, as mentioned, is free and you get updated about events like this and other activities that we do in the group. And, of course, if you are interested in contributing more in the activities of the group, you're welcome to reach out to myself and Erica, and let's go a bit into the topic of today, so today. 
We're going to talk about the common statistical mistake takes in the analysis of a vacation data and we have as a speaker Dr Nikolai border. Nikola is a senior lecturer in the engineering mathematics at the University of Bristol in the UK. His research investigates the nature and consequences of social interactions, combining theoretical approaches with a empirical experiment. 
So I guess we're both in theory and experiments regarding evocation and considering forging animal groups or evacuating pedestrian crowds. It's very interesting because I read a few papers from Nikolai looking at animal Dynamics. It's a completely different world, but it's very interesting, but we talk about humans today. A key focus of his work is to relay theoretical models for human or animal behavior to data. This involves a lot of Statistics. 
Manicolai does not consider himself to be a fault statistician, but he's currently also an editor. of the Open Access Journal Collective Dynamics, and I guess, Nikolai, you are very a humble, nice guy, but you are probably one of the most knowledgeable people in the world on a statistical analysis of a vacation data Now setting the high expectations for the audience. 
So I'm really glad to have you here, and I will give you the virtual floor for your nice talk. 


Presentation by Dr. Nikolai Bode

Thanks, Enrique hello. Everyone thanks for joining this early in the morning and thanks for the opportunity to talk about this topic. It's something that I really I'm really excited about, and I'm every talk about statistics. 
I'm always a bit nervous about that as well, because it's so easy to sort of put your foot into one of the many slightly problematic aspects or intricacies of doing statistical analyzes. Can you all see my screen before I start to talk? Yes, but. you have to put a full screen. We still see it. 
Let's say not, yeah, there you go perfect, thanks, okay, cool, so yeah, when I was asked to give this talk, we managed to decide on a fairly, I think, exciting title.common mistakes are sort of something where everyone thinks are. I'm going to go to this seminar and then I'll manage to avoid every sort of problem that comes up when analyzing data using statistics. 
We'll see if that is the case and I'd be interested to hear what you, what you think about this. But you've learned something or seen something that you weren't aware about, aware of? As I go through this and there's something that doesn't make sense or you want to ask questions, go ahead. I'll try my very best and it's probably oh there's a chat. See if I can see this. 
Yes, cool, and I'll try to answer the questions, and if I don't answer them, then it's either because I don't know the answer, or B because I missed the question. So what is this all about? So when we think about investigating evacuations of people from buildings, we can think about all sorts of different types of data that we might record or have access to. 
So these are just some examples of the work that I've been involved with, and you can see anything from a virtual experiment in the bottom left to movement trajectories in the top left, where we get the movement Paths of individual people all the way up to you know these really big experiments that some of our colleagues do, where we may look at movement trajectories, but maybe we only look at the actual numbers of people rather than trajectories and. 
What we get out of this is data, and then we want to investigate this data and ask questions about this data, and one extremely flexible and popular way of doing this is to use statistics and statistic statistical analysis. So that's broadly the topic of what we're going to look at today. Now, whenever we start to do statistics, obviously things go wrong, and this isn't. 
This is nothing new, and it's happened a lot and in a lot of different fields. So what I'm not going to do is to go through the whole range of issues that come up or the sort of the theoretical foundations of statistics and what happens when things go wrong, and I just refer you to a few of the really excellent Publications that are out there here in this slide, and there's a really old one from the 70s. No, I. think it's even from the 50s. 
It's called how to lie with Statistics, and so that's, I think, an excellent read and very useful thing to know about. And then there's a there's a hundreds of papers on this and different aspects of it, specific to different research Fields, all specific to the underlying Theory and how to go about it. So I'm not going to go as broad as this. 
I'm going to focus this discussion very much on evacuation data, such as what we've seen in the previous slide. And the second thing this talk isn't is it is not a recipe book for statistic analysis, so it's not. I'm not going to tell you what kind of statistical analysis to use, when I don't necessarily I mean that would be first too broad for this, and I also don't think that's necessarily the most useful approach there. 
Is a very good book, though, which I'm recommending here, so sort of a recipe book for statistical analysis, and it's really it's got charts where you, where you look at what data you have, and then it sends you through a sort of selection algorithm of where you can find of what kind of statistical analysis might be appropriate for your data and for the questions you might want to ask, and it's a very good book. 
The author always came to seminars that I gave during my PhD and he asked very difficult questions. So I expected so it's very good. So there's two things to talk is not yeah, not very general and not a recipe book for statistic analysis. Then the third disclaimer I should sort of make before starting this talk is that I'm not a trained statistician. Enrique obviously sort of highly recommended me. 
But he would do if he organizes the seminar and invites someone to talk about the topic. And as you will see as I go through this that I sort of talk about my own mistakes, like later on as well, and I think that's important, because I don't want to give the impression that I'm sort of all knowing and I always get it right. 
I think it's a learning process and for everyone, and it's good to just have a discussion about it. So that's how we should see this, not so much as a webinar where I tell you what's what, but more to start or the continuation of the discussion. So what am I actually going to talk about then? So there's three sort of main topics that I'm going to go through. 
I'm going to start off with a broad, very broad overview of what. statistical analysis in the context of evacuation data is and the uses of statistical analysis and a very short literature survey and but that's going to be very short just the types of things people have done. 
Then the second main topic is really that's where the meat of this presentation is, and that's all about the typical steps in a statistical analysis of evacuation data and then the associated issues. So I think it's very useful to look at what we usually do when we go through a statistical analysis, and that allows us into pinpoint where things might be, where issues might commonly arise. 
And then I'd like to finish off with a discussion of best practice, and that's really why I'm going to go through some of my papers, not in detail, but I'm going to say: well, I could have done better here, and hopefully that's. 
That's a useful thing to do, okay, but let's start off with them, looking at a general definition of what we're talking about and just checking the chat in case there's something cool and how people have used it before. Okay so I'd like to start with two sort of and I wouldn't call them definitions. I should also say this talk is not. I've managed to avoid putting equations into this talk. 
So, and I'm a trained mathematician, so I'm pretty pleased with that and that's deliberate, because I think yeah we can get lost in detail. But so statistical analysis can be thought of as the attempt to make quantitative statements about data using probability Theory. So really that's you know one way of looking at statistical analyzes. In this very short way of summarizing it, we want to say something in terms of numbers about our data. 
And we're going to use probability theory for that. So there's going to be uncertainties involved, so we're not debt certain about what's going on. Then a second related term is statistical models and statistical models. They're essentially an attempt to describe or represent data in terms of probability distributions. So these two terms are linked. 
And the first point I really want to make that regardless of whether we're talking about statistical analysis or statistical models, we always have to make assumptions. So we always make I will go through some examples in a second but we always make assumptions about what's going on in our data or the appropriateness of our model, and so on. 
So the first video, the first point I wanted to make really is that we could make an argument for saying that all statistical analysis is done via statistical models, and I find. this a useful perspective. 
So essentially what I'm saying here now is that whenever I do a statistical analysis and it can be, you know, the most sort of standard statistical test that you can run on your data automatically using almost any software nowadays, you know, let's say, a t test. 
I think most of you are familiar with those, and I would say this could be viewed as an example of using a statistical model for analyzing data, and the reason for why I think this is a useful perspective is because, certainly in my mind, as soon as we start to talk about modeling data, we very much focus on the assumptions that we're making or in creating this model, and I think that's a good thing to have at the back of your mind when you do a statistical analysis of data of evacuation data, so. 
Let's look at some examples. So let's left hand side. Let's assume we conduct evacuation drills under two conditions, A and B, and we just measure the total egress time. Okay, and that's sort of a very common scenario that we might want to look at. I don't know. Maybe we've changed the signage a bit or maybe you know the instructions people receiving are different. 
We want to see which condition is more suitable for fast egress, and this very much looks like something that we can analyze using a very typical statistical test, let's say a t test. For now, as soon as we go down this route, we're making assumptions. 
So a t test, just for argument's sake, assumes that the distribution, the probability distribution, of these egress times is a particular type of distribution, and we also assume that each of the measurements are independent. so they're not related to each other at all. 
So, in effect, what we're saying here then, to go back to this statistical model example, is that we model this egress data using a particular type of distribution, and that also makes assumptions about the variance in the different conditions. So that's a very simple example, but we can already phrase this as using statistical models. So if we move on to the middle here, we've got an example where there's some. 
There's some people inside a room with four exits and they have to choose which exit they use. It's a very common example again in evacuation research, and let's say we're focusing on the gray individual marked with a star, and we want to come up with a way of analyzing what factors are most influential for the decision of that person, so for which exit to? use, and so again we can use a fairly standard statistical analysis. 
For that I've written it underneath, but the details don't matter so much. Essentially, what we're starting to do now is, instead of coming up with a model just for data, we're starting to come up with a model for the behavior, so for the decision making of this person, and in statistical modeling we would Express this in terms of probabilities, so we might have a probability distribution over the exits. 
Over the four exits might give us four probabilities, and that then allows us to analyze the data. Again, we're starting to make assumptions, so there's very obvious assumptions. There's the structure of this model for the probabilities. So what goes into calculating these probabilities have to be taken account of all the important factors, but they're also statistically more important assumptions, namely this concept of. 
Conditional Independence, which again I don't want to go into too much detail, but essentially, if we if we did this analysis for every individual in this room, then it may be that we need to take account of the relationship between those individuals and how their decisions relate to each other or relate to the decisions of others. 
I'll revisit that later in the talk and then, on the right hand side, we can obviously that's an example for a much more involved analysis. So instead of just modeling discrete decisions, we may want to model the actual movement path of people in two dimensions in continuous space, and we might come up with them. 
Probability distribution is shown in this heat map that shows us the likelihood of where this person might walk next over, say, a time step, and these models start to get very involved and. they're not that common and we have to take all sorts of shortcuts. 
That's a slightly different discussion again, but you can see that with this sort of concept of statistical modeling, we can cover a very broad range of different types of data and different types of contexts, and in all of these cases we can think of it as coming up with a statistical model or for the data rather than just a standard statistic analysis. 
So really what I wanted to do with this is sort of broaden the concept of statistical analysis. This is not just about, you know, picking an existing technique from a whole host or a whole array of different techniques. It can be a very creative process of coming up with a new model that captures, you know, a very complicated multi dimensional probability distribution that might even change over time okay. 
So I would think of statistical analysis as a very broad concept. Okay, so how do we then use this really broad concept of statistical analysis? There's three main ways of using this, so the most common one is what I would refer to as inference. This is all about learning something about data, and this can be things like comparing the average regress time over different conditions. 
But it can also be something like if you have a microscopic model for the movement of individual evacuees, it can be something like fitting this model to data and then saying something about the fit of the model to the data. So this is this is all inference. 
It can also be comparing models, so if you have two microscopic models for the movement of evacuees, you can use a statistical analysis to compare the relative merits or relative fit to. data of these models. That's all inference. 
Then the next big use is making predictions, so saying something about things that have not or not yet been observed, and in my mind really that's the sort of the Holy Grail of this kind of analysis we're trying to do. If we can do this, well, we can sort of predict the future, and that would be fantastic. 
It's very hard, and I think you know as well, or even better than me, how difficult it is to make very accurate predictions in our field of research. I guess the key point is that if we do predictions as part of a statistical analysis or using statistical models, this the concept of uncertainties, even within our predictions, is inherent. 
So, rather than making pinpoint predictions, you predict intervals of certain quantities or you predict things in quantities within a certain range. and these ranges are described by probability distributions. And then the third main use that, in my mind, is currently underused, but I think it's very useful and promising is model checking. 
Now, if we go back to this idea of fitting a model to data, so calibrating a model on data using a statistical model on data, we can use Concepts or we can use the assumptions we've made directly to explore if our model is appropriate or not. And there's a. There's a. There's a huge sort of established approach for doing this, and it can be very informative. 
I'll go back to an example for that later as well in the talk, but this is something that isn't done too frequently and I think it could be useful so broadly. This is how statistical modeling can be used now. How is it used most common is? inference. As I mentioned, model checking and predictions are rare because predictions are difficult and model checking I don't know. I think that'd be good to do. 
I think a lot of models aren't necessarily formulated as statistical models, even though they could be. But in terms of uses this four broad categories of how statistical analysis or statistical modeling I'm going to use these terms of interchangeably now how they use this. The use of sort of standard statistical tests that just compare variables across conditions, for example. 
Then a slight development from that would be looking at the relationship between multiple variables, and it's often known as regression. So those first two are obviously incredibly common. They're just data analysis techniques, but then points three and four, calibrating and comparing microscopic and microscopic models. That's something where we start to move into a sort of more a territory. 
That's more sort of specific to evacuation modeling, because we distinguished between these different types of models and that's very important to calibrate them and compare them, and I'm not going to go into too many details on all of this. There's people use many different methods. 
Instead, I'm going to refer you to a review that Enrico and I wrote a few years ago, and what you can find in there is it's not an exhaustive survey of the literature, but it's at least a survey that covers many different approaches and I think can be used for starting point for finding an example of an analysis that may be related to what you might want to do in your work. 
Okay, so it tells you not only what people did, but also what kind of statistic or model fitting approach they used, what use of Statistics was. it inference of model checking that was covered Etc. So there's quite a lot of information in that. It's hopefully a useful starting point. 
But, broadly speaking, statistical analyzes are used a lot in our field of research, and statistical models are also used, you know, in the sort of really broad way that I described earlier. So it's not just about doing a sort of what we might call the standard analysis of data, because there's more you can do and people do more than that as well. Okay. So now I've set the scene. 
We've talked about what statistical analysis or statistical modeling is. We've seen, or we've gained an intuition of what it is people do at the moment. So now let's talk about what a typical analysis looks like, are the steps in a typical analysis and then obviously the issues that arise. That's really. I guess what you're here for, so jumping ahead good. 
So what you can see here is a list of typical steps that I used in a statistical analysis, and the order of these steps is not necessarily representative of how this is done, and it's also very common that not all of these steps occur in a statistical analysis, and I'm not going to go through all of them in much detail. I'll very briefly describe them. 
But, as I said, before looking at these different steps I think is useful for then pinpointing where issues may arise. So typically things start with data collection, and data collection is not as easy as just. It's not necessarily just a question of the technicalities of recording this data. So I don't know setting up equipment for recording data. It's very often linked to experimental design and deciding what kind of. 
Questions we want to ask off the data. An experimental design is not limited to running a controlled evacuation drill experiment, let's say, but we can also think of designing an observational study where we put some equipment just out into the wild. We can think of that as designing an experiment, and it's always important to think about whether the design of the experiment allows us to answer the questions that we're asking of the data later on. 
Okay, then we might want to look at the date, but that's an exploratory data analysis. That 0,3 is then deciding on candidate models, and now this really is sort of my way of thinking. I suppose another wave phrasing this would be to say decide on the statistical analysis you want to do so essentially selecting an approach for quantifying the data in terms of a statistical model. and mostly in statistical model fitting. 
In statistical analysis we then have to fit our models, and even in a sort of more standard statistical analysis, we can think of certain steps as model fitting. So when we compare two quantities across conditions, we might want to fit distributions to the data from the different conditions, and that will be modern fitting. Sometimes we do model selection. So if you have multiple candidate models, that's more relevant, for I don't know things like aggression regression. 
If you're interested on in whether a variable has an effect or not, you might have multiple models where you include this variable or you do not include it, or if you're comparing, say, different microscopic or microscopic models, okay, then checking whether model assumptions hold this is a key step. I'll come back to it later again, but we talked about how every statistical. 
Model makes assumptions about the data, and every statistical analysis consequently also makes assumptions about the data. So it's they're really quite important to check whether the assumptions that we're making, whether they hold and in statistics, because of the nature of the assumptions we're making, we have a very clear way of checking model assumptions. 
I'll briefly mention them, as mentioned before, and then obviously you want to maybe use these models, interpret findings and then report models and results. Now, where do issues arise? It can start as early as data collection and that's a very common issue I think in our field of research and that I'm sure some of you are aware of as well and then model assumptions. That's another really big issue in my mind. 
And then the other issue is interpreting findings and reporting of models and results. That they're important, but I. think they're more similar to things that are happening elsewhere in other research Fields as well. But I think certainly for data collection and checking model assumptions, there are issues that are very specific to our field of research into the nature of research that we do and that I'd like to share and talk about in the following. 
Now so but let's start with issues and data collection. So I think in our research, one of the sort of what people think of as almost the best type of data is very realistic evacuation drills and with many volunteers. Ideally if we're interested in evacuations of crowds of people. And now the problem with these kinds of evacuation drills is that they're very difficult to organize. 
You have to get lots of people into one place at the same time and often they're very expensive if you pay people. and as a result it's often not possible to do a lot of replicas. So to repeat these experiments or drills many times and what that can result in and it I think it does result in this is that we end up with underpowered studies. 
So what I mean by that is that we have studies that set out to answer a certain question, conduct some experiments, but statistically speaking, there isn't enough data to definitively say whether to give a definitive answer for the question that was proposed, and a little example for that is some something that I did recently with them young, and we essentially looked at a very simple question, namely whether the presence of social groups like friends or family, whether they increase or decrease the egress times of crowds of people, and so the figure here is some is from that paper. 
And all you need to know really that is that the x axis shows you a standardized effect and the vertical black line is the Zero Effect line, so that the social groups have no effect, and you can see that. There are six studies. 
There's not a lot of studies, but Six studies we investigated, and most of them, the sort of error bars around the gray squares, overlap with Zero Effect line, even though the gray squares are not necessarily on the Zero Effect line. So there are lots of issues with this study. 
But what this illustrates is that the is that the error bars are too big still to make a clear statement about what's happening, and that could be for various reasons. But it one possibility is that the studies are underpowered, so there aren't enough replicas in these studies. So, and that's I that. I believe that's something that very commonly occurs in our field of research. 
I don't think that necessarily means that we shouldn't do these studies, because we all appreciate that the data is still very useful. Obviously it's possible to ask multiple questions of the kind of data you get from these evacuation drills, but I think it's important to keep in mind and to be open about this possibility. So an alternative of getting around this issue of expensive evacuation drills is just to simulate data. 
So use computer simulations to investigate certain scenarios, and so this is obviously a bit tangential to this question of using a statistical analysis on evacuation data, because we all appreciate that. You know the simulation models themselves could be wrong, but I thought I'd put this in here because I've seen it a few times from reviewing papers and I think. well, yeah, it's good to know I'll get to talk about. 
So we often use simulations like the one shown on the right to investigate situations. Let's stick with the same example that I used before, where we have crowds with or without social groups within the crowds, and let's say we're going to simulate that using a model and what happens every now and then is that people then use a statistical test to compare the crowd egress times across these two different conditions. 
And now my, in my mind, a statistical analysis of computer simulated data is not is almost not necessary, and the reason for this is that most all statistic analysis and the answers you get from the statistical analysis depend on the amount of data that goes into the analysis, and with a simulation model it's always possible to simulate more data. 
So, in my mind, as soon as you observe a true effect or difference across conditions, it is not necessary to run a statistical test on simulated data, because any outcome of your analysis, be it BP values or confidence intervals, you could, you know, adjust arbitrarily by increasing the number of simulations you have. 
I can see some comments, okay, so there's a question on the previous slide on underpowered studies, because underpowered studies mean the level of variance is just too high to identify Trends or patterns in the data. That I not. That's not it. I mean that can be the case that you know It's a combination of both. 
I mean if you can have a very high variance, but as soon as you have, if you have enough data points, you can still detect patterns. So in some ways it's the level between the variability. in the in your data and the number of data points you have. Yeah, so in some ways, in some ways, yes, yes, power analysis is a good idea for this. 
There's another question, then, to say that it's good to run a power analysis first when designing a study. So so yes, I okay, so I that's something I want to comment on as well. So one of the big problems I have with running a power analysis, and that's why I have a lot of sympathy for underpowered studies in our field of research is that it's hard to know. 
So many approaches in power analyzes require knowledge of the expected effect size. So what kind of effect you're expecting to see and you can obviously make some assumptions. But for some you know very exploratory studies we simply don't know. So then I find it. difficult to do power analysis beforehand. But in principle, yes, I think power analyzes very good idea, Nikolai. I suggests we go on and then we take the questions at the end. 
Okay, we'll do yep times moving on, okay. So then the really probably the two big issues in model assumptions that we come across is that in the kind of data we get from aggress experiments, we have temporal and spatial dependencies. So if we think about people say, let's say walking together, obviously their movement depends on the movement of others and it depends on their own movement and it may change over time. 
So I don't know someone might get tired over time as they walk along evacuation route, and all of these things can cause Havoc with the Assumption. Statistical models make so many statistical models assume that people do not depend on each. other or that the model we use explains all different, all relation relationships between people, and that's a quite a broad and big assumption that one would have to check. So I'll quickly run through some examples. 
I'll focus on the first two because the third one we talked about before. So imagine we want to look at the effect on of age on the egress time in data from five different evacuation drills. Whether this is a sensible thing or not, from five evacuation Trails is obviously an open question. But if we plot the data as shown in this figure, you can immediately see that there may be issues. 
So it may be that the different evacuation runs or aggress experiments that we've conducted that they may differ in themselves, so I don't know if we use the same group of people over time they may end up. being more tired or maybe older people might Tire more quickly and that might influence our results. So we need to take these sorts of things into account when analyzing the data. 
And then I already talked about the example in the middle. If we think about the decisions for different exits, if we use a statistical model for describing the decisions of all individuals in this room, we need to make sure that this model captures the dependencies of the decisions of different people, and that can be. That can be tricky and the last example I'll skip at the interest of time. So issues in interpretation of findings. 
I don't want to talk about too much. I'll just put this slide up for you to look at. It's something that's common across different fields, so it's not really specific to evacuation research, but yeah obviously P values that's. important to interpret those correctly, and reporting of results is also something where we can go wrong. 
So in the left hand plot if we report the proportion, say, of people who evacuate on time or in time, it's good practice, I would say, to also give some sort of indication of how certain we can be about these measurements, even for proportions, for a fixed experiment it can be either error bus or providing the sample size or number of people used, and then something that happens very common commonly and something that I do very often as well is that we don't necessarily report results of model checking. 
So the table shown on the right hand side is actually from one of my papers from a few years ago, and this is very commonly how we report the results from a regression analysis and we don't talk. about whether the regression model or we don't talk in detail about how the regression model, where the assumptions of that model hold and how we can demonstrate that. 
Okay, so the last topic I then wanted to talk about is just sort of easing us into a discussion of best practice. So what can we do to make sure these issues, or any issues on big issues it's obviously it's good to try to design experiments with sufficient statistical power. 
So here's an example of an experiment where we looked at the effect of these social groups and we only had four experimental runs, and at least I mean to my credit, I admitted in the paper that wasn't enough. But yeah it's good to do power analysis or to think about these things at least and think about how the data is being used, and obviously I'd. 
Be interested in your views on whether underpowered studies should not be published at all. My view is that it can still be useful to have them studied, to have them published, because any data helps in knowledge generation. But we can discuss that later model checking. 
So now model checking is really the statistical approach for assessing whether the assumptions of the models that we've used but they hold, and I think it would be good to report this more. 
So table at the bottom left, like I said, this is an example of a paper where clearly didn't do this and picked on the top right, is an example where I did do it and I wanted to show the picture on the top right, because it's an example for where we can actually learn something about what's going on in our data or where our model is. going wrong from looking at the residuals. So very briefly. 
What we looked at in this analysis was that time gaps between people leaving a very narrow body neck, and we came up with a model for describing these time gaps, dependent on what was going around going on around people as they were leaving through the bottleneck, and we fitted one model and then looked at the residuals of this model. So you can think of that as the error between the model fit and. 
The observed data, and we just plotted those residuals over time. And what the red line indicates is that there is a sort of temporal pattern. Yeah, and the gray vertical bars separate different sort of experimental runs, so that's not so important. But for each experimental run you can see that procedures start low, then they increase and then they dip down again. 
And that's clearly something that shouldn't happen and that shows that there is something in our model that is not quite right, that we don't quite capture in our model. So it can be really useful thing to look at these residuals and we can actually learn something about our data or our models from doing that. 
And then I think the last point I wanted to make really about discussing best practices is that I think it's good to just admit room for improvement. So this is from my most cited paper on human behavior, on evacuation behavior, and if I look at it now, I'm a bit embarrassed about the statistical analysis I did, and I could have done a much more compact analysis and I could have reported model checking results and I could have avoided running as many statistical tests, which also raises issues. So I. 
Think it's good to sort of look at your own work and say: well, yeah, it could be better, and I think it's also okay to write that in papers. You know if model checking results aren't perfect. When I'm working with data. Things Are not typically like they are in textbooks. Okay, some brief discussion points. 


Open discussion

I mean the first one, I think I don't need to talk about. Hopefully we all agree that statistical modeling is useful. 
The question of why model checking results are underreported in pedestrian Dynamics or evacuation Dynamics. I think that's sort of an interesting one and I'd be interested in your views on that. My thinking is that typically it's not sort of what we think of as interesting. So if the model taking results are okay, ish, then we're quite happy and we don't worry about them or we don't even know. 
About having to check our models, so we just ignore it. So it's very rare that you know we make it an explicit part of reporting on our results. Our new methods specifically developed for pedestrian Dynamics needed so that's something we discussed in a previous paper and I don't necessarily think so in the first instance. There are many and similarly there's no need for a unified framework. 
There are so many different types of data, as you've seen even in this short talk, and there's so many already existing approaches for analyzing different types of data that I don't think doesn't necessarily a need for developing a unified framework or new methods specifically for pedestrian Dynamics. There's also danger in sort of coming up with a unified framework analysis. 
It's similar to the different steps that I mentioned earlier in a statistical analysis, and I think that. danger is that if there is a very clearly prescribed approach, then there's a danger that we might just follow it blindly and miss other things. So my view is that we don't need this. 
So these are just some take home messages say you can look at those later again and that's some further reading in case you haven't seen enough, and with that thank you very much for listening. This is Bristol where I work, and I'm happy to answer any questions. Thanks a lot, Nikolai, for the interesting presentation. 
So, as mentioned, we have some time for questions, so you can either write them in the chat or, if you want, you can also ask to. Yeah, have your mic on and ask questions, and while these happen, I have a general question, Nikolai, because this is a topic on which I put a lot of thinking. 
So in general, how do you see, like our traditions in a vacation research compared to what we see in other fields? Because, for instance, I mean and I saw a couple of comments on this like, for instance, how to assess beforehand the power of the studies and so on. This is not really that commonplace in our. 
So there are certain practices that are very well established, for instance in fields like psychology or other fields, which is not really so common. So the feeling is that sometimes you see a lot of those papers in which people collect data and then try to give a sense out of it, rather than having a hypothesis or a theory that wants to try to validate and so on, which is not necessarily bad. 
But I want to hear your thoughts about this. So where we should go and where. are we when we come to a vacation data? Yeah, thanks, that's one of those questions where I can really offend people. I think one way of thinking about it is that it feels like psychology. 
They're very mature fields of Empirical research, so people have been doing experiments on human behavior I don't know for a long time, much longer than in, let's say, evacuation research, and the volume of research has been much higher and as a result the sort of statistical training and the procedures that are required are very well established and it goes beyond just statistical analysis. 
It also it's also related to, let's say, good ethics principles in designing new experiments. So I know, certainly in our institution, that the ethics reviews for new experiments in Psychology are they're a lot tougher than in engineering. So I think in that in that. sense other fields of research are maybe just a bit more mature and older than our field of research, and I think over time. 
So in some ways maybe what some of the research in our community is doing is still a bit more exploratory. So I can imagine over time people may some it depends on what happens, but it could be seen as more and more of an issue. So some of you may have heard about the reproducibility crisis in Psychology. That's something I mentioned in, I think, the second slide. 
So maybe at some stage people will get a sense that could be an issue in our field as well, and only after that. I think people will then seriously start to change the or come up with more systematic approaches. But yeah, I think one way of describing our field of research is that. 
It's experimental psychology to some extent, and even with a very systematic approach, you cannot avoid some of those issues, I think even in experimental psychology where it's more developed. So I think I'm sort of I think some more statistical training could be good. So people are aware of some of these issues. But I think over time this might happen naturally or we will become more for necessity. Sort of answer your question without defending it. 
Yeah, I think this is a fair point, Nicola. I mean the follow up is that in certain sub domains of evacuation probably we have really no whatsoever data, very little data. So any data is data. So it's better than no data, even if there are issues with the statistical analysis or maybe the research hypothesis is not the best it could have been. Still, when the alternative is nothing it's. hard to argue against something versus nothing. 
Yeah, I think. I think my view is that actually what's much more important than you know making sure everyone follows a very rigorous, you know protocol in their statistic analysis is to make sure that everyone publishes their data, because that then makes it possible to revisit some of the things that have been investigated and check you know how they match up across studies and so on. So I think that's a lot more important. 
Yeah, I think sometimes the statistic analysis can give a sort of false sense of security and certainty, which it doesn't. It's just probabilities. This is a very good point and also to publish raw data. So like avoiding any first filtering or analysis before they get published. So put like the raw data that you collected, with all possible problems that you identified that described. in your work, but that's a good point. 
There is some comments in the chat. Let me see. Eric has commented also about publishing data. Guerta also said publishing data should include publishing code. Well, also, if you're doing certain types of analysis, maybe there are more sophisticated. That should include also how you've done it. I don't know. Get that you want to comment on this or you added the comment there? Yeah, I'm here. I just I'm doing two things this time. 
I have to forgive me because today is the start of semester here, so I was a bit late to put on to switch on my Minecraft. Yeah, I find that very often it would be easy maybe to really run something if people just publish their R code or python code or whatever they do for statistics with it, and that would be helpful. also to check things or to go through assumptions, and it's actually good practice. 
I mean everything's. I mean the code should in is data code is better and reveals how you've dealt with your with your actually original data, and I would really like to see it published. With the rest, it's a very good point. Get that. There is also comment in the chat from Lynn. I don't know. 
Lynn, if you want to comment yourself and add something means the journals and their viewers need to be more understanding.come to see viewers be high critical studies where the sample size be unavoidably small. I mean this is exactly what we were talking about. I mean if the alternative to a data set with small sample size is nothing, then better something than nothing. I don't know. Nikolai. Yeah, I think my comment on that would just. 
Be that I think it's always worth it to try studies, even if I try to publish studies, even if you know we think they're underpowered. I think what's that important is to clearly articulate why it's still necessary to publish this study and to not sort of try to claim. You know that we can derive very certain insights on the underpowered part of the study from the study. I think I think that's the important thing. 
Then at least everyone can make a sort of informed judgment on what's happening. But yeah, I mean I've certainly personally suggested that studies are published, even though I thought they were under and empowered. There is a comment also from my King's issue. We registered data collection studies before conducting them to improve quality of research. What are your views on this, Nikolai? So I think so this is. 
A practice that happens a lot more, I think, in the medical sciences and maybe in Psychology, where there are well very big and well funded studies or smaller studies as well, and they're pre registered. 
So before data collection starts, the protocol for how the data is going to be collected is published, and I guess part of the idea for that, for that is to what a force the researchers to stick to their methods, which is important, but B also to have the opportunity to publish something even like before you know the result, because negative publishing negative results is harder foreign, should this be necessary? 
I'm sort of a bit torn on this, because I'm also a fan of exploratory data or opportunistic data collection. So you know you can, you can, you can think of you know someone taking you know spontaneously taking a. video of a situation which they then somehow managed to analyze at a later stage, and that can be very useful. Even the findings may be most, even though the findings may be mostly qualitative. 
But you know, forcing everyone to pre register studies? Yeah, not sure. I'm sort of a bit torn on this sort of from a sort of rigor of analysis point of view, that sounds like a good thing to do. Yes, oh yeah, there you go. Good, gotta get us comment sums it up perfectly. There is also team. Maybe you want to turn on your mic if you have a question there. 
Hello, sorry, I'm starting my camera slot. Yeah, hi, I'm okay, so I'm a fire engineer, also based in Bristol Netherlands. I'm just down the hill for you. We do a lot of evacuation modeling and one of the things that we've. been looking at recently is kind of using statistical data to try and determine the number of runs that we need to do. 
So if we do an evacuation model because of variants within that model, we might get the absolute worst case scenario. We might get an actually really optimistic scenario, and I think the iso standard that was issued a couple years ago, a year ago now, it requires you to kind of justify why you've done for 10 runs or 20 rounds or 50 runs. 
Have you captured who's the worst case boundaries, and we've spent quite a lot of time looking at it and we've never quite agreed on what is the best approach. So I don't know what the answer is, but it sounds like they're moving things you've been talking about today. There is there is a way to get that answer of where. does. 
At what point can you say: yes, I've run enough scenarios and run enough repeat runs to know that I've captured you know my 95th or my 99th percentile worst case. So yeah, I don't know. It's kind of a fairly open question, but so I think it's a really good question. I think I don't have a. You know there's not one really simple answer for that and you know obviously others can chip in as well. 
That you know as much as I do about simulating models or more. So I think you know one approach is to determine some kind of stopping Criterion. You know where you say you're interested in some measures and you look at how, with an increasing number of simulation runs, these measures and the variability around these matters converge and then, once you know that's reached some sort of stopping Criterion. and there are various qualitative or quantitative ones. 
You might say that's enough, or you know you could use a statistical approach for that too. You know some sort of power analysis that would be a predetermined stopping Criterion. I guess the statistical approach. The issue there goes back to this question of do you know what the what the effect size is and what the variance is? If you don't it sounds like you don't, then maybe something that is more qualitative might be might be better. 
Yeah, we've been going down that variance analysis through and trying to figure out where that variance then it converges. I don't know. It seems one of those things that further you look into it, the more questions you get. I think I think that's still an open sort of topic of research. I think in a lot of research papers. 
People sort of just choose a number and that's kind of yeah, it depends on you know how much time they have and how quickly the simulation runs and so on, and how big the variance is as well. I mean yeah, okay, I mean yeah. I'm sure others have got fees on that as well. 
There is kind of Nikolai, a nice link that Lazarus posted the dishes paper that has been published on the topic, and there has been at least three or four fairly recent papers on this topic. I know Angus Granderson I was editor of that paper that has been posted, so I know very well that work. 
That is also the work that we have been doing with the Isis standard on trying to find another way to do this. There is also recent War publishing, fire safety Jonah, which looked, let's say, a. more classic statistical methods to assess conversion. So unfortunately we don't have a final answer, and even when the iso document was put together, there were many unanswered questions here. 
They think that we stated there is that this needs to be at least assessed. So there needs to be some thinking on this. You cannot just run 10 runs or 20 runs and expect it by default. Your simulations are convergent and you are capturing your distributional results when you use probabilistic mode. So that's a bit the message there. So something has to be done. There are some questions? Let me see, let's go. 
There was some comments on pre registration from Anne also about yeah, it's not necessarily something restrictive doing Prejudice, so you can do that, to set out rationale hypotheses and so on. So there was a comment on that topic. That is also comment. from Yuna compared to the p value that measures the significance between experimental conditions, the size of differences is really reported in papers. That's a good point. 
I don't know, Nikolai, if you want to comment on that. Yeah, that's a good point. I think it's good to interpret the effect size in terms of what it, what it actually means. I mean yeah, I mean if you, if you collect enough data, even the tiniest difference between conditions can be, can have a very low P value. But that doesn't mean that the difference between the conditions is Big. So it's good to consider both. 
So this is where my statistical knowledge starts to fail a little bit. I think you can do analyzes that are more sort of tailored to effect sizes and so on. But yeah, that sort of goes beyond what I wanted to talk. about here? Yes, of course. Yes, it's very good point. I think we're running a bit over time, so I take last one, two questions. 
There was a comment: after having run simulations like Pathfinder, FTS, could we apply any probabilistic analysis? If we can, what kind of probabilistic analysis? I guess depends on what type of model you have, because FTS set that deterministic model. Pathfinder use random sampling, so it's you first need to check what kind of model you're working with before. I don't know, Nikola, if you want to comment on that, but it's two different words. 
The point I wanted to make is if you, if you want, if you run your model and you want to compare two different conditions, then I think if you run enough replicates of your simulations, you don't need to do a statistical analysis because the p value you. know you can make that arbitrarily small just by running more replicate simulations. So if you do any sort of comparison, you don't need to do statistical analysis on models. 
In my view, and there's a good reason for that. If you, if you just look at one condition, I don't know one building and everything else is the same. I mean what could? What can you do? You could do some model checking. So but then you have to know your model and what you know, what assumptions it does. It's always good to report the full distribution of any measures that you're recording. 
So let's say, if you look at the aggress time, it's a good idea to plot, let's say the histogram of the egress time, or at least to some sort of box plot, because that gives you more of an insight and you know the. variability of the data that you've produced using the simulations. I think that's certainly a good idea. 
Nicola, this is a very good comment you did also for people that work with fire engineering design, because one story is to run simulation to compare which scenarios is better and the other than the other in terms of egress time. So okay, you can run then a statistical analysis and see scenario a improves, then it's better than scenario B when it comes to rigorous time. 
But then to study one scenario alone and do the typical analysis that you will do with the required safe egress time, then you need to kind of, yeah, put more efforts in, try to understand how, when and how you reach convergence, because otherwise you want you won't know so it's. It's Tricky, it's not a simple problem, and again there is been a. few papers on this topic. 
I think there is more questions, but I will have to wrap it up. I can just leave this Food For Thoughts there in the comments that there are Mike talked about, since there should be a good opportunity for using statistical analysis in model validation, can we use more statistical analysis and testing Beyond just comparing variants in times flow rates? Probably we should. 
Can we do statistical analysis of simulated that to understand significant factors and number of simulation runs? That's what we've been talking about, with Nikolai Arturo posted a nice link on p values Gabriel has been commenting on. Okay, we think moving from the mean to median to represent smaller samples of simulations, or indeed when the normality is not provide assumption, another. 
I mean we have a lot of interesting comments there and I think it's good that if. you want to discuss further when you call. I guess they will be happy to chat about these topics, but I think time to wrap up, because we are a six minutes after time. So I want to thank everyone for attending the webinar. As usual this will be. This has been recorded, so I will. 
Then we will share the recording and of course this is. We have also YouTube channel that I posted before, so you can subscribe and get updated on the new videos that we post there and stay tuned on our channels, because we will soon inform about the next webinar and the next topic. 
So I just want to thank Nikolai one more time for your time, and this was really an excellent webinar that sparked a lot of discussions on the topic of statistical analysis, and I thank everyone for attending it's. Really. nice to see that we have about 50 people turning up live. We all know we are all busy people, so this is very good to see so much interest on our activities. So thanks everyone and we'll see you all at the next webinar bye. 