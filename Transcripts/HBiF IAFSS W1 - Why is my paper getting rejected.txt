Introduction to publishing - Enrico Ronchi

Welcome everyone! My name is Enrico Ronchi and I'm the as just mentioned before I am the coordinator, together with, Erica Kuligowski of the IAFSS Human Behaviour in Fire permanent group and we are here today to open up this webinar series with the talk on the academic publishing world and I'm very happy to have speakers professor Guillermo Rein, professor Bart Merci, doctor Karen Boyce along with myself. 
The reason why we have these pool of speakers is because they are all involved in editing activities for some of the key I would say the main key fire science related journals in the field. As mentioned to you this event is the first event of our webinar series. We have a few ongoing activities within the HBiF group of IAFSS. 
If you want more information about this, you have here the link to join that I also put in the chat but I also put our emails and you can see we also have a Twitter account and a LinkedIn group that you can join. Everything is fairly new, we started recently so we're building up activities in this domain within the IAFSS. 
You don't need to be a member of IAFSS to be part of this group but of course it's encouraged and this is a good opportunity to see the type of activities that we do in this group and maybe also see for other groups and then possibly join in. So why are we doing this? 
We are doing this event because we want to be more clear about how the editorial process work and what type of decisions are taken by editors regarding papers that are submitted for publication, scientific papers. 
So the motive one of the key motivation of doing this is because we want to make sure that people know how those type of decisions are taken and what are the criteria used by editors of papers in general, in fire science, and more specifically today we will also talk about papers in the domain of human behaviour in fires. 
To do this we have three editors that are very active and involved in some of the key editing activities in our field. The first one is professors Bart Merci that is the editor in chief along with professor Luke Bisby of Fire Safety Journal. Hi Bart, I can see that you are online. 
Then we have professor Guillermo Rein who is the editor in chief of the journal Fire Technology and myself and Erika Kuligowski are actually associate editors in the domain of HBiF for this journal. And then we have doctor Karen Boyce who is the guest editor for the human being fire: special issues that are edited in Fire and Materials. Hi Karen. 
Then we will have, after the general introduction about some of the statistics and important information about why papers get rejected in those journals, we will hear more specifically about common rejection reasons for HBiF papers and you will hear some feedback on this from Karen and myself and then we will have some time at the end for question and answer so I will kindly ask you to add your questions in the chat so that we can read them and moderate them and you can ask pretty much anything related to academic publishing and decisions taken by editors regarding publications. 
So that's a bit the idea that we will have a discussion at the end so as I mentioned we are doing this because we want to understand what is the status quo of human behaviour in fire papers in three top journals in a fire safety science domain and also, we want to explain a bit the editorial process and in particular the decision making process when we actually decide what to accept and what to not accept because I've seen over the years there are a lot of misconceptions about who takes the decision how the process work and also there are some differences between different journals that we may want to be clear about. 
In addition we want to promote transparency in peer review so we want to be as transparent as possible about how those decisions are taken and what are indeed the factors the lead for instance to a rejection reason and help authors identify weaknesses in their manuscripts because if you hear now from us "okay you should not do this when you present your manuscript" hopefully this can be the type of information or even serve as some sort of a checklist for you to things to double check in your manuscript to increase your chances of publication and decrease the chances of rejection. 
And in general we want to promote a supportive environment in academic publishing so we want to dismantle this myth of whatever it is called "reviewers are too mean" comments. We saw a lot of things going on and jokes going on online about people being harsh reviewers but we want to actually create in the fire safety world a supportive environment in academic publishing because everyone has an interest in having better papers published and better science published and in general we want to promote and appreciate peer review as a constructive process in science. 
So this is just one slide that I put together to explain a bit what happened to your paper after submission generally at the beginning there may be a format check by the publisher not always some journalists do some journals do not do it and then the papers go to the EiC or one of the editors in chief if there is more than one and the EiC does the first assessment: is the paper is out of scope or it is evident that it doesn't meet scientific standards? 
In that case you proceed with the so called desk rejection, so this paper is not even sent for review but is desk rejected. Some journals have also associate editors some don't. For instance Fire Technology has associate editors, as well as Fire and Materials. Instead, Fire Safety Journal doesn't. So the paper either is sent to the associate editors or directly by the editor in chief for review. 
The associate editors can also decide since they are generally specialists in that subject that the paper is not good enough and proceed with a desk rejection. If they think it's good or if the editor in chief thinks there is some potential in the paper, then they're sent for review and then they go through the review process. Once it gets an accented role level, they are accepted. Some quick information: the number of reviews and reviewers can vary. 
On average we see in our field two three but they can be four. It can be more in some cases, can be less if we have very good reviewers that provide very comprehensive feedback. The reviewer feedback is advisory. So this is another thing that is not clear sometimes: the ultimate decision lays with the editors, not with the reviewers. The reviewers provide comments but then, it is the editor that decide. 
We sometimes see comments as editors "oh the reviewers were overall positive about my manuscript, why did you reject that paper?" Well, an editor can overturn decision sometimes or all the way around some reviewers may be very negative but an editor can still trust that the paper has some value and push the paper through another round of review in order to get it improved. 
It's the editor that has the ultimate decision and this can also be complemented with more reviewers feedback being asked or obtained and it can happen also that papers shift around between editors for different reasons. 
It can happen so that a new pool of reviewers can be asked to review the paper and in general the final decisions are always taken by the editor in chief generally also in consultation with the associate editors, depending on the journal, if there are associated editors So I leave the word to Bart now so that you can give an introduction about Fire Safety Journal and the common rejection reasons in the journal that he is editor for. 


Fire Safety Journal - Bart Merci

Thank you very much Enrico. Thanks for the good introduction as well on the process which I think was very clear and important to share. As you mentioned indeed I'm actually speaking now on behalf of Luke Bisby from the University of Edinburgh and as you mentioned but I will repeat: we do not have associate editors with Fire Safety Journal, meaning that essentially Luke and I as you described first do a quick screening of any paper that is submitted to Fire Safety Journal to check whether it's sufficiently within the scope of the journal and then also to have a first check on the overall quality of the paper including the level of English and I'll come back to that a bit later. 
As you can see here on this slide it's actually Fire Safety journal is trying to be a journal with a wide scope but nevertheless let's say the scope would be "added value in the field of fire safety science or engineering". Maybe we can go to the next slide please? 
I had a hard time, trying to find how many papers were dedicated to human behaviour in fire so what I did was a search to papers with the word "human" in the title, hoping that those would cover most of the submissions in this field. As you can see here with the numbers I will not go through all the numbers. 
It's a relatively steady a fraction of the submissions and it's a relatively high fraction of the submission so approximately 20% in the boxes that you see at the left hand side of this slide, you see that we have a relatively strong increase in the number of submissions and then the acceptance rate is a bit variable from year to year but I would say and that it is around 30% I have a few years with higher ones and these are the years where we have the IAFSS symposium proceedings which have been reviewed before they are then transferred to Fire Safety Journal. 
Also maybe worth mentioning is that there is only a relatively small difference between the number of accepted papers and the number of published papers per year with one exception perhaps in 2020 and that's because then again this refers to the symposium proceedings which had been planned to be published in 2020 but as we know the symposium has been postponed until 2021 so those papers have been then published in 2021. Next slide please. 
So then the question was asked to list a few of the rejection reasons in fire safety journal so the top one is. Maybe I should say that all these reasons here are not specific for human behaviour in fires and they are really general but they also apply to do the human behaviour papers. 
So one important aspect is novelty and added value: so if that is insufficiently clear, either assessed by Luke or myself or by the reviewers, then this is a reason to reject the work and that links also to the positioning of the work within the context of what is already known. 
So if the paper comes up with a method that has been well established and applies it to a single test case and believe that it's something new then it's clear that the author does is not really aware of state of the art. The second reason for rejection is that the relevance either scientifically or the practical significance is not clear. 
So it could be a well described experiment for example, but if it's not clear how the experiment itself would add to the knowledge or skills in fire safety science or engineering then that can also be a reason for rejection. An obvious one would be technical flaws in the work that cannot easily be repaired. 
These flaws could be at many different positions, it could be in the method used and that would be for example experiments that have not been characterized completely or clearly or simulations that have not been done with the correct settings but an important one is also the analysis so we sometimes find papers where many observations have been made but no explanation or no interpretation has been made. 
Well that could then perhaps be a report with interesting information but it's not a journal paper and so it's not a paper for a scientific journal. If it's out of scope that's also one reason to reject but. 
Also sometimes it's clear that it would fit really much better elsewhere and to give one example: if the paper is on a risk assessment methods and it's very mathematical and really focusing on the methodology to try and quantify risks and then the application could be fire related and still it would fit much better in a more mathematically oriented journal. That's just to give one random example. 
Then the final one, and it's that's an important one as well: is the level of English .It's much acknowledged and appreciated that this is not always easy for non native English speaking people. I am not native English speaking myself, but it's very important that this screening of the level of English is done before submission because otherwise the paper will not be sent out for review. 
It's not comfortable for reviewers to receive a paper which is very hard to read, where you have to guess what the authors are trying to explain. Unfortunately sometimes even assistance is looked for and still does not lead to a sufficiently high level of English but please be aware of this and pay attention to this aspect. That's all for now and I'll be happy to participate in any discussion afterwards. Thanks Bart. 


Fire Technology - Guillermo Rein

So we can go to the next journal: I see Guillermo that you're online. Yeah can you hear me well. Yes all good. Excellent thank you. Thank you Erica and Enrico for organizing this and good morning everyone. I think the organization of this event is a fantastic idea and I could not be happier about what this new group of human behaviour and fire is doing. 
So I would like to tell you a few things about Fire Technology. You will see that there are many things in common already with what Bart has presented to you with the journal of Fire Safety. 
Just to say: Fire Technology is the is the oldest journal that we have in fire science, started to publish in 1964/1965. It is the journal associated to two societies: to the NFPA (national fire protection association) and the SFPE (society of fire protection engineers). So we are the journal of these two agencies. You can see there are a list of the associated editors. 
The structure of how the journal works is different, probably the biggest difference that we have with Fire Safety Journal, is that the structure is based on one editor in chief and then there is a group of associate editors. Associate editors are experts on the specific topic in case I want to call for them to handle it. We are very fortunate to count with three world leaders on the topic of human behaviour in fire. 
We have Steve Gwynne, we have Erika Kuligowski, and we have Enrico Ronchi. It's a journal that no matter how you look at left, right, up and down is in a healthy situation, is attracting good papers, is being read and, is having an impact in the community so we are happy for that and obviously we want more. 
We think that fire safety engineering and fire science can grow much more and actually it is growing and obviously we want Fire Technology to be part of that growth. Enrico the next slide please. So these are a little bit of the metrics. It is a little bit of a small front, but just to let you know that we receive about 300 to 400 papers per year in all topics related to fire. 
Only a fraction of this, maybe a quarter or between 15 to 20 to 30% are related to human behaviour. We typically accept about a third to maybe 40% of the papers. Sorry 1/3 is rejected and the rest is accepted. Sometimes it goes higher, sometimes it goes lower. We publish per year in the order of 100 to 150 papers per year. 
It is something that used to be more like 100 per year but now we are moving more towards 150 per year just because the journal is growing. It is attracting more top papers. You can see the countries are very similar to the countries that also publish in Fire Safety Journal and in Fire and Materials. Maybe with us there is an emphasis in countries that used to be less represented. 
For example China was not very represented in Fire Technology so we have been for the last few years, having an emphasis on attracting top Chinese papers into the journal. When you look into the percentage of papers that are accepted and published in the journal, regarded to human behaviour: it varies from one year to another. 
If we have a special issue on the topic, then probably that year we have more and if there's no special issue or there is a conference going somewhere else and that typically means more papers on that topic appear somewhere else. We have between 10 to 20 of the papers in the journal that are on human behaviour. Next slide please. 
When I was asked by Enrico and Erica to list the five top reasons of rejections inside Fire Technology, I came up with a list that is very similar to the one of Bart, maybe expressing different words. It is very important to learn from cases that did not work out. 
It's very important to learn why papers are being rejected, why the research that has been conducted and the time that has been taken to write a paper doesn't end up being published in a specific journal. We can all learn from this. I'm very optimistic in nature that next time we do this seminar we do it next year, it should be the reasons why the paper got accepted and then we can focus on the good example. 
So for example, when my students say "How should I write my paper?" I always tell them "well look to the journal that you like, the journals that you like and look for the award winning papers, the paper that has been highlighted by different institutions on different reasons or maybe chosen by the editors, and read those and look into how they structure, look how they argue, look how they use the figures, look how they conclude, right? 
It's not just the method it's not just the experiment. Writing a paper is more than just doing the experiment." Anyway top five reasons are and I try to put this in order of importance, although towards the end it's more difficult. The first one is when we don't see enough scientific rigor. 
Typical papers submitted to Fire Technology do have some scientific rigor but there has to be a threshold above which the editors (me and the society) to say "well this is a serious endeavour of science and we think that the method used is justified or well chosen within the decisions that are in the paper are well justified and are convincing, we think that the model is validated, we think that the experiment is well done etcetera". 
So is it's very important to have scientific rigor. Also, the scientific rigor depends on the previous literature. When we have an absolutely new topic we cannot ask this topic to have the same scientific rigor as a topic that has been studied for the last hundred years. The concept of scientific rigor, in reality, is relative to how much we, as a community, knew of the topic beforehand. The second one would be the novelty. 
It's very often that we see papers which are doing things that have already been done. In reality is not bad that something has been done before but if it's going to be doing something that has been done before it has to do it better. 
And it has to refer to the previous work so if a paper that's something that has been done before doesn't even mention the work that has been done before and it doesn't do it better then this paper is going to be a desk rejection as soon as it is identified. 
The third one it is that the paper is going or has gone through multiple iterations of review and revision which is a very important task I will add something about this is a very important task of journals. The journals are not the police guarding a castle or the police guarding a treasure and it's like "oh yeah you're not good". 
No, we actually see journals as a part of the engine of science so we promote quality and we promote and signal direction of research and of building a community. So the process of reviewing and revising it is improving papers. 
It's not just the police to be convinced that is good enough, this is also a process where the reviewers are actually contributing and helping the authors and I have to say that this takes a significant amount of resources but it's one of the most beautiful tasks that a journal can do in a community. 
So sometimes there is plenty of reviewers plenty of revisions and the editors just don't see this is involved this is not improving fast enough, this is not going the right direction. It's taking too many resources so it's better to stop it now and that's one of the reasons that it's very dramatic because obviously the authors and reviewers and editors have all invested into this but if it's not working it's just not working. 
The fourth one is when the work might be of scientific or rigor maybe it's novel maybe it's not going through many revisions but actually maybe the authors are not putting their work in the right context of the previous literature. This happens often I have to say. 
Especially when junior authors are in charge and maybe the senior authors take more of a backseat on writing the paper, that there is not enough integration of the work into what existed before. This is something new and what does it mean in the context of what was done before? The integration and justification and being able to build on top of the shoulders of giants is very important. That will be number four. 
Number five is one that on itself is not a reason for projection but typically it comes combined with a little bit of three and a little bit of four and a lot of five. Five is when the paper just doesn't read well, it just doesn't look good, the English is even worse than my English, the abstract doesn't tell me the conclusions, are not convincing. 
When you see the authors have not written an outstanding paper and that reason five adds to the others, revisions that itself can lead to a rejection For a paper to be rejected only on the grounds of number five that, the paper is not written well, it has to be a pretty bad case, right? 
Because if the research is novel, the method is vigorous, there is an interest into these and is well placed into the literature but it's not well written, we can fix this in the process of reviewing and revising, signalling the authors where they actually need to improve the process. 
These are the top five reasons that we see in Fire Technology and before I finish I want to highlight that this community the human behaviour in fire community is one of the most dynamic communities that we have in fire science and in the journal of our technology you can imagine we are in contact with pretty much every single community that deals with fire of any side: buildings, industry, forests, small, large, even in space and this community that is being now integrated and led in the new working group is tremendously modern, it's very young if you look into who you are you're very young you're very dynamic and you are very constructive. 
Some of the best and the most impressive review processes that I've witnessed are actually in this field and I think that's very good. It means you have a bright future ahead and I'm very thankful for Enrico and Erica to organize this. Thank you. Thanks Guillermo. Thanks for the feedback and the information about Fire Technology. The next journal that we will see is Fire and Materials and Karen, please go ahead. 
Okay, thank you Enrico and thank you Erica for inviting me to speak today. Fire and Materials is not just quite as old as Fire Technology. I think the first publication was in 1976 and I guess this one you maybe wouldn't directly associate with human behaviour and fire papers unless you have been to the human hearing symposium over the year. 
Traditionally it would have dealt with more on fire properties materials and construction products, but certainly in the last 20 years the scope was widened and you'll see the list of topics that are covered there. Most importantly I guess for in this context is that we do publish human behaviour in fire papers. I say we. 
I'm not the EiC or even an AE but I've been very kindly asked by Steve because of my involvement with this symposium over the year, to be handling all the HBiF papers that come into Fire and Materials and I have been as Enrico mentioned earlier the guest editor for the human behaviour fire special issues. 


Fire and Materials - Karen Boyce

The journal publishes about 150 papers, I believe, a year and it has a good current impact factor which I guess like other journals has been grown in recent years. Next slide, please, Enrico. I did a little overview of what our human behaviour in fire papers in Fire and Materials and as I've just mentioned we've had a number of special issues following the human behaviour in fire symposia which are listed there. 
Most recently, we had a special issue from Interflam in 2019 obviously with the association with Fire and Materials in our science communications. So we've had 68 papers and special issues over the years, most of those being in the last decade but we also do I think this is important given what I've just said is that it may not be a journal you would naturally associate with human behaviour in fire papers but we do publish and we do welcome submissions and not failed very much so as I say I would be dealing with those, handling those as they come in and we have been over the last 15/20 years been getting about two to publish and about two to three per year and through that process and I would very much welcome that to continue and encourage you to think about Fire and Materials as a journal that you would want to submit to. 
Our acceptance rate is about 40% I guess it's quite similar to some of the other journals for the special issue papers that would have been higher because many of those would have been chosen or invited in a sense that some of the early ones might have been invited to submit Fire and Materials. But in general, that would be a typical acceptance rate for the journal. Enrico we go to the next slide. 
I did a little analysis of the breakdown of all the different types of the papers that we have published in Fire and Materials in HBiF over the years and you can see there I guess not surprisingly that about 40 percent of those were maybe observational or experiments that might have been controlled experiments in a lab or it might have been drills on last evacuations again not surprising. I guess, given the context of our work. 
Many of the papers were questionnaires/survey type studies, questionnaires interviews Some of them were very particular to certain case studies: big fires that have occurred and I guess an important point is that many of them I think 15% were mixed methods and I think that really lends weight to your work if you do have and that sort of triangulation in terms of the approaches that you use or the sources of data that you get because that lends weight and can very often enhance the types of study that you are presenting. 
I can go to the next slide now, Enrico. I think that the common rejection reasons that I have sort of come up with for the HBiF papers that I have seen coming into Fire and Materials and I guess also as a reviewer and reviewing papers in the past, they're no different really from what Bart has said and what Guillermo has said in terms of Fire Technology because we are looking obviously for certain standards in all our papers and I would again sort of reiterate the fact that you know, one of the main things is lack of novelty or practical significance of the work I mean that's really important I'd say, practical significance of the work I guess and it's not necessarily the case that there has to be major practical implications of the work. 
It could be a conceptual model for example in human behaviour that maybe isn't directly going to be applied somewhere but it does need to be significant in terms of developing our understanding of the field and the fact that I guess that isn't clear sometimes in papers I do feel stems from a lack of understanding of what is already known in the field and that sometimes is apparent from the literature review. 
And I think Bart maybe mentioned this earlier on you know that often what we do see in the literature reviews that it does lack depth and the paper that's being presented, the work, that the researchers haven't actually understood what is actually published already in the field or what is the state of the art and therefore how their work fills that gap. 
Often it's been put forward as something that's novel but isn't actually novel because they haven't maybe widened their literature to but isn't actually novel because they haven't maybe widened their literature. 
Often and we do find papers being submitted where the literature review is very much focused in the country of origin of the authors or of the work where the work has been conducted it doesn't actually recognize that work has actually or similar work has been conducted elsewhere which in itself is not a problem because there may be certainly very good reasons to do similar types of work in different contexts or different countries particularly because there might be cultural differences or other differences in human behaviour and that are really important to explore but I think if it means that the authors don't show that they do recognize the field and there's no critical comment on the field then and knowing where their work sits or not then that certainly can be a reason for rejection because that also leads in then on into the work that they've done and on the novelty of the experimental work and on the and the actual significance of what they produce in the end and part of that I think Enrico will mention this later on more detail but sometimes we do see lack of misuse of terminology and again that in a in itself is not an issue, but if it is one of those things that are building up a picture there's maybe some lack of understanding of the failure lag of real consideration about what we really do know in the field then that can lead to issues down the line. 
We can go to the next slide I think there Enrico. The second reason I guess and this has been mentioned of course as well as the methodology and I think one of the first reasons why a paper might be rejected is because there's just very simply incomplete information I mean, at the very first instance at first scan of the paper if it's very scarce information about for example the sample which is important obviously in our field and the data collection tools that have been used the procedures that have been adopted, then we start to raise questions I guess about that paper. 
Now generally I would tend to if there's if I see something as the person I'm looking at this in the first sentence I see something of worth I'm not like Enrico said, you know, I would send that out to review but if these things aren't addressed in the review process then it really does start to raise questions about whether proper consideration had been given to the sample and the tools and the procedures and we start to question that. 
I think that when we do go forward we do get that information of course this is like any paper I mean the methods we have to have the correct methods given the research question that's being asked and again unfortunately I still do see papers coming in where people authors have used a questionnaire for example a very simple questionnaire to explore behavioural response so they'll just ask quests would you just send out a survey and ask some questions about what would you do if and I guess in terms of you know really understanding human behaviour and the complexity of that's not good, that's not an appropriate method. 
So the choosing an appropriate methodology is really important and of course that relates back into the research question that we're trying to ask and then of course, given that the approach might be appropriate it's then you know thinking about the tools that have been used, often again we see questionnaires or we see interviews where it's not very clear even what the questions ask have been, how they've been asked, how they've been formed? 
Sometimes the questions might be asked in such a way that's very obvious that there might be misinterpretation of those questions or there might be bias in terms of the responses. 
Maybe in terms of the fact that the questions aren't, you know, if there's a number of series of different things to choose from that they haven't been asked randomly so that could influence that and again in and of itself it's not necessarily an issue but it's how that's interpreted. So some of these rejection reasons as Guillermo said, I think, they all combine often to a rejection of a paper. 
In terms of experiments I guess one of the things that we see sometimes is that the experiment created has really no little or no relationship to the real world and I guess we're talking here about the external validity. 
But again, in and of itself is not necessarily a problem we realize that we can have real value benefit of controlled experiments, but again, it's in the interpretation of that, is there recognition of the limitations in the work and is that being considered when we get to drawing conclusions from the work. 
Sometimes I've seen papers that have come in where there's been different trials for example an experiment on very little understanding of the fact that there are or controlling the fact there may be learning between those multiple trials with the same participants and that's not taken into account and there's conclusions drawn about that just aren't valid given the way that the trial was set up or the experiment was set up and, of course another main problem, I guess, in our fields, often is in terms of the sample, and again not an issue of itself we know that in our field that there is very hard sometimes to get samples particularly when we're doing experimental work, there's lots of ethical considerations with respect to having maybe very diverse populations. 
Certainly if we want older populations and on people that have impairments and we want to do work in that area sometimes that can be hard to get an approval for sometimes it can be hard to get participants. 
We do recognize that but it is really important to recognize that if we do have smaller samples or we do have samples that aren't maybe representative of the more general population that we're very careful in and how we interpret that data and we can go to the next slide I think Enrico. 
And I guess analysis again has been mentioned by Bart and Guillermo and I would certainly reiterate that as a common reason and again sometimes it's the fact that there's just lack of detail being provided and, for example beyond how the data was extracted from videos how we define parameters, how density was defined for example, how we measured speed if that was if that was something that we were looking at, and again something that maybe can be refined through the review process but if that's not if that's not clarified to our satisfaction then again that can lead to and that paper being rejected. 
And again, you know, incomplete or superficial analysis again not a problem necessarily that we do sometimes use very simple analysis, but often what I see is that the analysis doesn't really take and in terms of the way the experiment maybe was set up in the first place or maybe with in terms of the sample or the questions that were asked to characterize the sample or characterize the participants but then the analysis is just superficial. 
It doesn't really help us get deep down into really understanding the impacts and the implications of the findings and often again that leads to conclusions that are drawn that are unrealistic given the given the way the experiment was set up or a questionnaire for example was set where the interviews were conducted the analysis that was done. 
So these things tend in my experience tend to follow through and compound at the end of the day to lead into rejection of the paper and of course inconsistencies analysis. 
We see that as well but sometimes that can just arise from a simple mistake that the author has made or the researchers have made in terms of all that analysis and again can be dealt with through review and as I said, most of many of these things can be dealt with through that review process but sometimes we do see an unwillingness of authors to address what are identified as shortcomings or to an unwillingness to undertake more meaningful analysis, that has been suggested by reviewers, that would really let deepen that, make that a much more significant piece of work and I think that as well ultimately could end to a paper that does go through the review process, been given opportunities and be given suggestions how that could be deepened and it's not taken up and that could lead to ultimate rejection and finally then and I think I probably said all of these things, we can move on the slide there Enrico. 
I guess I've already said all of this together but you know we recognize that it's really important to say that and in HBiF, it's always going to have limitations. I mentioned before, you know, we can't, we all know, we can't go and set a fire in a building and look at how people respond. There's always going to be limitations there's always going to be ethical considerations because we're dealing with human participants. 
There may always be limitations with respect to the sampling and the involvement of human participants. So that in and of itself is not necessarily an issue but it's how researchers interpret what they have done what they have found so again a real main reason for rejection would be drawn conclusions as the slide says that are all substantiated given the research approach given the design given the data collection tools given the sampling not recognizing what the limitations of the research actually are and drawing conclusions that are or that don't have value given what has been done and part of that I guess is generalizing to other groups onto other contexts that aren't substantiated, given how the study has been set up in the first instance. 
So those are some of the reasons I guess maybe a little bit more particular in my experience to human behaviour in fire papers and I'll hand over to Enrico. I'm very happy to get involved in a discussion later on if there's anything to any detailed questions. Thank you Enrico. 


Papers related to modelling HBiF - Enrico Ronchi

Thanks Karen for this nice overview. 
I will just bring up a couple of more thoughts about rejection reasons that occurred in my experience and since you focused a lot on experimental papers I decided to focus more on modelling because both as assessed editor for Fire Technology as well as I'm an associate editor actually also in Safety Science which edits a few papers linked to modelling of evacuation, there I actually edited around under 50 papers in this domain the last three years so quite a lot. 
Also as reviewers in Fire and Materials and Fire Safety Journal sometimes, Bart or Karen have reached out to me. I see those type of papers. So what I generally see as main issues is there are either a flawed modelling assumptions and this is something that Bart for instance brought up in general for Fire Safety Journal but I think this is specifically important when we look at HBiF because sometimes the modelling itself is a good exercise done by the research that did the work, but they're not modelling reality. 
They're modelling something else they're modelling what they think is reality and this is very much linked to the use of terminology a certain concept that we know from existing literature that are not actually valid in our domain. When we see papers that attempt to model irrational behaviour that's by itself because you cannot model rational decision making if you're attempting to model irrational behaviour. 
"Panic" or use of terminology like "stampede", "cotangent" so really read up about the literature on this. I put a couple of papers. One, the main author was Rita Fahy that explained the misconception and the use of the panic word and what is called panic behaviour. 
Another one, most researched that from Haghani and some colleagues that did a very nice overview on the use of different words that are use in the field crowd dynamics that have been, let's say misused, in a few contexts. 
So what I generally do, I'm very explicit: I go even like with ctrl+f and check if one of those words is used and if I spot one of those words then I go read deeper and say "okay what are they actually really" because you know as editors we don't have time to review ourselves all the papers so we want to have a general idea about the paper before it's sent for review. 
So if you are attempting to model "panic" then you are not knowing what you're doing. That's the thing. So be careful about the choice of terminology and the choice of wording that you use because this is a really red flag for editors in our field. Another thing that sometimes, we see is a mere application of an existing evacuation model: somebody that takes a commercial model or a research model and they apply for a case study. 
A given case study itself is not a problem alone but you need to have a very strong motivation for having this being a research paper so there should be either a very unique application or something beyond the regular state of the art of applications in the domain of fire safety engineering because otherwise, just applying a model to a case study, that's not research. 
That's the work that the consultancy do, that's the type of exercise that we teach to our students in our educational program so that's not research. Another thing that I see very often happening is incremental modelling and again without validation becomes quite critical so in itself is not a problem to expand the model. 
I put here the example of the social force model, because I saw many of those papers but in itself that's not a problem but again you need to have a very good case and a very good argument to have a pure modelling study without of dedicated validation, so without experimental data, in order to be sure that there is an added value in what you're doing because just taking an existing model adding a parameter making some assumptions that are not strongly substantiated make your paper very weak so you have to be very convincing in your assumptions modelling assumptions and the value that you are doing in this tuning of an existing modelling framework. 
Ii put here the example the social force model because it's the one very commonly used in our domain but it can be any other type of modelling approach for evacuation. Another thing that often happened and this also was brought up before, is lack of specification in the input calibration and their impact. 
So if you are running a evacuation simulation, you need to make sure to have a proper specification on how you calibrate the input and omitting information is always a bad idea so better to be honest and say "okay I did this calibration based on these data so I'm doing an open simulation" rather than a blind calculation and explain what you have done but trying to hide things in a modelling study is never a good idea because that's what makes the editors more suspicious that they are trying to hide a flaw in your model and reviewers as well, they can get like "okay that means that there is no clarity in hearing what they're doing, I cannot understand the results" Probably the results are flawed if something happens that the results is not convincing so better to not over calibrate an evacuation model but rather to be transparent on what are the limitations because there is value in publishing actually results that have some problems and explaining what those problems are and how can this study improve the literature. 
The other classic issue is extrapolation beyond the scope of the model. A classic example that I use a cellular automata model, so a model that is based on the assumption that you do the size of the cell is going to give you the output of the densities. 
So this is just an example, like using models that have been calibrated with data on evacuation for a low or medium density then instead are applied for very high highly dense situation that won't work. 
I put here a paper that I suggested reading that I wrote a couple a couple of years ago together with Nikolai Bode that is a very good research with background in mathematics that explain the problem with statistical model fitting in pedestrian dynamic research so there is a list of common errors done in papers in the impedance dynamics another issue is the similarity with existing modelling work and this has been brought up a few times today so I don't go too deep on this. 
Another problem is applying probabilistic models because we model human behaviour, so we don't see deterministic models in our domain but using a deterministic approach. So we're running a Monte Carlo simulator, Monte Carlo simulation and then just presenting one result that's conceptually completely wrong and again this is something that also tell us as editors that you have not really understood the problem so be very careful when you use an evacuation model and you do probabilistic analysis. 


Open Discussion

So that is it from us so I would like now to open the floor for comments or questions so I will kindly ask you to write them in the chat or if there is somebody that wants to speak up please add them because we want to also hear from your experiences and especially for those that felt like okay I felt here that it was unfair that my paper was rejected because I felt I did this and maybe you can present your case briefly and we can try to see probably why based on what we discussed was the case that your paper was rejected or if you have a success story of a paper that went through the editorial process anyone wants to speak or write in the chat, we have over 50 people attending. 
I can kick off the discussion if we don't have comments. There is a question: How often do human behaviours are published outside fire journals? That's a good question, as I said, in Safety Science I see a lot of papers linked to evacuation, but not so many about fires. This is probably, I would say the three journals that we have today are probably the main hubs for fire papers. 
Sometimes it's as I say simulation papers, you can see them around also in modelling journals or mathematical modelling journals, that's my experience but I would like to hear from the three editors here about one thing that I had for that I think it's useful for the community: if you will have to give one strong advice to the authors, especially to junior authors, of papers that are trying to submit their papers, they want it published, what would you tell them? 
So maybe Bart you can start with this? Yes Enrico. Well I would say that my major advice is to take your time before submitting something and it kind of connects to what Guillermo also said: don't think of editors or reviewers as kind of police. We are all trying to serve the community to have efficient advancement of science and in that sense I would say take your time before submitting. 
Don't take too much time I would say, don't be scared to submit something, but don't push yourself to try and submit something that has not properly been finished and it's kind of the elephant in the room, we all know that there is publication pressure, particularly on young PhD students, but "don't get carried away by that pressure" would be my advice and maybe it's also partly due to the fact that we haven't had as many conferences as before the pandemic but I have the impression that some authors seem to be under a lot of pressure to try and have something published: that's not time efficient for anyone involved in the process and also not for the authors themselves because then often they will lose time rather than save time so be sure and look for quality first and make sure that you're confident about the quality of what you submit, that's the advice that I would give. 
Karen or Guillermo, do you want to ship in on this? I can. Oh Guillermo, would you want to go next. No please Karen go ahead. Okay I guess what I was going to say and I suppose I did a little during the talk through my slides was that really take on board the concerns and advice and suggestions that are given by the reviewers I mean I think we all, and we all don't like sometimes critical comment but it is you know supposed to be constructive comment. 
Yes there might be there may be some serious technical flaw that maybe just simply can't be addressed, but very often you know we want to see papers published we want to see good work published and sometimes it does take that review process to make that paper the best it can be so I would say I mean I myself have had papers in the past where you put it in and you think "that's a really good paper" and then the reviewers have come back and actually there were lots of things that you thought had been explained and weren't done and I've had needed significant work but ultimately what we've ended up with is a better paper so we all want our work to be you represented in the best way possible so that would be my advice is to take on board the comments reviewers don't see them as something that you have to just have a quick comment a quick fix and put it back really take on board spend time to revise the paper as best you can because it's your work that's right there at the end of the day and it's going to be read hopefully by many people and the better it is the more competition it's going to make ultimately down the line so that would be my piece of advice. 
Guillermo do you want a comment. Yeah total symphony with Karen and Bart, a hundred percent, to see the review process as a constructive process that actually leads to a better paper. I'm fully aware of the stress and some of the difficulties that going through peer review means especially the first time or the first times. 
I'm fully aware of this I remember my own experience and I see this with my students on a weekly basis but just two notes on this reviewers are humans and we all humans have levels we are not perfect we could actually be giving subjective views we might not have read the paper in the reviews might have not read the paper in detail they might have under misunderstood so the important thing is what a reviewer says is not the law of science it's just a view of a person that is has dedicated a significant amount of time to your paper therefore engage and embrace that golden opportunity of getting your paper better by having a strange conversation through anonymous peer review online with a person that really wants to know about your work so engage but they are humans they are not robots and they are not Nobel price winners. 
Well sometimes they are price winners but most of the time we're not noble price winners because as Bart is referring: peer review as a concept is actually changing towards something that I personally feel uncomfortable. 
We're all familiar with journals that are offering no peer review although they say that they offer peer review and I worry what could happen to our community or to your community, which is a subset of our community, and you are younger and you are more modern what could happen to this community if people start to move away from proper peer review and start to look into almost like a magazine like whatever you send you pay them and they say they review it and get accepted? 
What could happen to this community in 20 years or in 10 years if that is the future of publication of science? I worry about that and I invite you all to think about it. Thanks Guillermo for the thought, very much sharing your views on this. We have a couple of questions, very briefly because I know some of you have to leave soon here we're all very busy people. 
What advice would you give for people who are struggling with English in their papers? How to arrange proof reading? Anyone wants to comment on this? Bart, Karen, Guillermo? Yeah I can say. sorry Bart, you were first, go ahead. It's not a very modern issue, it has happened since ever. English is the language of science for historical reasons, some of them good reasons and not very good reasons. 
It is a fact and it's not going to change and if your science is truly scientific universal knowledge as we all claim it is, it has to be read in English. Otherwise it's not going to happen and it is a struggle, English is not my native language. I struggle to learn how to write and how to do write papers in English. 
It is 100% a requirement on this planet, if not we would have to change planets. So it's not negotiable. There are services that are trying to help authors to improve. Those services are interesting, they are a nice way of helping you but that's not the future. 
The future is to read a lot, to use good examples, to polish our English, to try hard and we don't have to write like a beautiful paper like some native English speaker that has been writing in English for the last 50 years but our English has to be sufficient enough that the paper can be read and understood. 
Maybe to add to this: sometimes it's also a problem, even for native English speaking persons, who were just sloppy and grammar sometimes and it's a matter of I would say, also but let me rephrase. I mean the level of language also often mirrors the overall seriousness and overall investment in time that has been put in the work, not always. 
Sometimes you have very you can see that it's very high level research but it's difficult for the authors to find the correct words, that can often be remedied. What I'm also referring to is more where you see very sloppy language including, titles of sub sections or even the title of the paper that has a typo, that's very disturbing and so, if the authors do the effort, then very often you get to the level that you that you want. 
It's more a matter of attitude sometimes, I think not always, but often, it is and I think it's worth investing time in, you need to really try to get to get that right as Guillermo says I mean, this is the lingua franca, whether we like it or not, that's what it will be. For the foreseeable future as well. Just two more things and then we close. 
There was a comment in the chat and I promise that we didn't talk about this with Mike that wrote the comment: Yes it would be nice to have the next webinar about how to do peer reviewing. That's the top of our next webinar and it's on the 30th of November at the same time and Erica and Peter Lawrence from. 
The University of Greenwich they will talk about how to do a nice peer review for HBiF papers. So you have the link here to register and I will share this soon in the chat and also to in social media and there is one last question and then I free the speakers because I know they're busy. It was about register reports. 
So in psychology for instance, it's getting more popular people before doing experiments they publish publicly the ideas or at least their plan about their experiments, get feedback from the community and then they work on their experiments and papers. Would this be an idea for our field or do you think it is not really a good fit for the fire science community? We are not there yet, doing this type of collaborative work in this domain. 
Maybe Karen or do you want to share some thoughts. I think it's an excellent process but I guess my concern. I don't know how others are feeling but I think that at the minute this is getting more and more difficult to get reviewers and good review and I think that we're a relatively small community, that would be just my concern as to how that would actually work in practice. 
But I'm sure there might be mechanisms to make that work, but I guess. Yes: a really great idea that we can get involved on the operation you know at the very outset to make sure that some of these things that we did talk about earlier on, are addressed at an early stage I guess it's just in terms of. Maybe this group could be something that looks at that and how that might be forward. 
Great idea but my concerns would be in terms of just getting the buy in and the commitment across the board to make that work on a regular basis. Yeah, I mean it could be some sort of a paired system. So I review your idea, you review mine and if we all gain. Something like that. But I think we have to wrap up because we are now over the schedule time. 
I have put the link in the chat to register for the next webinar event. The structure will be exactly the same as this one, so I really want to thank again the speakers for the contribution because, as all know, you're all busy people and I really appreciate your time and I hope that all the communities appreciate you! 