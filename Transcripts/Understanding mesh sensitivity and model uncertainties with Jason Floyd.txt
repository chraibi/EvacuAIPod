 Hello everybody and welcome to the Fire Science Show, session 65. For this week, I thought that one tool that almost every fire engineer uses, or one of the most used tools, is CFD, Invitational Fluid Dynamics Modeling, and in particular, Fire Dynamic Simulator, FDS. A tool that is so overwhelmingly used in the fire industry, it's one of the staples. And it did not get much love on the podcast yet. We didn't talk that much about using FDS and what does it mean to use it, how to use it properly. I thought it would be a great addition to the podcast series to discuss this subject. So instead of broadening your horizons, as we usually do on the Fire Science Show, this week we're doubling down on what we fire engineers do every day, and we're diving very deep into Fire Dynamics Simulator. And obviously, if you're not one of the fire engineers who use it daily or very often, or maybe you don't use it at all, I guess it's still nice to listen and build your expectations on what you can expect from numerical modeling of fires and this episode goes also beyond modeling so i guess it's gonna be interesting to almost everyone my guest today is dr jason floyd jason is with fire safety research institute ul at the moment and he's been with Jensen Hughes for like 20 years, so a huge chunk of his career done in an engineering company. Jason is on the FDS developer team, and he's very, very knowledgeable about how the software works. And the trigger for this episode was actually a discussion on issue tracker of FDS. There's Google groups that's called the issue tracker, where people submit their problems with FDS and modeling, and the FDS developer team is trying to answer their questions or help them. And there was an issue where someone was asking why increasing mesh resolution does not improve their calculations. And I thought it's like super nice point to start because one, what does it really mean to increase the resolution and what does that do to your solution? And the second, what is a better answer? What's better accuracy? What's a better simulation? I mean, if you look at that philosophically, it's not that obvious what simulation is better, which one is closer to reality, what the reality really is, what are the uncertainties of the measurements in the experiments, what are the inherent differences between two seemingly same fire experiments and why you sometimes obtain two completely different answers from them. This is something that Jason has actually answered in the issue tracker, and I've tried to pull him further in this talk. So I think it's going to be an interesting take on how do we model, how simple choices like the choice of mesh resolution, the choice of your models, how they affect your simulations, and what the reality of modeling fires and fire experiments really, really is. So without further ado, let's spin the intro and jump into the episode. Welcome to the Firesize Show. My name is Wojciech Wigrzy≈Ñski, and I will be your host. Hello everybody, I'm today here with Dr. Jason Floyd from Fire Safety Research Institute. Hey Jason, great to have you in the podcast. Great to be here. Really happy to have you in the podcast. Great to be here. Really happy to have you on the show. This is rapid development. A few days ago, Brian Klein from Thunderhead has posted on LinkedIn an interesting post from the issue tracker named, Find Mesh Decreased Accuracy, where someone's reporting on how changing the mesh in FDS did not yield better results for their CFD calculations. And it's an interesting issue in the issue tracker, but not very much related with core of FDS, I guess, but very much related to how are we using the modeling tools that we have. And I immediately felt in love with this, with this issue. If you can fall in love with an issue, an issue tracker, but it's really interesting, like, because it pinpoints the things that we should be discussing when discussing CFD. So I would love jump into this very deep. So tell me why, why fine mesh decreased the accuracy of this guy? You know, there's a number of things potentially that are going on here. In this particular case, the user was attempting to replicate, I think it was a wood crib experiment out of a paper. So you always have a number of difficulties with things like that. I mean, one, if you take the same wood crib, supposedly, right, and you ignite it multiple times, you're not going to get the same heat release rate curve. They're going to be similar, right? You're going to get similar growth, similar peaks. They're not going to be identical. So assuming you had perfect properties and a perfectly tuned FDS model, right, there's no guarantee that your model is going to match that kind of an experiment, right? I mean, hopefully you're reasonably close, but if everything is good in your model, but you're definitely not going to replicate it. So the idea that, right, changing the mess resolution in this particular case sort of pulled his results further away from the experiment, right, but you don't know how that single experiment relates to the mean behavior if you had repeated these tests a number of times. So one sort of, you know, issue there is, right, you don't necessarily know, right, what's a good answer, right, just on the basis of a single test. And then going into it, then there's also, right, you know, if you're just doing two grid resolutions, you've not really demonstrated whether or not you sort of grid converge to some, you know, steady answer. And if you had gone finer again, right, I mean, there are things that happen at different length scales in the solution, and maybe the answer could have come back up a little bit, you know, if you had done a third grid resolution. With this two, you know, you've not really established anything about, you know, grid resolution. I think it's interesting to even move back a little bit. Why would one expect that making a finer mesh would improve the accuracy of the simulation? It's quite interesting to start thinking about the fundamental concepts of CFD. I guess we all, to some extent, understand the implications of the impact of mesh and the discretization of space and time on the results of CFD, but I wonder, I see it all the time people would associate smaller meshes with better results and it's always finding as large a mesh as applicable. However, on the contrary, I also have the feeling that it's not size that matters, it's to what extent your mesh or your numerical domain is fit for the problem at hand, for the scales of the problem you're solving. So I think this is an interesting direction. Why do you think we associate finer mesh with better solution? And is this something we should associate? I think a lot of times when people think about CFD, their mind sort of goes to the fluid flow solution. And there, you know, there sort of is this expectation that if you start with a really coarse mesh, start making it finer and finer, you should get better resolution of the flow field for a set of known inputs to the model. And so I think this idea comes from that, that if you're not fine enough to sort of resolve the important length scales for your flow, as you get that resolution to those length scales, hopefully you should get a better answer. But in this case, we've got also this pyrolysis problem going on. So one, are any of the resolutions being used here adequate for trying to predict this wood crude? I mean, predicting pyrolysis is still very challenging in any of these models. I mean, just measuring the parameters and how you get these engineering grid scales to really give you good flame spread when you're not resolving flames. There's a lot of ongoing work there. So there it's not necessarily clear that in all cases reducing grid resolution is going to make things better. There are within FDS some correlations related to heat transfer and other things where those don't always, at this point, transition well from coarse resolution to extremely fine resolution in all cases. It's definitely something we're working on, but it's not a solved problem. I really wonder from your experience as a developer, how far did people go with mesh sensitivity? How small meshes have you seen for real problems? Has anyone taken FDS to sub-millimeter scales? Sub-millimeter scales, that's only been single flame, like a slot burner. Some domain like that where you're only talking about centimeters of domain size. On any sort of engineering-type problem where you're dealing with a compartment, I'm not sure I've really seen anything in a paper or any presentation where grid sizes have dropped below more than a couple of centimeters on that scale. It's not a cheap thing to do because it influences not only the number of elements, but also the time step. So there's a big cost associated with dropping the mesh, which is also the reason why many people would try to get with as coarse meshes as they can. In many cases, I see mesh resolution studies performed just to have a reason why I have picked the coarse mesh, where the choice was done even before the mesh study, because that's what you're okay with, your calculation time. Now, to think about the flow problem, where would you need a fine mesh? I mean, even if you consider a system like a pipe or something, the places where you have high gradients of variables, like velocity or pressures or sheer stress or something else, is the boundary layer, like just at the walls. In the middle of the pipe, you can go with a very large numerical elements and that will be good enough. There's not much happening there. You have an average flow. So these tiny meshes are often needed for the boundary layer problems, which let's face it, we don't have that many in fire. problems, which, let's face it, we don't have that many in fire. I mean, we're working with fairly low flow velocities, and in terms of compartment fires or burning in buildings, you deal a lot more with entrainment and free plumes and some small velocity flows caused by inlets, outlets. From your experience, like vast experience as an engineer, did you ever felt need to really explore this super fine mesh for a real real world project problem um yeah i think the cases that i've encountered where you sort of need to pay a lot more attention to the mesh or where you're where something important you're trying to get deal with is flow through an opening as being an important part of what it is you're trying to do with the model. That flow is being purely driven by points and not by some kind of mechanical ventilation system, in which case that defines the flow. predict flow through an opening through like a doorway or a window or some other small orifice, if you want to sort of get some reasonable answer in terms of the pressure drop or the flow, I mean, you can't just put a couple of grid cells across the opening. You know, you've got to be up, you know, sort of double digit kind of cells if you want to start talking about getting, you know, something good, whatever good. Because in this case, again, you have the boundary layer problem. There's an edge which creates a disturbance on the flow. To capture that flow narrowing in, you've got to give the model some chance to resolve some kind of vortex on either side of that opening and in the opening. And that minimum number of grid cells you need to do that. You know, in cases where you're really trying to define flow through small openings, you know, I think where that's a critical part of your answer, that's a place where you might need to pay, you know, attention to grid size. You know, there aren't necessarily a lot of problems for us where that's very important. Most of the time are people using a lot of the usage of things like ventilation systems for fire where they're mechanically driven, very few cells across the opening because you're letting just whatever it needs to make up what you're exhausting elsewhere. So in cases, I mean, that was the topic of, I think, 2014 or 15, everyone was dealing with JetFans. And I think I remember issue tracker being flooded by JetFan topics, like my FDS does not solve JetFans, I don't have good results. Yeah, definitely, you know, you need fairly decent, I mean, if you want to sort of really resolve that flow in the core of the jet and get something that looks correct in terms of the penetration distance of a jet, you've got to be putting like a couple dozen cells across your jet at least to get that. And that's something I've seen in some work I've reviewed where people are doing like a slot diffuser in a ceiling, and they put one crystal across the slot. And it's like, no, you're just not going to get anything meaningful out of this. To the listeners, some context with JetFun systems, that is a thing. Still is a thing. It's a system often used in car parks, and because of how this system operates, it's fairly very difficult, if not impossible, to design this system without any CFD, because these flows are very unpredictable. Like, they are predictable once you're in the know-how. Some things are what you're trying to get out of the model, if you're looking to understand that near-field behavior near the edge of the jet fan, I mean, yeah, you're going to need really good grit. But if all you're trying to capture is sort of some reasonableness with the bulk entrainment of air into that jet, that may not require the same resolution. Yeah, because... That's something you've got to think about when you're jet, but that may not require the same resolution. That's something you've got to think about when you're building the model. What is it you're using it for? And that your grid size may change depending on what answer you're looking to get out of the model in terms of what grid is good enough. Yeah, I think back then there were some changes done to subgrid scale modeling and FDS. And I know the different turbulence models were rotating in and out of FDS, some variants of LES, obviously, to help solve this JetFun issue. Because there was a large group of FDS users who relied on this software to just do their job, because that was part of their job to simulate car parks. And we were among them looking a little bit from the side. But it was a really interesting time and a discussion. Rainey put a lot of effort into investigating turbulence models. And then there was also just the issue of how you treat, how you treat what happens at sort of an edge in your domain. And that's a tricky problem as to how you formulate that. And there's also a lot of effort done into trying to make that better. better maybe let's try also to maybe you can try to to explain to the listeners like what's on the level of the software does changing the mesh size due to the solver because you have a bunch of equations you solve and they don't ask for the for the cell size specific. What does really change in the solver from the solver perspective when you change the mesh? Is it just the size of the element that's being solved, meaning you just gain more resolution, like it's more grained, or does the physics change in a way? There really isn't any change in the physics in FDS as you change mess size. It's mostly resolution. We do have in FDS these sort of different modes of operating FDS, right, where you can be DNS, LAS, VLAS, and SVLAS. But those are user-selectable modes of operation. And roughly they're sort of intended to correspond to grid sizes, but we're not having, you know, FBS tend to choose on its own, which is appropriate to use. And so if you do change that mode, you change some of the underlying physics. For example, in DNS, there's no correlations for heat transfer. You just use the actual temperature gradient at the wall. And if you pick SDLS as your mode of operation, FDS just treats everything as constant specific heats without using any temperature-dependent functions or things like that that change. And some of the ways that mass advection and things like that operate in the model change with mode. But as long as you haven't changed the mode, changing the grid size doesn't really change. Other than there are some sort of, like, for example, some of your initial time steps are tied to grid size. That changes when grid size changes. But other than some basic things like that, changing grid size doesn't change the equations being solved. Yeah, but still, the size of the grid is then used to determine which part of flow is solved directly with LES and which part goes into the subgrid submodel, right? Yeah. I mean, the only place into the subgrid submodel, right? Yeah. I mean, the only place where the subgrid models would sort of change is, you know, if you're in a gas cell, it's adjacent to a wall cell. You know, a wall model will be invoked for friction along that wall. And obviously, as you reduce grid size, which cell that happens is gets smaller, you know, because your cell wall is smaller. But, you know, the actual, but it's the same equation in use in that case. I always thought that if I go to cores, like the bigger and bigger chunk of my turbulent flow will be solved with this like one-dimensional models instead of Navier-Stokes equations. So, and I guess it's probably true when you go to very, very large grid scales. And I also, as we are still messing with the grid, there's also one thing I must ask, and that's the D-star criterion. And it's something people love to refer to when doing their analysis as the ultimate condition for which the mesh was chosen. Maybe you can give the listeners a background on how this even came to life, if this DSTAR must be, if I'm not wrong, between 4 and 16? Is this the range? That sort of came out early on in FDS. Part of it was there was this effort between NIST and the Nuclear Regulatory Commission to investigate different fire models for use and risk assessment. And so one of the questions that came out of this, the documents that came out of this went before the Advisory Committee on Reactor Safeguards, which is this panel empowered in the U.S. to oversee the Nuclear Regulatory Commission. They had some questions about, well, how can you evaluate sort of goodness of grids? And that was also coming out of other users. So a lot of the problems that were of interest there were problems where you're mostly looking at layer development. You know, is a plume impinging on some piece of equipment or are you making a layer that's hot enough to damage, you know, cables or something like that? So this D-star is just basically, it's a scaling quantity related to basically sort of the length scale of the plume. And so the idea is that, well, if you have some reasonable amount of cells across this length scale that you should get plume entrainment okay in your calculation, right? So if your plume entrainment is okay, your plume temperature should be okay, and the rate at which your layer drops should be reasonable. So it came out of that, and that 4 to 16 was really came out of that was the range of resolutions that were being used in the validation suite at a particular time. You know, this is the grid size range that we had used. And look, the validation results, results are reasonable. So, you know, this range was noted in the guide as being the range that was used in the validation guide. I think people took that as gospel, right? That as long as you have this D star or DX in this range, that you're always good. And that was definitely not what it was intended to be. And actually, you know, the current guides remove some of that language. We still note the D-star quantity, but they definitely try to explain that it's not a one-size-fits-all solution, but this sort of this myth still seems to have stuck around. You know, that this is what you need to do. And, you know, I think, you know, the sort of the DSTAR 4 to 16, if you're really just interested in a smoke layer in a compartment, you know, I think it's a reasonable rule of thumb. I mean, you still need to look at your results and, you know, make sure things seem to make sense for you. But if you're doing anything else, you know, if you're trying to predict, you know, radiation to an object really close to fire or trying to predict flame spread or anything else other than just really plume behavior, you know, you should be putting some effort into making sure that, you know, you're using an appropriate grid for what you're doing. I must say it is very convenient. As an argument in your publishing, I must say I am guilty of it. Once I had a really rough review. It was really annoying and really rough. And the reviewers were constantly asking about the mesh sizes and mesh... Even though we did mesh sensitivity and everything. But they were very insistent. Are we sure these meshes are okay? Eventually I said, yeah, we now check that it fulfills this criteria. So it's good. And they were, oh, okay, then it's good. So I'm guilty of it as well. But I appreciate that you've mentioned it was made for layer calculations, plume calculations. That's completely different fire physics than fully flash-over compartment or analyzing a false flow in road tunnel. These are like from a different planet. And now you've mentioned you should check your mesh. I wonder, what's your idea on how a good sensitivity study should look like? Classic approach to a grid study is to run at least three grid sizes, roughly having or something like that. That can be difficult to do for your real problem. Oftentimes, you know, if you've got some tunnel simulation, you have some fire revolving over, you know, a period of tens or, you know, hours or something like that in the tunnel, you know, it might be difficult to afford that finest resolution, which you can, I think in most cases, you know, pick maybe important points in time in your simulation and run, you know, a steady kind of scenario and compare the results of the grid resolutions there to see if things are not changing greatly between the, you know, the grid sizes you're looking to use in your final answer. I mean, there's some discussion in the FDS guide, and there are some of these sort of quality metrics that we can output. And those can be, you know, useful to look at some idea of, you know, like this mean turbulence resolution quantity. You know, if you're hitting that in your region of interest, you know, that your grid may be reasonable. But, yeah, it's tricky to sort of specify, you know, one-size-fits-all approach. Sometimes you also get very confusing results over the grid study. Like your coarse mesh gives you a value, then a finer mesh would give you a lower value, then the finest mesh would give you a higher number than the coarse mesh. And you are left with a scatter that doesn't converge in any way. And it's kind of confusing. What would you do then? Like add many more points or move back and change something else? It happens a lot of times. Yeah. One of the things to look at is, you know, how much are things changing in your results? I mean, one of the things you have to understand, right, is that, you know, FDS as a model itself has an uncertainty associated with it. And in the validation guide, we estimate what those uncertainties are for various quantities of interest, like bloom temperature and layer height and species concentrations, et cetera. If you do have this case where your coarse grid is a higher value, a medium grid is lower, and then the fine grid comes back up, if those are all essentially equivalent in terms of the uncertainty in FBS, it may just be that model uncertainty showing up in the results. and you can pick your medium resolution or something like that. But if your answers are really changing significantly, then maybe when you think about more detail about what it is that you're modeling with FDS, especially if it's something where you're dealing with pyrolysis, I mean, there's just interaction between the solid phase and the gas phase is not a very certain thing right now. So there I definitely notice strange things can happen as you move grid size around, more so when it's just a specified fire. So when there would be a solid fuel and you would use pyrolysis model to generate the gas fuel from that and burn it, you would expect there's much more interesting things happening than when you just specify a burner vent and just drop the fuel in it and burn it. Yeah, I've definitely, when I've tried to play around with, you know, pyrolysis or use predictions, I think I see more of an impact there with the grid. You know, I think some of it is that you get, you always wind up, right, you measure your properties of material with some very small scale test. People comment around now, maybe some kind of like TJ, DSC, try and get reactions and specific heats and, you know, then maybe do a cone test and try and, you know, predict that. And then you go to the full scale. But there's definitely things don't necessarily translate well in scale that way. So this almost always seems to be the case that there's some tuning needed of your very small-scale experimental properties to get something reasonable at a larger scale. But then right now you've sort of got properties that are grid-dependent in some sense, and we don't necessarily have a really good handle on that in all cases. This is very, I mean, sort of how the radiation model and convection, I mean, those have some grid dependencies in them that we have not gotten rid of yet. Understood, yeah. And when you try to apply these small test parameters into real-size model, but you don't know the outcomes. When you're modeling an experiment, there's always something you can refer to. You've mentioned there's difficulties in that, and maybe we'll come to that in a second. But when you try to model something that you do not have an experiment for and CFDs your only tool, like you want to model a flame spread in a compartment using solid fuel and some sort of ignition methods, is there a safe way to proceed that you would recommend? I don't know, maybe starting with a smaller model or working out the literature example, you know, and then moving from that. Or maybe just don't touch that because you have no idea what you're going to get. Where are we in this kind of modeling? Yeah, if you've got properties for the same material that you're working with, I think you just got to put a lot of effort into building your calculation up, right? I think you just got to put a lot of effort into building your calculation up, right? You just can't do a very small-scale test, take those things, and plop it into your final FDS simulation and run with it. You know, I think you need to do the very small-scale simulation and maybe simulate some simpler model problem, you know, with those properties and look at the results. I mean, there's a lot of times you can look at, and we sort of have some idea, sort of what we kind of expect in terms of burning rates and flame spreads for, you know, a lot of items. So, I mean, if you go from the small scale to some more intermediate scale, and, you know, if that doesn't look right, you need to go back to the start. But if that looks right, you know, then you can move on to the next scale. So, you know, I've done simulations looking at trying to assess cardboard box commodity in warehouses. And there, right, you're really fire growth, you know, when you're trying to look at when sprinklers are operating and all that. And that's really cardboard that's driving that. when sprinklers are operating and all that. And that's really the cardboard that's driving that. When there, there is test data at various scales where people have tested just the cardboard or a stack of a couple of boxes or a rack with 2x2x8 array or something like that. So there you do have the opportunity of you can try and figure out some properties, go to the single box, the couple of boxes, this rack. And on each of those steps, you're getting something that's, you know, reasonable in terms of the heat release rate measured, I mean, you might feel like you have some confidence that you're okay for your larger problem. But you do need to probably put a lot more outputs and really look at what the model is doing and making sure that things are okay, right? You're not getting burning rates of six kilograms per meter squared per second or something like that when it just isn't plausible, who's never seen other than really exotic fuel, perhaps. Yeah, someone once said that bad simulations, bad simulations where it doesn't look look real but it's even worse when it it looks real but it's it's completely untrue so that's a kind of a danger we we face and now you're being associated with ul i guess there's a whole career to do in modeling plastic cups for the spring that's a whole that's a whole industry there man and i i appreciate your approach building your own skill set and building your own, in a way, expertise, but also, different one for the carpet, for the wardrobe, for every single item. the complex set of fuels, I think I would settle on some surrogate fuel that can literally, like, average them all because in the end, it's going to be gases that will be burning. And if I introduce that much complexity into my model, there is no way in the world I can handle uncertainty of that. Yeah, I think be careful sort of complexity for, you know, complexity's sake. Especially if you're the default combustion model, right, in FDS, right, you just specify your fuel without kinetics, that you're just a fast reaction. At that point, there's not a lot of benefit to having all these different fuels in a lot of cases, unless maybe there's extreme differences maybe in their sooting behavior that for some reason was important to count. Low sooting fuel at the start that ignited some high sooting object later. I mean, in that case, maybe then you'd have two fuels, right, the low and the high. But if things are kind of similar in terms of their hazard that you're trying to evaluate, I don't think there's a lot of benefit in making things too complex. It all just kind of gets, as you noted, right, everything's all fast reactions. It all just averages out together anyway. So at that point, just make it the average and save yourself the time and effort. But then the other complexity in these kinds of problems is, you know, we don't have, right now, solid objects don't disintegrate, right? I mean, you know, the solid cube might use up its fuel and get removed in a calculation, right? But for something like an upholstered piece of furniture, we don't have the foam melting and dripping into a pool beneath. There's some hooks we put in to sort of support people trying things like that, but it's kind of relatively new. But getting things like eventually, you know, it sort of collapses into some pile of rubble that burns. I mean, those sort of later time things just aren't there. And, you know, I'm not, that's definitely, that kind of stuff is a fairly long way off. So for predicting pyrolysis, I think you also need to be careful about those kinds of behaviors. You know, if your time range of interest is where things are still relatively intact, that you don't worry about those kinds of issues, you might be better off than if indeed you've got, you know, your object melts into a pool and the pool burns, you know, then maybe you need to think about how you're, you know, approaching the problem that you don't want this solid object to burn as a solid. You need to think about some other way of modeling it. I think we also do not really appreciate the complexity of fire. I mean, this podcast could be called Complexities of Fire because this is in essence what we are discussing here for more than a year. And it seems we're nowhere close to an answer of how complex a fire is, because with every episode, it's more and more complex. And people often would expect a single answer from a single experiment, just burn the building once and understand how does this building behave in the fire. Well, as a simple example of one of the recent experiments we've done, just a normal compartment fire with cribs as a source of fire of the recent experiments we've done, just a normal compartment fire with Cribs as a source of fire inside the compartment. And we had a Crib collapse and it felt out, it produced a slightly different size of fire, which triggered the flashover. And this is like, you would not model a Crib collapsing in your CFD model. And even if you had the perfect representation of this compartment, this crib, the fire dynamics in it, you could have modeled that and would say, okay, flashover does not occur in this compartment based on the crib behavior as it was intact. Whereas when it collapsed and suddenly it was like three times the size in space and the air could then train the middle part of it better, we exceeded this point and the flash-over would happen with the same mass of fuel in their slightly different configuration. But a tiny thing that changes the outcome. And we often expect a single answer from a single experiment where it's just a point in your uncertainty matrix that you don't know. And I think that's something that goes back to this initial question of why fine mesh did not improve the accuracy of that simulation. Maybe the simulation was accurate already as it may have been within the uncertainties of that experiment. What do you think? I mean, these uncertainties add a lot. Like, what kind of uncertainty you could expect in a simulation and in an experiment? Yeah, well, I mean, for something like that particular experiment, you know, there's a tremendous uncertainty in those material properties that you're specifying within them for their action. I mean, things like density is something that's relatively easy to measure, right? I mean, you can measure volume, you can have a scale, and you can divide one by the other and get density, and those are relatively, for many materials, right, those are measurements that you can do fairly easily. But when you start getting into all the kinetics, right, things like the specific heat and the activation energy and all that kinetics, right? Things like, you know, the specific heat and the activation energy and all that for reactions, right? I mean, those aren't things that we can measure, right? It's not a tape measure that you put out to measure the activation energy, right? I mean, what we do is we apply some heating to some sample and measure some temperature response. And then we have a model that we apply to get these parameters out of it. And so all of those things have uncertainties, right? I mean, that heat flux or heating rate input has some uncertainty, that temperature measurement has some uncertainty, and that model that we use has a model uncertainty. So each one of those properties could have 15, 20, 30% error or something associated with them, perhaps. And that can definitely make a big difference in the outcome of something like fire. We are predicting the fire growth as to whether or not your fire grew as quickly enough to get it involved, get the crew involved at the right rate. So I would expect that just that simulation he was doing probably had 25%, 30% kind of uncertainties, differed in terms of the temperatures and the heat release rate, at least in terms of the data that the user had posted by 15%, 20% or something like that. So in that case, if things differ by less than the uncertainty of your experiment, you really can't say one is better. And that's something that I don't think it's always realized when people just look at one or a very small number of experiments. If you don't really sort of understand what that experimental uncertainty is, it can be difficult to just do a couple of model runs and go, well, this one's clearly better, which they're not necessarily better if they're both well within the uncertainty of the test. At that point, you can't make that distinguishment. You can say they're different, but you can't say that one is better. I also think many times researchers confuse uncertainty related to the capabilities of their measuring systems with uncertainty that is relevant to the position of the measuring system, especially when you're measuring temperatures in flows, not forest flows. Yeah. I started at the FSRI, was looking at some furniture tests at Northbrook, and they weren't quite getting the FDS predictions to match up with temperature predictions that were measured. But in this case, there was a single rake of TCs. And you go back and you look at the test video that was taken, and you can see that there was a little bit of lean to the plume, right? So even though you put all the effort into centering the rake over the burner, FDS, of course, is going to give you that perfectly straight fire if you don't specify asymmetry in your boundary conditions on the sides. But you can show that in FDS, right? You can put just one rake. You can put an array of rakes and go show that, well, if you're only 10 centimeters lean at 4 meters above the fire, you've changed this temperature now by 25% because you're off the center line now. So that needs to be considered in some measurements. It's more important in some areas than others. Like if you're far from the fire and you're sort of in a layer, I don't think how accurate that position is is not as important, unless maybe you're in an opening. I think it's important also if you consider something like ceiling jets. Like in a ceiling jet, five centimeters lower could be a difference of a few hundred degrees. That's a big, big, big difference. And then if you model it with any CFD tool, if you have like a 20 centimeter mesh, what you get in the end is an average temperature in your cell, which half of it may be a flame, half of it is not a flame anymore. So it's not necessarily, you are not comparing it to an average temperature measurement over the height of 20 centimeters. You're comparing to a point measurement in a certain position. Not directly. So it may be valuable to also go different measures than just temperature. I'm a huge fan of comparing the flows. The computational dynamic software is solved for, like, pressures, the flows. That's the bread and butter of it. That's the reason why we have these tools. So comparing these parameters in your experiments versus what you've modeled, it's a really good indication that whether the model is correct or not. How many times you've seen in the issue tracker someone reporting that their simulation crashes or they get very bad results just to figure out that they had like pressures of hundreds of thousands of pascals somewhere in the model because of some stupid error. And they never checked the pressures in their model. That happens all the time. I guess you see it a lot dealing with the issue tracker. Yeah. I mean, we definitely had a lot of issues where if you have a, because of the way the pressure solver in FDS works, if you have a fully enclosed volume, definitely were cases where that led to problems. I mean, Kevin did add in the last release this ‚Äì FDS now searches on its own to find what we call pressure zones, right? Sealed volumes that aren't somehow connected to an open vent, and each pressure zone has its own treatment of the background pressure. And that hopefully should go a long way to eliminating many of those issues. But yeah, but you do see still people posting cases where they've put a room or something in the middle of their domain, but they've not opened up any of the exterior boundaries of the domain. And they're like, well, my fire goes out. You have this little box and you've burned up all the oxygen. And again, I think that goes to people aren't always taking the time to learn FDS. And there it's more than just jumping into the problem you're trying to solve. It takes a long time to become a really good FDS user. And you should start with simple problems and playing with inputs and seeing if you're understanding when you change this and the simulation is this, does that make sense? And, you know, you should spend some time doing that before you leap into your massive full-scale problem. I would qualify FDS under easy to learn, hard to master, you know? It's very easy to set up your first simulation. It is very easy to set up, and I think there's good and bad for that. The downside is that it also means that you can get people who use it without taking the time to master it. I see much more good in it than the bad things because the bad things relate to people's character. If you're a person that would just take a tool you learned five minutes ago and then use it as an oracle, then you should be in fortune telling business, not in fire research. And I also think that the fact that it's easy to learn and there's not really steep requirements to start. It's a great learning tool. And if you learn and appreciate what's happening and appreciate the learning process and really put your heart into it, trying to understand what's happening, you can really learn a lot. Even now, I have people in my office who would like to learn Ansys, which we're using in the office. And I tell them, okay, first step for you, if you've never done FHCD or anything, you should learn FDS. I mean, that's the first step because you will understand the problem you are trying to solve with the numerical model. And once you are good with that, then switching the tool into another tool will be a very easy, or maybe not very easy, but it will be an easier experience for you than just stepping into a very complicated tool from the start without this knowledge you get by learning FDS. And it was my path into engineering, learning FDS first and it was my path into engineering, you know, learning FDS first and moving into different softwares without any particular reason outside of that, the work environment I entered used this software. So it was not a choice that I don't like the software anymore. I need a different one. It was just a matter of the environment I've entered. A good sort of comment you made about FDS and then more complex tools. I think the same thing applies to just using FDS, right? I mean, there are, in many cases, problems that people using FDS for, there are hand calculations or a zone model can provide you some insights. Maybe not the detailed answer that you need, but they can sort of give you some insights as to where you think that answer, you know, lies. And I think, you know, you see that also sometimes in discussions on the forum and in issues, you know, where people sort of run a simulation and get some strange results and, you know, sort of throw their hands up rather than maybe trying, you know, to simplify it down to the point where they can use a correlation or a zone model and see is that answer making sense or what seems to be off. I think there's definitely a lot of value in before you go right into that complex tool to learn some simpler tools so that you sort of build some experience history in your mind as to the kinds of answers you expect for different problems. And that way, as you start getting to more complex things, you have something to rely upon in terms of the experiences. Is this answer looking reasonable or do I need to rethink this? I really appreciate this answer because it also answers a question that was posed in the middle of the podcast, what do I do when I face a very complicated problem which I don't have experiment for i mean this is also a good answer that you model it with different tools and at least see the are you within the scales you know are are you like 10 percent off or or thousand percent off because that's a very good indication where you should be more or less so this goes with experience and and i really really appreciate mentioning other tools in existence because we are living in this very, very weird age where sometimes it's more difficult to convince someone they don't need CFD than convince them they need it. Yeah, and I think that's also a challenge at times is, especially sort of in the consulting world, you sort of client wants that pretty picture but you know there's a simple tool that's going to give them as good or maybe even better i think there we got to do a better job sometimes of educating clients about what these tools can and can't do and not just always do what they ask you know sometimes you have to take the time to educate them, to get them to understand that, no, in this case, all we really need is this hand calculation, and you're getting a better answer. Yeah. Now, because we're running out of time, and there's one more thing I need to ask you. How's life at the Fire Safety Research Institute, man? That was a twist of career for you, 20 years at Jensen Hughes and then moving into UL FSRI. I guess you're thriving there because I love this organization. And when I saw the announcement that you're moving, I thought, oh, what a perfect match. You're going to have a great time. So are you having a great time? This is a really great organization to work with. There's a lot of great, great, great people here to work with. There's a lot of really interesting experiments they've done over the years. So there's this sort of this data that's sitting out there to analyze and model and understand. And it's also, you know, really nice to be in a position of being at an institute where we're essentially self-funded. So that definitely helps. It eliminates a lot of that burden of the grant and proposal writing. It just lets us focus on doing the research and doing the work and gives us more time to work with our stakeholders and understand their problems and how we can help them. Yeah, so it's been great. I feel kind of like getting the playground. Nice. What was the number one most exciting thing you're working on now, unless it's top secret? I guess one of the more interesting things I've been starting to do here, work with Dan Madrakowski. They do a lot of work with fire service. They help out at times where things have gone wrong in the fire ground and help fire departments understand, are there changes in tactics or decision-making that might help reduce injuries while fighting fires? And so I've been helping him with sort of a mix of, you know, it's modeling, it's investigation, right? You're trying to piece together photos and witness statements and videos, and then try and couple that with modeling to help understand how their actions have changed the fire environment and then, you know, try and understand where are decision points where you might have had an impact. So it's really kind of a fun detective kind of problem to do. That's the most appreciated research, and there was a lot of that coming from UL, and I guess we will see a lot more. Jason, thank you for your time in here. Mostly thank you for your efforts in working with FDS, developing FDS. We didn't talk about HVAC solver in this talk, but I certainly appreciate all the work that has been put into that part of that model because it's brilliant. I would love to have a clone of that in Ansys. Seriously, I believe that copycatting is the highest form of admiration. So we may actually go there. But I... It would be too difficult to do. It's just the one file, basically. It's a brilliant idea that's executed even better. So I appreciate that. And I appreciate you responding to issue tracker daily. I think it's pretty important for us developers to, in the extent that we can, try and interact with the community. It makes what we do, the better we understand the problems people are having and the better we can make the model. So that interaction is very valuable. Users are great about breaking things, right? They try things at the bottom. The end result, we get a better model. You get an issue and the issue track, and it's like, how did this ever work? Without even thinking much, I can give you a list of like 20 commercial software packages for which I pay a lot of money, which have worse support than the FDS that is provided for everyone. So I highly appreciate that. And on your hands, props to the whole FDS teams. Okay, Jason, thank you so much for coming to Firesense show. It was a great time. And see you around. Bye. And that's it. I hope you've enjoyed that. You cannot praise the FDS team enough for maintaining the issue tracker. Like seriously, from outside, it looks like a hell of a job to do. It's the questions being asked constantly and people are like endlessly running into problems with how the FDS is used or problems with their modeling. with how the FDS is used or problems with their modeling. And these guys really try to give them solid science-based, fact-based, evidence-based answers. And you cannot appreciate that enough. With Jason, we went through the issues with mesh sensitivities. What does it mean that you increase mesh sensitivity? How to measure it? I think that's a practical takeaway on how to apply mesh resolutions. I love that we've touched the topic of D-star and debunked some myths around it, why having D-star between 4 and 16 is not enough for most of the simulations. Maybe enough if you're simulating compartment and looking into layer, but for most cases that's definitely not sufficient. And I think this had a lot of practical takeaways like that. So I really hope you've enjoyed this episode. I've learned a bit myself. Now I know that I need to get more people from the FDS team in the podcast. It's great to talk with them, and I hope to be able to talk with Jason again. I mean, Jason is the person behind the HVAC solver for FDS as I brought up in the end of the interview. I have not talked with him about that, but I really like that solver and how it can be used in analysis of buildings. So maybe I should bring him back to the podcast to talk about this particular issue. And for the end, if you're still here with me, I appreciate you a lot for listening to the show. If you have ideas on what questions to ask and to whom to ask them, I'm here for you. I'm doing it for you. I'm doing this exactly for you. And if you have a question in mind and you have a person in mind, let me know and I'll get it done. I'm the voice of my audience. So if you tell me what you want, I'll get it delivered. So you have bigger impact on what this podcast is than you think about it. Send me an email and we'll get stuff arranged. Thank you so much for listening and see you here next Wednesday. Bye. This was the Fire Science Show. Thank you for listening and see you soon.