Introduction by Enrico Ronchi

I am, and here we start recording this. So as you know, my name is Enrico Ronchi and together with Erica Kuligowski, we run the human being fires permanent group of the ifss, the international association for fire safety science, and one of the activities that we've been doing within this group is indeed to arrange this webinar series, and we had this is the fourth event, so we had three events before two on the publication process and one on terminology in the vacation field. 
But today we have two fantastic speakers. We have dr shilaj zhao and dr guetta custer that will talk about machine learning in a vacation research. I'm very excited about this, because this is really fascinating topic. They will talk about what we can do and what we cannot do. So that's also the interesting part of this with machine learning and as mentioned. 
This is the fourth event of our webinar series and you can join in to get updated on what are our future events or if you want to contribute or if anything to say about what we arrange. There is a link here that artur is a phd student working with me on this here in university, has put in the chat. 
So if you want to sign in and be updated in the next events, please do so, and today we have two speakers. The first one is guerta and gerta teaches at the munich university of applied science and she does research modeling, simulation, computational sites and algorithms, and the current research projects are on social, cultural, pedestrian dynamics, uncertainty, quantification and surrogate models. And the second speaker for today is chile. 
Zao and chile is an assistant professor at civil and coastal engineering at the university of. florida, where she leads the smart equitable resilient mobility systems lab, the thermos lab and a war, focuses on developing and applying ai methods to model and understand human behavior in normal and emergency scenarios. 
So this webinar is being recorded so that whoever wants to re watch it, or if you know someone that might be interested in watching it afterwards, we will put it on our youtube channel. So I stop the sharing here and I will give the words to gerta so she can start a very interesting presentation. 


Dr. Gerta KÃ¶ster

So welcome guertin, welcome chile, and the floor is yours. Thank you very much for the very nice introduction. 
I hope you can hear me well and I will not share a full screen because I have this new magic stick that I will use to. Well, welcome here, hello everybody, and my talk today will be. on machine learning in education research, and i'll address the questions. What can we do and what can't we do, and I have goals set for today. I would like to have you take something with you. 
First of all, I would like you to be able to put machine learning into the context of modeling and stimulation of evocation, evacuation and fire dynamics. So I basically assume that all of you have sort of a background in evacuation or fire dynamics and maybe a little bit of modeling experience. I would like you to have seen promising areas of application of. 
There are many of them, and I would also like you to see to have seen some key limitations, and there are also many of those. I will start right with it. What are typical problems in evacuation dynamics? Many of you know these pictures here, like this. is a bottleneck and there are many experiments, lab experiments on this, and usually we are interested in evacuation times. 
For example, one parameter of interest how long does it take for everybody to reach safety? It's a typical question in evacuation dynamics, very often addressed through modeling simulation. But there are no questions emerging, and one of them, for example, is if we want to do a very big stimulation like this is an overpass in at a train station in switzerland, and we wanted to simulate that, and for that we needed to know where people came from and where they would go to. So what we needed were original destination matrices, and they don't fall from heaven. 
So we had to learn them somehow, and this was an area where we actually were forced to use new machine learning technology, and there's another area that chile will. talk about later on. For example, pre epic evacuation times are very important and we would like to learn which factors influence people's reaction times. So hold on until my talk is over and actually we'll talk about that. 
So I would like to put this in the context of classic modeling. So what do we actually do while modeling? I listed a number of steps here, and it always starts with observation. Here we observe some sort of phenomenon. Then when we see it, we put forward a theory about the phenomenon and nowadays we implement it as a computer model. Then we test our theory against new experiments. So we run a simulation experiments. 
We look at the outcome and see whether it matches new data and we call this validation. And if we get a good match, we are so happy and trust our program, our model. and then we run what if scenarios to predict outcomes, for example evacuation times, so that we plan a building and according to rules. And what you see on the right here is a typical picture. You've seen those a lot. 
This is happens to be a simulation done by marion, who's one of my former doctor, fresh doctor in computer science, who ran these simulations to see what the what, the width of the bottleneck and how it influences the evacuation times. And you know that there are many models to do the job. This is with done with the optimal steps model, but there are others who also do the job. 
What I want to get through here is that we have a competition of models who do similar jobs. I always like to put things in one of those flow type of diagrams, so when we model. 
We start with a real world, we look at a lab experiment or a field observation and we observe parameters and from these we try to drive some sort of abstract model, maybe a graph, maybe a verbal description, most often some sort of set of mathematical equations. 
We call this operator operation operationalization, by the way, and with this abstract model we well, this is on a piece of paper, so it isn't really that useful unless we turn it into an algorithm which then we can implement as a simulation program. And all these three things this is trinity, by the way is what I call a model. Lots of things can go wrong. 
This is why we need a lot of verification steps so that we can trust our model. But let's assume that we're happy with our model, then we're also happy to play with it. 
We go back to the my magic stick lost its magic here and we can go back to playing with our simulation tool to vary parameters and to, yeah, see, see how our simulation depicts a virtual world, a virtual world that maybe reenacts what we've seen in the real world. 
Like we can compare here, which is the process of validation, and if we're happy with that again, we well we trust our program or a simulation model well enough to do predictions. That is the typical way and in the end, what we have is rules that help to explain a phenomenon and for me very important that is we as humans learn and we'll now look at what's wrong with that. Because I mean this sounds fantastic. 
It's working, it's wonderful and fun to do, but it has some disadvantages, and one of them is time. for some models. Well, if you think of physics, it may take hundreds of years to get a good theory, and I mean those of us who are modeling at the moment in pedestrian dynamics know that we haven't reached the end at all. 
There's a lot of competing models and we don't have the theory that covers everything, and even if we have it in a certain field where we find okay model, does the job. The implement implementation on the on the computer may still be slow or it may fail and we run into problems whenever we really try to apply our theory. 
And what is for certain is that we have no life prediction so far and, to be quite honest, I don't see it on the near horizon. No life prediction at the moment. So it's a bit of a disappointment and we all. 
Know that there's this exciting data driven approach out there that we can use maybe a neural network and let the machine learn and let machine predict what happens and maybe we can get further with that tries to pace. Then we maybe don't learn the rules ourselves. But let's see what we have to do to do this ai data driven modeling approach. I've just put together some typical steps. 
There's I mean this is a rough, rough overview, so there's a lot more detail if you really have to do it. But the first thing that you have to do is to collect data, and I'm not talking just a few lab experiments. I'm talking massive data. Any type of machine learning algorithm that you use is data hungry, so you will end up with terabytes of data that then you need to pre process for once it's. 
Much too much, so that your algorithm usually is not able to handle it. And for the other thing, you often have to obtain the grand truth. You need to know what's true so that you can train your algorithm. So you very often have to manually go through your data and label it. You have to reduce it so that it's of a lower dimension. 
There's tons of things to do, and once you've done that, you actually select a method. Now it you. Basically you don't develop it like we did before. You don't operationalize it, but you just take one that is there, for example a neural network or a support vector machine or random force. This process is called model selection and this model you then tune. 
You have to set certain parameters, which is again not trivial, and then you can train the model. on part of the data. It's important to only use part of your data, because if you train your day, your, your method, your model, very well, you might get a really good fit of this part of your data, but it might not generalize. 
So if you use it on a different set of data, you might see that okay, now, the fit is really bad. Usually you just you measure this in scores. 
You might get a wonderful score, close to one for your training data, and then when you do it on test data on the rest of your data, you get some like 0,3 or something, and that means that you've grossly overfitted your model in the tuning process and that you now I mean you need to find a good balance between generalization and good fit on your training again, not here. 
If that happens in a way that you're satisfied again, you're in a situation where you may be able to predict new cases. So a little bit of a different process. But, and on some points you have similar problems, and I again like to put it in one of those schemes and I try to make it draw it in a similar way so that you have a better comparison. 
We start again with the real world, and there we what we call what we observe is called a quantity of interesting, and we gather a lot of data. Instead of deriving rules or putting forward hypothesis, we just gather data, massive amounts, we pre processes, we select an algorithm, a model that we want to use, for example a neural network, we train it and then in the end, once we've done that, we've when we've tuned and. 
Trained our model. We have this. Well, basically what we have is the substitute model. This tuned and trained thing neural network, random force is a substitute model. It's a surrogate and we can a surrogate of the quantity of interest, depending on the data that we have, and with that then we can again validate and predict if we are happy with the situation. 
So there are a lot of similarities, but also very fundamental differences, and I would like to give you one example where we used this one of those machine learning methods to answer this question about the od matrices. What is an od matrix? It's an origin destination matrix. 
Just imagine that you have a lot of entries and a lot of exit in some scenario, and you need to know where people go and you put if this is one, two, three, four. your exits. You put it in a in a matrix, where you have those four entries and x's and then you, in this, for example, two you just look at well, two is probably not that good. 
This would be all people who walk in and out at the same entrance and exit. But and I think you get the picture so an odin matrix basically stores the number of people that go from this exit to that exit, and we had no choice but to use data driven approaches for that. 
Of course, we could also use traditional statistics, but, and so far traditional statistics have not been very successful in this area, especially since we wanted to do this in a dynamic way. We wanted od majors, not only over a long time period, but really depending on actual data. We did this without programming a lot ourselves. 
But we used standard software so I could learn, and when I say we, I actually mean marion did this for her phd project and the machine learning methods that we selected were linear regression, a linear map method and random force, a non linear interpretable method. The data that we got actually swiss data from a basel station and it was a density of a time series of density heat maps. This is what I mean here, right? 
You see, basically you had this overpass that I showed you before, where I mean people walked through this overpass and down to raid to the to the platforms. I hope you can get an idea, and this is basically what you see there and highlighted is the densities, and we got these densities actually from trajectory snippets. Why we did this extra work. Somehow I don't want to go. 
Into detail here, but it had something to do with the project that we are working on. But the question was: can we predict from a collection of density vectors? Well, we put these in vectors right. Can we predict the od matrix and this is a this is again the od matrix and this these entries. Here the color just indicates how many people go from this exit to the other as targets. 
We had this the od matrix and the crown ground truth was obtained by manual labeling. That was a painful process that a lot of bachelor students suffered through and very often this is something you have to do. It's a supervised learning task because we had I mean we had a ground truth. We had labeled data and we used part of it as a training data and we followed the test data and we. evaluated the results through scores. 
We used an r square score, which is very traditional both in machine learning and in regular statistics, and the bet the closer you are. It's between zero and one, and the closer you are to one, the better it is. And I have to make a confession here. We didn't get great our square scores. 
We got something like 0,6, for let me I mean I told you about this balance that you have to find between the training score and the test score, and we've got wonderful training scores and then allows the test scores. 
And if we wanted to have a balance with that, we ended up with something of 0,6.0,7, which tells you that our algorithms actually both of them were able to learn the od matrix, so artificial intelligence is capable of learning the structure of. that system, but we haven't reached the final goal yet, which is a much better quantitative prediction. The race is out. Do it better than we did. 
If you want to read more about that, it's actually under review or norm. It has been accepted by jstad. Our struggle, honest struggle, with ai, and i'll set you a new goal to do it better than we did so. I've already started to talk about the problems that we ran into when we did this od matrix estimation. Let's see what it what it is. 
So we basically I mean yeah, the idea of ai is very convenient. There's so much software out, but there were so many bugs. The first is: you need massive data and in many cases you don't have any data at all. Think about evacuation trajectories in a fire. There's no way you can get these. at least not in an ethical way right. 
So there is no way you can do this with machine learning and once, even once, you have data, it's not like you just unfill the data at the algorithm and you're done. Really you have to do a lot of pre processing. It's not trivial. You have to establish a ground truth. You have to filter, you have to reduce. You have to do a lot, put in a lot of computer science and mathematical knowledge. 
Then you have to select the right method which is non trivial. Again and once you've done that, you still have several problems with accuracy, like the overfitting problem I was talking about that. You get a very good training score, but when you try to generalize it, when you try to apply your method to data, that hasn't been that wasn't in the. training data, then you might actually get really bad scores. 
And then there's a final problem, which is a black box problem, which I really dislike. When you, especially when you take models like neural networks, you cannot look inside. 
You don't know what rules actually drive the network that you end up with, and I mean you've probably heard about these things that people don't get banned credits because they have the wrong skill, skin color or the wrong gender, or they simply live in the wrong street, and this is a bias that is maybe covered in such a black box system, and I especially in evacuation and fire dynamics, where we are talking about saving people's lives. 
This is something I'm not happy with. So black box machine learning processes are something that I'm very wary of in this context. So okay, so what should we do? what approach should be adopt and also a question that many of us of us have been asking: with the machine learning hype, will machine learning replace classic modelling? Will it be out of a job in another 10 years? 
And I don't know the answer, but I've tried to put together a little list where I sort of oppose the two approaches, the classic modeling and the machine learning. And the first one is as I said in the beginning: when we look at classic modeling, what we try to do is to find a model that explains something. 
So there's this a learning task, a human learning task involved, and when you look at machine learning at least at what most people do, you run a neural network or something like that as a black box, and that means that the machine learns and we humans remain stupid. there is one exception to the rule and I'm putting a lot of hope in that which is so called explainable artificial intelligence. 
These methods and actually random force, which la is going to talk about later on is one of them, are open to at least scrutinized to an expert, and so we get closer to this old idea that we humans also want to learn, and it's not easy. These are not triggers, so it's not like everybody gets an explanation, but at least it's a I think it's a step in the right direction. 
The next issue is when you look at modeling is at least when you look at locomotion models. You know there are so many competing models in classic modeling, social force models, optimal steps, model cellular automaton, gradient navigation models for all sorts of models, and in some areas they'll all do the. job quite well and others some fail, others don't. 
So you have this competition of explanations, this competition of hypotheses, whereas in machine learning, when you talk about a model, it's actually a technique, an algorithm that you're talking about. Model selection I e a random forest is not closer to the actual phenomenon than a neural network. These are these are neutral, if you like, and you select them for but maybe because they're your favorite toy or they are simply your poison of choice. 
You're familiar with them. You don't usually don't select them because they already are have a have a are very suitable for a certain problem in terms of phenomenon. And then we have this data observation. In both cases we need data. Without data there is nothing. No modeling without data, but in classic modeling you may do with comparatively little data because. you have your own intelligence that helps you derive. 
That helps you put forward hypothesis and then check them, run through lab experiments and things like that, whereas in totally different, in machine learning, you gather a lot of data. The advantage here is that you may and you may detect patterns through a machine that you wouldn't see if you just looked at them with a naked eye. And then there's this feeling about progress in. 
I don't know how you think about it, but sometimes I have this idea that every pad, every tjf conference, we're talking about the same things and progress seems slow. You make two steps in advance and then you go one back, whereas in machine learning there's so much activity and it feels fast, but I'm not sure about whether it's always very goal oriented. There's a lot of things happening, but i'm. 
Not always sure that they bring us closer to saving lives. So this is I put a question mark in both cases behind my little yeah entries, because I'm I haven't decided yet. Is it slow, is it fast or is it just an illusion? So let's see what's next. What is my opinion, at least about what's coming? 
I think that classic models will be complemented with machine learning and vice versa, which is actually a very happy situation. I think that there's gaps in classic models that need to be filled by new techniques and that machine learning techniques are just the thing that we need, and this on the example that I showed you with the original destination. Matrices is one of them. 
We couldn't, we wouldn't be able to do it without machine learning, and then what I really find also a very charming idea. Is that we might use artificial data to train machine learning algorithms to fill this gap in machine learning, where we cannot get data, but we might want to use the algorithm for whatever reason, because it fits in our surveying system or whatever is the reason, and then we could use the artificial data to actually get our algorithm trained. 
Of course we need to be careful about this, because only a well accepted, well validated classic model can be used to train a machine learning model. And then finally, my great hope is an explainable ai that is some sort of interpretable method, such as random forests, but also in operating form, models that I'm personally very interested in, and they would start to improve the black box situation. 
So maybe if we improve the accuracy of these methods and get them closer to what neural. networks. Do we then end up with something where we have the advantages of machine learning and the advantages of actually learning ourselves? This is my hope for the next 10 years. If you want to read more about all this stuff, this, etc. 
That I used, and actually I'm exactly on my 27th at 27 minutes, and I'm very happy to answer your questions. Thanks a lot, guetta. I just suggest that we go to the next presentation. Then we take all questions at the end. So, and I will then ask chile to put up a presentation and, as I say, it was very interesting to see this global overview on machine learning versus classic models. 
So but I leave the floor to chile so she can bring up some specific applications for human behavior in disasters. Can you see my screen? 


Dr. Xilei Zhao

Yeah great, thank you, enrico, thank. you guys for the for the for this invitation, and I'm very glad to be here and see everyone. 
So after gerta's introduction about her opinions and ideas about the, the pros and cons and machine learning, and the next part of my talk is about how we can leverage machine learning messes to investigate human behavior in disasters by using a case study. So my talk today will be divided into five different parts. 
I will just quickly go through the motivation of this case study and the method we use over here, and then I will go into the details and share the findings and then summarize the key takeaways and have a little bit further discussion regarding sample size and data selection. So, as you many of you probably know, there's a definitely a very strong need to study human behavior in disasters, so the behavior of. 
Building occupants before moving to the exit, also known as pre evacuation behavior strongly impacts the total time required to leave the building and the number of casualties in case of the fire emergency, and the pre evacuation time can be simul can be can be simulated with within existing simulation models using different methods. So that's kind of a very important input for a lot of existing software. 
So in the past several years or the past decade, I would say render utility models have gained a significant popularity for modeling pre evacuation decision making. It was first invented to study travel more choice problems in the 1970s and by mcfanden, and has been widely used the since then and more recently have been applied to model people's evacuation decision making. So this model has been as very nice. 
So it's theoretically founded in the random utility theory and. can produce results that could be directly used for decision making and policy analysis. However, those models require a lot of time and energy to specify the utility functions, especially the popular method like the mixed logic method, which requires a lot of time to do that, especially when non linearities and interaction exist and those models cannot automatically detect those underlying relationships make which makes the modelers extremely hard to do all this by him or herself to trying to random guess what we should do for the mixed logic specification so recently. 
So we have machine learning methods becoming very popular right now. So it have flexible modeling structure. It usually can capture high prediction accuracy and with the new methods in explainable ai or interpretable ai. So there's a lot of arguments going on. What's the similarity and differences between these two? So i'll just stick. to the term expandable ai over here. 
So expandable ai is actually is a set of method that can be used to explain black box machine learning models. So with those methods so we will be able to kind of explain the non linear relationship and interactions behind the machine learning model to help us gain insights about how this machine make decisions and to make to ex to draw insights out of it. 
So because of this emerging technology, so that's kind of motivated us to see if we can instead of using traditional random utility models and how can we use machine learning methods and explainable ai to model the evaluation problem and evacuation decision making problem and to see what kind of new results we can get. So in our modeling process so we're based on the following four assumptions. So we are assuming the decision. 
Makers will have two states. So essentially including the normal state and the response state, and all the occupants can behave rationally and their passages from normal state to response state is ruled by a binary decision making process. So the decision making process is strongly influenced by environmental or external factors and occupant or internal factors. 
So this kind of factory selections were based on the protective, the very famous protective action decision model framework first proposed by my michael, ninjal and perry. So lastly, we are assuming the decision making process used. The study can be well approximated by random forest. So gerda already mentioned random forest a lot in her talk. So I want to just say a few words. So random forest is kind of a very popular general purpose model. 
It usually can produce relatively good prediction, accuracy maybe not as accurate as like deep. neural networks, but it's very. It requires much less tuning time and is very robust to outliers. And under the neural net, random forest, we have all different kinds of decision trees in there, so potentially those decision trees can capture humans decision making rules quite well and by combining all of them together. 
So we are hoping the random forex model can produce better, can produce more accurate results for modeling people's evacuation decision making. So that's kind of the basic structure of the random forest. So wait, I don't want to go into too much technical details here of random forest, but I just want to just quickly explain how it works. So essentially you have the training data and after you have this one training data. 
What we do over here is essentially do bootstrapping. bootstrapping is the fancy name of sampling with replacement. So essentially you are doing random sampling with replacement of your original data set. You are generating a set of different bootstrapped data set of your original training data and then you are feeding on decision trees on each new bootstrap data set to capture the underlying relationship and those when you are building those decision trees over here. 
So you are using random feature selection so essentially go into each decision tree. You are not allowing all the features to go in there. You only allow a success of the future to go in there to fit the decision tree in order to reduce the similarities between different decision trees. So that's pretty much the idea of the random forest. It's very easy to understand and it's actually very robust. 
So by feeding hundreds, even thousands of decision. trees within the random forest, usually for a classification problem. So we were looking at what's the prediction of each decision tree and we will use the proportion of the voting or the majority of the protein to predict the classification label and sometimes we're using this proportion to estimate, to do solved prediction for the choice probability. 
So that's kind of how random force work and how it can be applied to model people's decision making. So after we have the random force model. But however, as you can imagine, in the random forest model, each decision tree can be very big. It can be, you know, half depths of 20 or 30 depths. 
It's impossible for humans to directly looking at each decision tree to try to interpret, let alone you have, you know, hundreds of these injuries in there. So what should we do so? then people are coming up with some ideas to explain the random force model after is fully trained. So in this case we are calling those explainable ai methods. So those methods that we use to interpret random force model include the two popular method. 
One is called variable importance. The other one is called partial dependence fault. So both of those methods they are post talk method. So it's like model agnostic post hoc methods. So they can be applied to any machine learning message and can be applied after the model is fully trained, which is quite useful and powerful and the results can be compared among different machine learning methods. 
So for many of you who are familiar with the login models, so you know that in logic models, when we are trying to interpret the results, what we are looking at is something called you know. the beta coefficient or sometimes, if we do standardization so we can interpret the standard standardize the beta coefficients from the logic model. 
So when we are looking at the standardized beta coefficient of the logic model, we usually interpret the sign of the beta coefficient and the magnitude of the beta coefficient. So the sign of the beta coefficient indicates you know it's an increasing trend or decreasing trend, and the magnitude I indicate are indicating how much the influence of these features. 
So those kind of concept can be directly corresponding to these two methods output from the machine learning models. So for the variable importance it's used to compare the impact of different features on the model. Prediction essentially is evaluating the imaginary imaginal effects of different features on your prediction outcome. So the value of the variable importance is directly corresponding to the magnitude of the. 
Standardized beta coefficient in the logic models and for the partial dependence plot is it illustrates? You know the influence of the future has on the predicted outcome. You know so it's essentially showing the relationship between the future and the probability of choosing to respond, for example in our case study. 
So we can see a strong non linear relationship over here and from this, from this kind of relationship, you can estimate like approximate slope for this curve, and this approximate slope is actually directly corresponding to the sign of the standardized beta coefficient over here. 
So essentially this is showing the trend and however, as you can imagine, this is not only a sign, it's a plot, it's a curve, so this curve contains much richer information compared to just a sign for the beta coefficient. It's becoming much more useful and can give you additional. insights to help us understand why people are making evacuation decisions okay. So that's kind of the con the methods we used for the for this study. 
We're using random forest and we're also using these two methods to interpret machine learning outcomes. So here is our case study. So we are so we are using the data collected during the unannounced evacuation experiments in cinema theater in sweden. So the goal of this experiment was to test influence of different alarm systems on the pre evacuation time. 
So we can see the green squares represent the occupants visible to the decision maker and the dark gray squares are the occupants considered close to the decision maker and learn the. This kind of squares with diagonalized represent arguments belonging to the decision makers personal group. So that's kind of the concept and what's and also the features we're extracting. 
So with this kind of camera data, the cctv data, we were able to transfer that data into tabular data or numeric data to help us build such models, and we are considering all the following features. 
You know the alarm type, the time, the number of occupants visible to the decision maker who are in response state or normal state, the occupants visible to the decision maker who are close to her and in the normal state or the response state, and the decision makers personal group that in normal state or response state, and the group size of the decision maker and the location of the decision maker. 
And before we are doing fitting the random force model. So one thing we did is to to check the multiculinarity. So I think that's a common wrong practice right now in the field that many people are directly. throwing all the all the data they have to train the machine learning model and try to interpret it directly, without checking multicollinearity, because they think red. The machine learning models are not sensitive to modern culinarities. 
There is no need to get rid of that. Like the you know the linear models. However, this misconception is it has to be kind of has to be corrected, because if you are just trying to train a very accurate machine learning models, it doesn't matter if you have molecular linearity in there. It's just you know that's as much information you can bring to the table. 
The model will be more accurate, however, as soon as you're trying to interpret the machine learning model. The assumption of using such interpretation techniques. Like you know, variable importance and partial dependence plot are relying on the assumption that there's no strong multiculinarity exist. in your original data set. So essentially, if you have multiculinarity, you cannot apply machine learning interpretation techniques. 
So that's kind of the thing that a lot of people didn't notice at this point, and I want to bring this to everyone's attention. So when you are doing this, you should consider those assumptions, try to check the multiculinarity before you're moving forward to build the machine learning model and make the interpretations okay. 
So after we did that, so we are kind of figure out these two features are not bringing anything much to the onto our data set and are bringing a lot of multiculinarities. So we remove these two features before we fit in the machine learning model. So after we did that, so we are fitting the random voice model and we also compare the random force models results with the traditional logistic regression model as. 
You can see so we also compared different metrics, so including accuracy, precision recall and f1 score. So many of you are probably familiar with accuracies. Just how accurate your prediction is. However, for our data set, we have unbalanced the data set as you can imagine that so the so we have the people in normal state and response states. They are not in have similar number of observations in there. So we have unbalanced data set. 
So other metrics, such as f1 scores, is usually preferred to do to interpret your model performance than just using the accuracy over here. So as you can see by looking at the model performance on the testing data set, the random force model is significantly outperforming the logistic regression model in terms of all the performance metrics, especially the f1. We can significantly improve that to on 2,6. so run the force model or other tree based models. 
So usually they are considered better to deal with unbalanced data set. If you don't want to do any you know, smooth or over sampling or some other techniques trying to deal with the unbalanced data set to change the distribution of your data. But that's kind of the stories and the process accounts for those kind of things are probably for another day. 
But here I just want to say you know, because of we have the unbalanced data set and the f1 score is kind of a better metric to look at and rendezvous is a better model compared to logistic regression. So after we did that, so we have the final random forest model. So what we did first is to explain the variable to extract the variable importance of our features, as you can see the. 
Decision makers is located where the decision maker is located. Essentially the road and the seat is the most important features for our model prediction. So this is consistent with previous findings and the signal of the alarm system concert in terms of variable importance, also very consistent with you know padm. So protect your action decision model. So the so the social cues and environmental cues. So those things are strongly impacting people's decision making. 
So that's kind of the variable importance. So these are all quite consistent with our expectation. So then we further dive into the partial dependence plot. So this represent the relationship between the independent variable and the probability of responding to the emergency. So let's take time as an example. It shows that the time has a low impact at the beginning of the emergency and the time start to have a strong impact. 
After the 20 seconds from the start of the emergency, as it increases to the choice probability of nearly 0,3. So for the group size, the relationship actually takes a new ship, with the choice probability reached the minimum at the group size of the three people. So at the beginning we were quite confused, but we double checked. The literature is actually makes sense. 
So with the decision maker is by herself, the probability of choosing response reaches the maximum, because people are easier to get panic and the results may also be linked to the you know social influence. When a person was alone, she does not need to check with others whether they respond or not. 
If there are many in a personal group, for example, you know five people, someone is likely to respond more quickly, which influences everyone else in the group, so for. mid size group. Usually you know three people to four people belonging to the same group. Social group may take some time in a parent discussion deciding you know what to do and to assist each other. So this was observed in previous literature as well. 
So that's kind of explained why we have this u shape for the group size, which we didn't expect at the very beginning. So we also plot like two dimensional partial dependence plot, which can automatically show the interactions between different features. Here we are kind of plotting the role of the person's seat and the the people that are visible to the decision maker that are in the response state. 
So an important observation that is that the decision makers who lie in the diagonal of the figures are most likely to respond first, regardless of their low numbers. So we're. seeing the same number of people responding. People in the back are less likely to respond than people in the front. 
So that's kind of shows the decision maker may be more sensitive to the proportion of the occupants visible to them who are responding than the absolute number of occupants visible to them who are responding. So in the previous findings I think we are thinking it's on. It's actually the number of occupants that visible to them who are responding and impacting people's decision making. 
But in the end we figure we figured out by this two dimensional partial dependence plot. It's actually not the absolute number. It's actually the proportion of people who are in front of you that are responding are really impacting people's decision making. So that's which indicates people are sitting in the back, even though they are seeing same number of people responding. they may not be acting as quickly as people who sit in the front. Okay. 
So that's kind of the partial dependence plot and some results. So here are some just key takeaways to just save some time. So the machine learning can offer high prediction, accuracy and good model interpretation when modeling pre evacuation decision making. 
So some factors such as time and elapsed after alarm has started and the decision makers group sites are presenting strong non linear relationships with the responding probability and we see strong interactions between the role number of the decision maker's location and the number of responding arguments visible to her, and the decision makers are more sensitive to the proportion of the responding occupants than the absolute number of them. 
So that's kind of some new insights that we can extract it by just looking at the pathway dependence plot and. how we can, how we can you know, generate additional insights or extract new numerical evidence to support some empirical observations. So that's for the case study we have. So I want to have some further discussion about what kind of data needed for building machine learning behavioral models. 
So those cross crash kind of questions come up a lot, especially for the folks not in the audience that are dealing with the data, like the survey data, not only the. You know the video data or the od matrix data that gerta has, that usually when we are having you know unstructured data, you know the video data, text data, all the other types of data, so those data you really have a larger sample size. 
However, for the data in many cases that we're dealing with their survey data, so those data usually have relatively smaller sample. size. That's kind of could be an issue for machine learning model training. 
So, but in general, if we want to build machine learning based behavioral models, we have to kind of transform, transfer all the unstructured data into you know format structured data with numeric categorical variables in there, in order to build our machine learning models. And so that how much data do we need is actually dependent on how what features you are choosing. 
So usually we need roughly 10 times as many examples as there are degrees of freedom in your model. So essentially you need 10 times of the observations than your select features. And usually, if you want to build good machine learning based behavioral models, you have to based on behavioral theory. So the feature selection you really have to based on the behavioral theory to in order to help you to arrive. 
At good relationship to for your machine learning model. So you cannot just throw everything in there. So that's kind of something that important and I want to highlight. So base, base your feature selection on the behavioral theory and the previous literature. And for the sample size, like I said, you need at least a few hundred of observations. The larger is better, but the small is doable. 
But at least you need like 10 times as many observations compared to the number of features you are selecting, because more complex the models, the more features you are included in there, the more you are prone to overfitting if your data set is too small. But you know, but smaller datasets are still doable. So in some cases that we're dealing with. 
So I had a stated preference survey data set which only have a few thousand observations. so we were able to compare, I think, 10 different kinds of machine learning models and also we look at the mixed logic model and the and the multinomial logic model. So the so renova is actually is doing pretty well for that data set, and recently we had another data set which only have 270 observations, around 300 observations. 
We did try a lot of machine learning models, but in the end we found out actually a simple decision tree is better than a larger model in terms of accuracy and all the other measures and the decision tree, structures and als, and also offers some new insight compared to the logic model. So you don't know which model is gonna perform best for a specific data set. 
So essentially the best practice is usually you need to select a wide range of machine learning model with different. complexities to apply to a specific data set and compare all those models, using, you know, cross validation to select the best model and for different data set, because the underlying relationships is unknown and some is simpler, some is more complex. So yeah so in certain cases you may find even larger model. Is the best model. 
Okay, so that's kind of the common practice we would do. But that's kind of my you know suggestions for building machine learning based behavioral models. So yeah, so if you can do all this, so we are expecting the machine learning based behavior models. 
They can generate accurate predictions, can help you review non linearities and interactions and in the end there's actually a trend of how we can use machine learning to help us better specify the render utility models to improve the existing statistical tools or. how can we integrate these two types of approaches together? 
So that's kind of an emerging field in this area right now, and I strongly encourage everyone to think about these problems and we can work on this together to push the field forward in this direction. So lastly, I want to just thank our founders to fund our research, and this paper has been published on the automation in construction. 
So if you are more interested, so please refer to the paper, and we also post our code on github so you can also go there to download our code to start your machine learning experiments. Thank you all for your attention. So let's conclude my talk. Thanks a lot, shilae. This was really interesting also to see more applications in the domain of unbearing fire, particularly you look at pre vacation modeling, as mentioned. 


Open discussion

We have time. 
For some questions and I asked people to start putting me in the chat. We already have a few, so I will read them out loud and maybe either gert or chile or both can comment. Feel free to step in. The first one is from mike kinsey. 
They say either of you consider using reinforcement learning couple with the vacation modeling to found out the shortest total evacuation time, because this could treat the vacation model like a game optimizing agent decision makes so basically looking for informing crowd management and cloud management procedures. Did any of you start looking into this or thought about it? I've done some experimental work with reinforcement learning, not exactly on that question. 
Question is: chile, have you had your success story to report, because mine is the story of failure in that area? Yeah, I haven't tried anything for reinforcement learning for. this kind of experiment. So what? So what we are doing right now? We are using deep learning, trying to do like a new real time forecasting for evacuation travel demand during wildfires. So that's kind of something we tried. We are just using graphical models. 
You know neural networks and transformers in there, but we haven't. We haven't tried reinforcement learning yet. But I think it's a potential area definitely to incorporate in reinforcement learning with existing simulation tools and to trying to generate those kind of better you know draw finding situation over here and estimating the evacuation time is definitely an emerging area, but we haven't got a chance to try that. Okay, in that case I'm just. 
I just briefly will tell you my story of failure. A colleague of mine and I've had a very talented master's student and we asked him to try reinforcement. Learning to get an agent get through a group of other agents and he managed to make the agent actually get through, to squeeze through, but we found out in the end that all he had done was put another model, a model, a behavioral model, a full behavior model, like we would put in directly in the reward. I think this is this is very. 
It sounds totally promising at first glance, but then you have to look into what do you actually use to reward your agents, and temptation is very big that you put a full phenomenal model, classic model, into the reward. I haven't found a way out of that yet. So we got. 
We went back to classic modeling in that particular case, and it was not an evacuation times, but it was just on the task of getting obstacle avoidance if you like, so. it was even simpler if you, if you manage better. 
I think this is a very interesting research question, and there are a couple of question after this one from lazarus and another one from mike that I think I can combine because they're along the same lines. So we have been looking at application machine learning models towards replicating certain behavior that have been observed. But to which extent, if I can summarize, are those about replicating? 
At which extent are also able to predict things outside the domain of applications of the specific scenarios that we looked at? Because that's a bit of a very common question that everyone that looked at any modeling applications like: what is the generalizability of the findings that you have if you go outside, let's say the specific scenarios that you're looking at? Maybe you can both answer to this, not so. much. 
I'll answer that question, because I think monster has really refers to my work there on the ob matrices. I find this a very interesting research question for me. These od matrices are a dynamical system, vector valued external numerical systems, and I'm not even sure whether this sort of dynamical system is the same. If I take it from my the platform forms. 
I have there at that station to say the city of melbourne and look at people walking around there, and this is something I would actually would very much like to investigate whether there is a structure in this system that is similar and that can be transported from one building structure or city structure to the next. 
It's open in my eyes and I think that machine learning right now is just one more hammer and we can use it to hammer in. nails, but we can't use it to solve all the world problems, and one of this is more the philosophical way to answer the question. There are lots of lots of questions that we just can't solve, for that they do don't generalize. 
Just because we are successful in some areas very successful when we predict shopping behavior or something like that doesn't mean that we can generally predict human behavior. She like, do you want to comment on this as well? Yeah, I think that's a common issue. So not only for machine learning right, but also for statistical models. We also have the same issue. It's just a generalization problem. 
It's the lack of data and it's the same issue and especially for machine learning models. They tend to even overfit the data a lot and tend to capture a lot of noise of a specific data set. which will bring more problems if we are trying to use a model that trained on this data set and to make a prediction for another place, that may perform very badly. 
So I think that's kind of a potential danger out there of using machine learning model to directly use that for decision making. So that's why I think we need to careful about when to use machine learning and how we can combine machine learning model with existing statistical tools to complement or integrate these two tools together, for example. 
So there's a new line of thinking that people are thinking about how about we don't replace the random utility models with machine learning. We are using the machine learning, like the partial dependence plus results, to help us specify what's the distribution of the beta coefficient into the random utility model and what's the average distribution and. things like that to help us specify that. 
So then the mixed logic model or other types of logic model can be automatically or strongly improved with such information provided by the machine learning results. So in the end so maybe that could be one way that going forward, using machine learning to assist the transparent statistical models to help us build more trust in the model building process. I think that's a potential solution out there. But yeah, everyone's exploring this right now. 
We have a lot of questions, a lot of interest, but we already going over time. So I will have to take a selection here and of course you're welcome to reach out the speakers if you have more questions. But if it's okay, I will go on for a couple more minutes. One that is very interesting is from alastair to say, as anyone looked. 
At crowdsourcing data sets like asking member of the public to send in videos or gamification, because this looks seems like a good idea to collect a lot of data. No, no, no, neither. Yeah, we haven't done that. Yeah, it's. I think it's similar to twitter mining. You know text, text, type of data analysis, your natural language processing, but we haven't got a chance to do that yet. 
And one more question to chile from professor seifried, to use your approach, structured data needed. Could you comment how much obstruction of the real system is necessary for that step, if you compare it with ai, for instance? Yes, so I think that's a great question. 
So usually, when we are constructing those kind of data with the features usually we are based on, like the empirical findings and the previous literature, and also, you know, like the protective. action decision theory to see what kind of features that usually people traditionally use to model people's evacuation decision making, and we're extracting those features out of those kind of video data or other types of data to do that. 
So that's kind of how we do that. So that's definitely a lot of abstraction and selection, but we mainly do that based on the previous work, so we don't want to. Yeah, so that's kind of what we did. And I take another question from unitong: how do you deal with data from different contexts? This is actually can happen. So how do you decide whether the data can be merged or not? For machine learning? 
That's a very good point. You might have data that are not exactly from the very same context, but you may want to decide to merge them. When can you do it's? a very general question and I don't think you can't be that general, at least when I work with machine learning techniques. We always put in a lot of expert knowledge, a lot of knowledge about the situation too. I mean pre selection, pre processing of data. 
I don't think it's trivial. You really need to know your subject when already that. So I haven't seen any people doing that in the evacuation field, but I do see people doing that in the travel behavior field. So usually they will have you know, for example in the household travel survey. So from different states. So usually there are some questions that are common. Questions are asked in different, in different surveys. 
So they were able to combining certain those kind of data set together into a larger data set in order to build a more robust machine learning model. so that's kind of some people have already done that. There is one comment from gabriela that relates to the fact it would be nice to have a support like a central data collection or central way to create such databases. 
These are all kind of ideas that we could bring up, for instance within our group. If there is some volunteers that is willing sometime putting this together. Many people will benefit for that. I see another comment. Very quickly, there was a more general comment from peter lawrence. What about the fact that often these methods require a lot of data that are based on your observation information? 
How do you deal with the fact that it's much harder to collect latent data such as people preferences for choosing to do a given action? I guess this is a bit more philosophical quest because I guess machine learning. is taking completely different route. Right, I think it's again the hammer and the nail thing. I mean the methods are simply like a hammer. You cannot use it to. 
You can use it to hammer in the nail, but you can't use it to sew together two pieces of cloth. It's if you want to use this kind of methods, you have to choose your problems well, or if you have a problem you might have. I mean machine learning might not always help you, it's yeah. If you're, if your true data is so well hidden and covered, you won't be able to use this. 
The new technology yet I mean things are advancing and we're getting better at revealing features. But it's not. It's not the new messiah you can't apply to all sorts of problems. I take two last questions. One is from steve can the? approach be used to identify the set of independent variables to be examined, rather than looking at the influence of given variables of the dependent variable. 
So how do we come up with the impact of generating new factors, some sort of staged approach? Have you thought about that? Is that even feasible, I think? Is this related to future selection? Is that what this question is about steve? Yes, precisely that. Yes, so do we have to know in advance the factors that we're interested in or and then work out which of them have an influence? 
Or is there a way, using one of these techniques, to come up with a genuinely new factors that we haven't previously identified? That's a great question. So usually those kind of machine learning tools like random force can be used to do feature selection automatically. So you can do that you. know throwing all the features you have. 
You know hundreds of features in there and you can help you tell which features are important and will help you to do feature selection out of it, and maybe you can perhaps compare that data driven feature selection result with the theory based you know feature selection result and to see what kind of the new features you are getting out of it. So that's just an idea. 
But this still presumes that we have features that we can select from. So we made a decision on teachers and there is actually new ways to find new features that, for example, when we do studies, we often think about density or speed or something like that, and maybe there's some losing vox that we that really influenced the thing and that we haven't observed yet, and if you go into deeper operator. based methodology. 
There is ways to find these, but they're not necessarily easy to understand, and this is also really very mathematical stuff, but yeah there are ways they're just not easy. I think there is ed that wants to comment. Just please be brief, because we are already running overtime ed are you there? Yeah, it's unfortunate. It's not a brief comment, but excellent presentations. 
By the way, I mean very interesting stuff really I think a lot of promise in this. The concern I've got, though, is trying to extract understanding of behavior by looking at the interpreting the predictions and the results. I mean that's theater and how you generalize that the theater that you used was a very unusual one. A is very small, b there's only one exit. 
C the exits at the back, where no one can see it, and so. The behaviors that you were generalizing are very, very specific to that situation, and the danger is that people will begin to try and generalize this and say: ah, theaters, this is how people are going to respond, whereas the work we've done and looking at response time of people in theaters, we come up with some very different generalizations to what you've come up with, for example, the proximity to the exit and the exit route will have a big impact on the response time. 
So it's really a comment about trying to understand and generalize the behaviors that you're seeing and, and mistakingly, I think, extracting that to the domain, you know theaters. I think there was also another comment that is kind of related to this, because timothy comment okay, if you look also at other additional contextual considerations that have informed your od model, so. even if the context is different. 
I guess one of the main concerns that many people have is how much of the boundary conditions, let's say, you put into consideration when you develop this type of models. But I mean my two cents on this is that depends a lot. It's it. The problem is not the tool. It's also how you use the results of a given, of a given application. 
Because we should not forget that everything that we do has a limit of applicability. But this applies for any type of model. But maybe guest and chile, you have some final thoughts you want to add of this. I think I agree with what ad just said, because you know I think, like I just said, as just a few minutes ago before, that there's a lot of danger of inter over interpreting the results of the. 
Machine learning outputs especially, we are trying to apply those techniques, trying to and those machine learning models tend to, you know, over capturing a lot of underlying noise in there. So that's and we, because it's a black box model, we don't have the complete trust in the machine learning model, even though we can extract some insights out of it. 
So there's so that's why there's the emerging field called you know, trustworthy ai that trying to bring the trust in the machine learning model that we can certify the machine learning model and understand how it makes decisions. Are they making equitable, inclusive decisions or predictions? 
So that's kind of we are transforming this in the field forward to see if we can not completely relying on the machine learning model but bring some insights that generated by the machine learning model to help us specify the. statistical, the transparent statistical models, to get better results out of the statistical models and use the statistical models instead to gain the trust again from the fields. 
So that's kind of something that I would like to further pursue in the future in this area. Thank you, ed. I'll just make it very brief. We have an exciting new set of tools and, as usual with tools, we have to use them with care. Thanks to everyone. I think we are well over time and it was really nice to have this very, very interesting presentations and very good discussion on this topic. 
So again, I again tell you that we will have more events like this. So follow us on twitter and sign up to our youtube channel, where we will always share for free all the recordings of our webinar. So stay tuned because we will arrange. many other interesting events in the domain of ubm fires. So thanks everyone for joining in today and thanks especially for the very nice speakers. Thanks, guerta. Thanks julie. Thank you. 