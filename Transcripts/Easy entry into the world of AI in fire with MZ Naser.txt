 Hello everybody, welcome to Firescience Show Session 28. Today we'll be discussing one of my favorite topics in all of the fire science, and that is the use of artificial intelligence. I consider it one of my favorites because it's something that I would really, really love to learn myself. And I'm exploiting this podcast to bring me the best guests that can explain it to me a bit more. Honestly, I was quite confused where to start with all of this. And after the discussion that you will hear in this episode, I have a little bit better idea of where to start and how to start. And actually, I should go on this journey because I'm absolutely convinced it's worth it. Today with me, I have one of the young leaders of fire safety. He's an author of literally countless papers on AI in fire science. I'm actually astonished by the amount and quality of work he's putting through and publishing. I'm really, really admiring him for this. He's a professor at Clemson University. Yeah, let's jump into it. I'll prolong this because you want to hear what's after the intro. Please welcome MZ Nasser and let's jump into the world of AI and fire science. Welcome to FireSense Show. Today I'm here with Professor MZ Nasser from Pemson University. Hey Nasser, great to have you here. I'm fine, how about you? How's it going? I'm fantastic, I'm about to learn so much about AI. I'm really happy for that. I hope so. I've invited you here because you are a rising star in our industry, and you're probably one of very few people who has a good clue about how AI is working and how can we use it in engineering, actually. There's not so many of AI luminaries in our community. And through your papers, which are actually very educative, they're not like fleshing out. You see how advanced algorithms I can use. You publish a lot of like introductory level AI papers, review papers. And I appreciate that so much. Thank you. What puts you on this pathway to use computers to enhance your learning? This is a very good question. Actually, so the first time I learned about AI, it was sometime in 2012 or 2013. I was taking a transportation course, and in that course, the professor was discussing how we can use AI to organize traffic, traffic light, synchronize all those different types of infrastructure. And then I kind of did like a very short paper at the time on fire and AI get lost because you know once you go to your PhD you focus on experimentations, every simulation, every you know those things and you cannot forget AI. And once I was done, I was trying to find a faculty job. And as you know, fire experiments are very, very expensive. And you have to have lab and equipment as you know, you have a massive lab here. And in my case, it was very hard to develop this lab. So I was, I had to do something and I wanted to do something a little bit different than simulation. And so I went back to my roots to AI. and that's when things clicked back again and from 2012 2013 to maybe 2019 things has rapidly changed on the AI front many many things has changed we have different algorithms different training systems learning systems so I had to relearn everything else at that time and hence some of my papers are, just like what you mentioned, they are very on the same level, because as I was writing them, I was also learning. So I decided to go with a very smooth path because this is how I learned, so perhaps it would also be easier to somebody who's not as familiar with AI as me at that time to go about those papers. So that's such a cool path. So you basically were documenting your own way through the world of AI. That's so cool, man. And that also confirms the theory that you just have to be one step ahead from others to be an expert and to teach. You don't have to know everything to provide useful guidance. And I really appreciate that you are doing that. And I assume on your path to the AI, you've stumbled upon the same confusion everyone is stumbling against. For me, it's like, what the hell is AI after all? Can you even define it? And then what kind of AI should I go? Because once you start like digging, you enter this loophole with hundreds of algorithms, models, approaches, and it's really confusing. So how was it for you? Yeah, 100%. I didn't know three, four years ago. I didn't really know, Or I only knew neural networks. This is what I was, like, you know, a burp on earlier. To me, this was my math. I could solve anything with neural networks. Because it was, you know, one algorithm, one tool that you can put in the data points. It will run. It should give you some kind of a good performance, if not, you know, excellent performance on different problems but as you mentioned nowadays we have all these different types of learning all these types of algorithms and the easiest thing in my case was i need to learn i need to learn the basics so i had to go back and see okay what are the basics for supervised learning what what is classification what is regression and so once you go back to computer science and see those definitions then you would see okay you know what most of the problems, then you would see, okay, you know what? Most of the problems we do in fire engineering are really regression. You know, we have a phenomena, and the outcome of this phenomena is a number. You know, fire resistance. It could be like, you know, heating rate, burning rate, some kind of a number. If your output is some kind of a number, then this is a very good chance that you are going to be dealing with a supervised learning problem with a subcomponent that's going to be regression. If your output is going to be something like a category, for instance, this column fails or doesn't fail, slab collapse or doesn't collapse, you have charring, you don't have charring, for instance, this fire is heat, you know, ventilated control. If you are trying to put a phenomena into one group, this is classification. So once you know the problem, you have to define the algorithm. Then you would say, okay, well, now my problem is, for instance, regression. What kind of algorithms are there out there now that can solve a regression problem? So from there, you would go, you'd find probably hundreds of algorithms that can do the same job. So the question becomes, which one of these algorithms I'm going to use? And the answer is, to be honest, you could potentially use any single algorithm of these. And if you have a good database, you will come up with a good answer. You will come up with a good prediction tool. The problem becomes, as you might have guessed, is why would I go with algorithm A instead of algorithm B or C or D? What are the motivations behind these algorithms? And the answer to this is interesting because this is exactly like saying, shall I use ANSYS or ABACUS to solve a problem? It's basically which algorithm you're familiar with. It's going to come to your own experiences. In my case, I've always used ANSYS. I use ABACUS very, very slightly. So if you go back to my papers, they're all answers. Same thing with my algorithms. You'll see that the earlier work was heavily towards neural network. More recently, I've learned more about different algorithms, the modern ones, because now, as you know, modern algorithms are almost superior when it comes to trying for prediction power. And to be honest, if you have a nice database, if you run, let's say, 10 algorithms, out of the 10 algorithms, most likely nine of them will give you R squared, R of 95%, 90%, 85%. So the science is really not in running the algorithm. The science is what did you learn from this algorithm? I mean, let's say that you use this algorithm, you have a a good performance but how does this actually advances our science our our knowledge so to break the first wall for anyone jumping into i've from your papers i've learned their supervised unsupervised and semi supervised methods and this seemed like the very first critical choice uh one would make when they enter. So could you try and briefly showcase the differences and give these examples? So supervised learning, the term supervised means you know the inputs and you know the output. So everything is being supervised. So for instance, let's say that we are trying to figure out if this column is going to fade under fire. We know the column geometry. We know its material properties. We if this column is going to fail under fire. We know the column geometry. We know its material properties. We know if it's going to be boundary conditions fixed. We know all of these things. We've done a test. So we also know its fire resistance, or we know when it's going to fail. You know everything. You know the inputs. You know the outputs. So this is supervised. Let's say now you know all the inputs. Let's say you have a group of columns. You know all the inputs, but you don't know when they fail. So you would use unsupervised learning, and this way the algorithm should cluster or combine the columns that are similar to each other into groups. And then the algorithm would say, well, these five columns are group one. These four columns are group two. These four columns are group three. You don't know the output. You don't know why these are in groups. But if you go back and study the FIRE test results, you're likely to see that the columns in group one, maybe they failed within an hour. Group two, maybe they failed within two hours. So unsupervised learning is when you know the inputs, but you don't know the output. You don't know what the phenomena is. You're just trying to group them together. Semi-supervised learning is going to be somewhere in between. Semi-supervised would be something, let's say, that we have images of columns failing. Now, instead of us going image by image and saying this column fails, this column doesn't fail, we could potentially only label 50 images. And the algorithm should be able to label the additional 50 images that we didn't label. So this way it has a little bit of knowledge on the inputs, outputs. It doesn't have it for all the database. So it's somewhere in between supervised and unsupervised learning. So if you had, let's say, a supervised algorithm with a database on existing columns, and then you come up with a completely new column. The supervised would tell you when it will fail based on this knowledge. The unsupervised would tell you to which group of columns this one looks more familiar to. And the semi-supervised could just continue the task you were doing with the previous columns. Was it painting it pink or measuring their moment of inertia or something? Okay. 100%. This seems useful. You know what? You just reminded me when you said your technician has an AI. This is what his algorithm would do. The algorithm would recognize noise or maybe like my mic touching the hoodie, and it would label that as this is like noise or this is like not voice because it has seen before through training that this sign of scratching is not really a voice, so you have to take it away. Let's just jump quickly from enthusiasm to the dangerous region, because you've mentioned it's seen, but if it has not seen something, it's very unlikely it's going to predict that behavior, right? Like if you show it a thousand fires with flashover, it will not know that the backcraft may happen, right? And this is the problem. This is exactly the problem. The problem is when you develop an algorithm and you have a good database and you have good performance, the researcher needs to remember that this performance is only valid for your database. To go beyond the database is going to be very, very tricky because when you have a database, you're immediately constraining your algorithm. So you have a space of, you have a phenomenon, you have a space of inputs. You can't possibly collect everything. You can collect some features of the space. And then for the algorithm, all what it sees is those features as the whole space. So if you have an additional feature outside of this space, it's going to be very hard to give you a correct prediction. Maybe it could sometimes, if the algorithm, or maybe if the problem is simple enough, it could. But other than that, it's going to be very tricky. It's very similar to experience, actually. If you experience a lot of things, you're more likely to predict things. That's something we share with the machines, I guess. Yeah, one thing, because you mentioned experience, and I value this a lot. When we have experience, usually, at least us, we have a knowledge of what could happen. We could see beyond the experience that we have. Algorithms can't, and that's going to be the problem that we're going to be dealing with. We can't go beyond what we can see, beyond the data. However, they're very, very good to see between the lines, and we're not. So this is how we can complement both of us, because if you have a complex database, for us it's going to be very hard to visualize. For them it's easy. We can see things and this is why they predict things with high accuracy. But that doesn't mean that this prediction is actually something that's physically correct. I had this episode on AI and fire already with Sien-Yen Huang from Hong Kong Polytechnic University. And Sien-Yen is doing a lot of crazy things with smoke control, fire detection in tunnels. And he also mentioned that this human-machine combination is the most powerful. And in a way, I had a feeling he would like this AI be a way you could transfer the collective experience of the whole industry. you could transfer the collective experience of the whole industry. And for me, that was such a powerful and beautiful idea that so much knowledge is lost between us. And if we could have this collective mind helping each other, it would be fun. But it also seems very difficult from the technical point of view to achieve that, right? Because to what extent the structure of the database is also important? To what extent you can drop scattered data into an algorithm and expect correct results? So first of all, we're not computer scientists. We're appliers. Computer scientists, they develop the algorithms, they validate them over multiple databases. We just take them and then we do our own little experiment and we have good performance and we think it works. The second part of the issue is the machine learning we're using now or the algorithms that we're using now, they're highly data-driven or correlation-driven, which negates the purpose of science. Not everything correlates means that there is a cause and effect. This is why, I mean, even myself, I'm trying to move away from all this data-driven nonsense and go towards modern algorithms that at least can give you cause and effect. Because if you know the cause and effect, regardless of how much data you have, you'll always get the right answer. The goal is to know why this happened. you'll always get the right answer. The goal is to know why this happened. The cause of the issue is not to know I'm seeing this or I have seen this in 10 experiments and this will happen in the 11th experiment. There is no guarantee. Observations help. However, to come up with knowledge, you need to know why and you need to know the cause and effect. And if you teach an algorithm cause and effect, then you have to completely negate or move away from the type of learning that we have now in commercial machine learning. Because commercial machine learning is purely data-driven. Now, I'm not saying that correlation or data-driven doesn't have a purpose. It does have a purpose, and it will work for different problems. However, for our own, if you want to advance knowledge as opposed to apply knowledge, application is going to be fine for correlation or data-driven because you're looking for a solution. You see this every day. You want a surrogate model that tells you, if you see this, this is likely to happen. You know, we're good to go. But if you want to know why things happen, we can't rely on AI. We have to combine AI into our experiments, and we have to use a completely different kind of teaching methods for AI to figure out cause and effect. In one of your papers or in one of your talks, you've used the definition of AI as a computational technique that exploits hidden patterns between seemingly unrelated parameters to draw solutions to a given phenomena. But often when you see this data-driven AI, it just seems like really complex statistics. It's like something you could not plot and have in R-square on a single plot. It's drawn from multiple dimensions, let's say. And this statistical one, it seems attractive, it's interesting, possibly useful, and probably very useful. But it's this exploitation of hidden patterns between seemingly unrelated parameters. This seems like something that could tell us why facades are burning or why spalling occurs. Or I don't know why in some conditions firefighters may die in the room. But to achieve this hidden pattern recognition, you need knowledge beyond data, right? You need to have observations. That's what you meant by cutting the experiment in AI. You need to have a methodology of saying, I have experiments, I've seen this, but this experiment is going to be limited by whatever equipment I have, sensors I have. So sometimes I'm picking up data, I think it's noise, maybe it's not noise. So you have to do multiple levels of experimentation, use that data, and you have to teach an algorithm at a different level what each one means. And then the algorithm should be able to put an overall picture of hidden pathways between how these factors react. For instance, many research papers now on AI, not just in fire, in really any field in engineering, the first, the second, usually the second section of a paper would be, like, description of database, and then they would list database, and then they would say all these, like, we have min, max, average, median for our database. And the second thing they always put is like a correlation matrix. And then they would say this is the relation between. But the correlation matrix is only going to be linear because you're using a linear correlation. There is no guarantee that the relationship between the factors themselves or the features themselves or the features and output is linear. So having that matrix or that table doesn't really tell me anything about cause and effect. It tells me that I could use any statistical model to come up with an equation. However, the thing about machine learning and statistics is the following. In statistics, to use a model, you have to be confined with the assumptions of that model because each model has a certain kind of assumptions. It's applicable for a set of distribution of data. And hence the statistic cannot be applied to many, many of our problems because we have highly nonlinear problems. In machine learning, these are non-parametric methods. So they don't have assumptions for data. They don't have assumptions for distribution. They can fit very complex functions within our database. Most likely they outperform statistics in our case. If our problems were linear, you won't find anybody using machine learning because why would you use machine learning for such a simple problem? I guess in 20 or 30 years when this becomes mainstream, you will see people using machine learning for linear problems, just like today we use CFD for extremely simple cases instead of zone models. I predicted it was going to happen, and you didn't say that. But I assume that it's also highly related to the amount of data you have to be able to get this quality or these multilevel correlations there. at this quality or this multi-level correlations there. And like, how much data, when does one know they have enough data for the problem? And another question that's something I would be very interested in, when I am planning my experiment, how should I prepare myself to create sufficient amount of data, you know? So I want to have a grant, and I need to know if I will need a thousand experiments, a hundred experiments, or ten experiments of this type and fifty additional of another type. Yeah, it's a very good question, and it's one, I know there is a lot of research in computer science to figure out this answer. I know there are a few papers, like I think five, six years ago, the minimum number of observations somebody would need is maybe, I think, 10 or 12. Now the number is 25. So if you have a 25 observation, most of the time you should be able to have some kind of a model that performs in a nice way. I know, however, for some type of learning classification problems, I think the minimum number that you could be confident about your results is about 100 observations. So these are the numbers that we play with, somewhere between 25 and 100. Now the problem becomes is once the database becomes wider, once you have many, many features, then you will have to have many observations. Because if you think about it, a database is a matrix. You have rows and columns. The wider the matrix, you have to have it very, very deep so you can figure out some kind of correlation between the different parameters. So the wider it is, the more data you would need. And I don't know the answer. I don't know if you have 50 data points, is this going to be enough or no? I really don't know. I don't think we'll have an answer anytime soon. But if we are within 25 to 100 points with maybe 4 to 7 features, we should be, or at least the algorithms we have now, would be able to give you something that's maybe with some confidence. And to complement what I just mentioned, nowadays the algorithms that we do use, they could be augmented with different tools. For instance, you can add confidence intervals to your model. So this way, even if you have a small database, the algorithm should tell you, okay, this is my prediction. I'm predicting this column to fail in 60 minutes, but this is going to be within a confidence of 90% or 70% or 60%. So even if you have a short database, you'll have a prediction. But on the other hand, you have some kind of confidence in your model. It's not like the old days, two or three years ago, where we couldn't apply confidence intervals to our models. And it's just, you know, this is the number that you have. Nowadays, we could potentially add confidence. And then this would give us some level of trust. Because even if you have a short database or a very, very wide database, the prediction is going to be combined with some kind of confidence. It would say, okay, my prediction is this much with 70%. In the past, maybe I was not working much with them, but I got familiar a bit with some design of experiment methods that allow you to figure out the number of experiments to identify the, for example, the influence of variables on the outcome, like Box-Beneken design. There was late in Hypercube sampling, Monte Carlo, of course, many, many methods like that. And I was very interested in them because in my PhD, I did a very ugly thing. I've taken like 100 geometries, a few fires, and some combinations on ventilation, and I just brute-forced them. And it gave me a beautiful array of results to work with and complete my PhD. And I was very happy with it. I still am happy with that. I just feel like a caveman bashing a wooden stick against a wall to get an answer where I could have done this way more elegantly in a way. So is there also, let's say, preparation, best practices to drive an experiment so it's useful for this? Yes. So there are a few things. For instance, we have some algorithms that what all what they do is they look at the distribution of your observations or the experiments that you have so far. And then they are able to zoom or to pinpoint some regions within that distribution that say, okay, for this region or for this distribution, we need to have more experimental data points. So this way, let's say that you're preparing two or three experiments. You have to keep in mind, maybe I need to allocate maybe two more experiments with this that would give me... To cover a specific variable, for example. Yeah, because if you think about a model and how it would validate itself, it basically runs some kind of performance metrics along different regions. What we normally do is we collect all the data points, all the predictions, and we run R-square. Now, R-square doesn't tell you the performance for specific points. It tells you performance for the whole database or for the whole predictions. But if you plot X and Y, you would see that your curve at some regions are much, much larger than the other regions. And for those regions, you might want to end up with more experimental points because this tells you two things. This tells you, one, the algorithm was not able to capture that phenomenon at that region. And two, it tells you that maybe there is something there that we haven't seen before. So maybe if you do an experiment, maybe you'll be able either to confirm it or deny it and figure out something new. So this would guide you towards the potential outlier that could actually unravel new physics or something completely unexpected. That's cool. Let's move a bit more into engineering. I've taken a look on your papers and you have used AI to identify fire vulnerable bridges, design columns, change measurements for fiber reinforcement, polymer strengthened reinforced columns to determine spalling, to identify failure of beams. Is there any field of engineering you have identified that it will not work at all? I don't know. It's a very good question. And this is what scares me most, to be honest. I'm a little bit lucky because I get to play with AI a little bit earlier and get to see how it works. And so far, it's working really well, which tells me it's either the problems we have are simple enough for AI. Because if you really think about it, computer scientists, when they develop an algorithm, it works for insurance, for medicine, for space. It's not just bending, buckling, flammability, collapses. We have much, much more complex problems. And if it works well for complex problems, like finding a new star in galaxies, that's a very complex problem. Maybe our problems are not that complex, after all, for AI to solve. Maybe it's complex for empirical methods that we apply or for finite element methods that we use. They're not really that hard. They're just computationally expensive for FE or like FDS. You have to run them for a long time. You have to mesh. You have elements. But collectively, maybe they're not. The other issue I'm thinking about is maybe because an algorithm is really a black box, and we don't know how it behaves. We only see the output. What if the output is correct, but the map that links the inputs to the output is not correct? Maybe the output is correct for this database, but once you go for outside the range of your database, it's going to be very, very hard. Or maybe you start to get some errors. The counterpart is the following. The counterpart is very interesting because, let's say, in structural fair engineering, we have certain sizes for columns and beans. We don't go beyond that. So our databases are usually good because you won't find a very, very, very thin column or a very, very short column. We don't use that. So going outside the norm will give you error measurements but at the same time we never used in practice so there is like a in a brazen cons for for every issue in the same way you will have only combustion within limits of flammability you would have certain sizes of fire only in certain ventilation factors. So there are like boundaries to the fires that we know empirically. And we could work with that. With all this innovation that you show in machine learning, I'm really wondering how hard is it in a field? So let's say a field with concrete like ours, like construction is not the place of raging innovation. We're using 100 years old standards to quantify fire resistance and it's not unlikely to change very soon. The problem is not with innovating something. The problem is innovating something and then breaking everything else. And we're very slow to adapt new technologies, new methods. How is it going for you as a pioneer of this technology in construction? You must have funny reviews for your papers. In the earlier papers, I would get very unique reviews, yes. I would say now it's much better now, much, much better. But I would say the following. 100% we're slow to adapt. I would agree with this. Two years ago when I was trying to push for something in a conference or with a funding agency, the answer is complete no, complete no. I had to reconsider my whole path because I couldn't get anything from anybody. Nowadays, I think the industry is interested like we we had with the american concrete institute we had a few talk we have we published a book with them on ai completely with concrete they were very very open to it okay i know the state industry is looking into something in the close by mass and actually one of my grants is from masonry and they want to use ai to design mass masonry structure for fire. So it's a little bit of that, but the thing that's good for our case right now is the following. We have a lot of startups, and many of these startups they're trying to automate many of the routine applications or steps that we use. And an easy way to automate a routine step is to use machine learning, because you're not really going above and beyond in something. You have a procedure already. You're just trying to make it much, much faster, much more accurate, less error, and all of that would accumulate to less time, more money. So there is a push from the industry now, and I think it will grow within the next two or three years because there's a lot of startups. And these startups, if you look at the investors where they come from, they're not really engineers. They're mainly into developing softwares and apps and computer science backgrounds. However, they don't have the domain knowledge that we do. And this is why they hire civil engineers or structural engineers or fire engineers. Once they learn the problem, it's going to be very, very easy for them to develop solutions. The issue with me is the following. The issue is we need solutions that come from somebody who has been practicing and been educated in our field to solve our problems. We can't just give domain knowledge to somebody who doesn't have our background because they're looking at the surface. We need people to do fire engineering or structural engineering from the beginning, from their undergrads. Then you would come up with a solution that would work better, would work best for our case, and would advance our knowledge as well. Because, you know, I'm not really looking for a software that tells me this is the amount of fire you're going to get or this is the heat intensity you're going to get. Because anybody can do a software like this. I want to know why. If I know why, I can redesign, I can change things, I can come up with unique designs, innovations that we don't have right now. And a computer scientist can't give you that. You have to be an engineer to do that. I'll challenge that because for a paradigm shift to occur in a field, it must be done by someone from outside of that field. If you're a graduated fire safety engineering, it's very unlikely that you will change the fire safety engineering completely because of the way how you have been taught and there is certain experience factor in your computer in your head that will prevent you from touching stuff but however you escaping the field and jumping into computer science and coming back is actually quite a nice path to to carve such a path for new. You've also used the term black box many times, and it seems like that. I don't understand it. You know, I see it. I know I can put some stuff in. It will give me stuff out. Even for CFD, I mean, CFD is very, very hard. But I can more or less understand CFD. I don't claim I understand it completely, but I more or less know what the equations do, what CFD. I don't claim I understand it completely, but I more or less know what the equations do, what the schemes are, what's a turbulence model, what's boundary layer. I know these things and I can track back my simulation, identifying each of these steps and going in. Then I see a pattern of neural network and it looks like a Christmas tree to me. It doesn't reassemble anything, equation or something. And in your paper in Automation Constructions, Engineer's Guide to AI, you've championed this explainable AI as a necessity. So tell me, what would be this explainable AI? And why it would be something that would make me use AI while I'm not using it today? So what I did is a very similar exercise. So you take an equation from a code and you apply it in a database and you see that the equation from the code that we have to use as engineers does not perform as well as an algorithm. So this fact by itself should make you pause. How can you not trust a code over an algorithm? Then the second question would be if the algorithm can predict better than the code, why do I have to use the code when I have a better method that can predict better than the code? The second question is the following. Why does the algorithm do that and the code cannot do that? Now, to know why an algorithm does a certain thing, we have to break the algorithm open it and see how it does the way it does. And right now, we don't know. We can't do that because, one, we're not computer scientists. Two, even computer scientists can't really track how the algorithms work because everything for them is, the goal is to get as good of a prediction as an experiment or as an observation. We don't care how to get there. In our case, we do care because we have to justify our decisions. How can I justify using a column with two-hour fire rating in a building when I don't know why the algorithm says yes? If I know why, I need to figure out what. So this is where you have to use, if you have AI, you have also above it explainable AI. Explainable AI, the way it does is the following. Each also above it explainable AI. Explainable AI, the way it does is the following. Each algorithm should be able to tell you exactly how it came up with its own production. It has to break it down for you so you can understand because maybe numerically it's correct. However, physically or from an engineering background, maybe it's not correct. Maybe the algorithm will say the relationship between material and geometry is linear, but we know from our experiment it's not linear. So how can I trust its prediction if it negates what physics tells me to do? Now, the problem with explainable AI is the following. It will only explain its results based on the database that you have. So if you don't have a good database or as many features as the physics would allow you to do, even if you have an explainable AI, it won't be as good as the one we have in physics because it won't be able to capture all the interactions that one we see in physics. So it's not really about using explainable AI or AI, it's about using a system that can tell you this causes this. When we use explainable AI, we basically have a very small code within our algorithm that can track the prediction back to its origin. How did the algorithm link parameter one with parameter two with parameter three with parameter four to come up with the prediction that it did? Blackbox doesn't tell you that. So, for example, if I employed AI to predict smoke movement in a, let's say, buoyant plume, predict smoke movement in a, let's say, buoyant plume. It could actually, in the meantime, tell me that it works when you assume the gravity is as on Mars, and then it works. While, in fact, it's just a matter of entrainment coefficient that is elsewhere, which could accidentally be the same number as the ratio of gravity here and on Mars. But the algorithm would never know what happened. It just used used this and it worked and for them it's perfect for engineering you need to understand and this uh so breaking it into steps and seeing the more or less what has been done gives you this let's say higher power to unravel these hidden patterns exactly and you care less about advanced statistics. Exactly. And the other thing is, at least in my eyes, if I know how the algorithm sees the problem, I might be able to figure out a new phenomena or sub-phenomena that they haven't known before. And maybe this is why American methods, they're by design very conservative because we have to be conservative. But maybe we could, if we know why, we don't have to be extremely conservative. Plus, we know now something new that we didn't know before and we can figure out why this thing. When I think of AI, I always think of a tool that can give me an answer to why this thing, to why I didn't know this before. What's this new knowledge to me? I'm not really looking for correlation. My earlier work was heavily data-driven correlation, because, I mean, I didn't know better, but nowadays it just makes more sense. For some papers, of course, data-driven will work, because the paper itself is for a data-driven problem. But the overall idea should not always be data-driven. It should be much more than that. It should always be advanced science. How can we advance our knowledge without having to spend thousands and thousands of dollars? And here's the thing. Somebody that does an experiment now, 15 years from now, it's forgotten. Somebody goes back and repeats the experiment, and they get a grant to redo the experiment again, or they don't expand on the experiment. Our papers, when we publish something, in a way, after a few months, it's shelved away. It's in a database, a few months it's shelved away it's it's you know in a database and science directors bringer it's being online we rarely visit but why do we have to continue doing this cycle all over again if we accumulate our knowledge and we are able to come up with something new then we can explore different directions that we haven't seen before i've started with classifying this ai into supervised and supervised semisupervised, and you were talking about regression, classification, and other ways to formally classify this. But I think the true first choice is do you use it for discovery or you do it to calculate something? And I think that's the first thing, because if you just want to figure out a number out of a very complex array of results you have obtained that you're unable to process in another way because the correlations or something are multidimensional, then you probably are seeking a different path than when you try to employ this method to find unexpected and discover something. And as an engineer, I would like to have better numbers. I would not necessarily be happy discovering a completely new failure mode because I'm either wrong or we're kind of screwed as a humanity if I do. But as a scientist, I maybe care less about the numbers and I would care more about discovery. And coming back to your thought about collectively adding to that, okay, to what extent the data from the past is useful? To what extent you can take papers from 50s, 60s, 70s, I don't know, from last IFSS and use them to develop your own models? How big of an issue is that? The thing is, because when we talk about FHIR, it's a very niche area and it's a very expensive area. We don't really have a lot of experiments or large tests that we can use, but we do have some. So if you want to start with machine learning with a goal to come up, let's say, with a black box surrogate that tells you failure mode or failure time rather than doing a very lengthy calculation method, you really have to choose what you have. And those would be experiments, the old experiments. Now, the good thing is the following. The good thing is those experiments are the same ones that we use now, standardized fire curves. So, you know, in a way, we have some kind of similarity. On the opposite side, material is different. Like, for instance, concrete 50 years ago is really different than the concrete we have now. So the experiments 50 years ago, which is also, which is what the codes are built on. Codes are not built on new experiments. I'm happy you've added that. Codes are built on very, very old experiments, in the 60s, 50s, 70s. So even the code, why would you apply the code now when it's 60 years later? How does that accumulate to what we have now? So in a way, I know it may not be as comprehensive or as accurate as doing the knowledge that we have now. However, this is the practice that we're using. the knowledge that we have now. However, this is the practice that we're using. And if you want to compare, if you really want to compare, let's say, a coded procedure against machine learning, to be fair, you have to kind of use the data that developed that coded provision, which is the old data, and apply it to the algorithm and see the comparison. You have to have a fair line of comparison, too. So this is how it works. But however, am I happy with using 50-year-old experiments? I'm not happy, no. But this is the ones we have, and this is the standard we have to use. Maybe in the future it will be different. On the good side, on the other dimension, using older experiments, and let's say that you have two columns, one very, very old, one very, very new. The failure mode is not going to be something new. It will still fail in the same manner. However, the time to failure, it's going to be different because we have different chemicals, we have different stuff now that we use in our material. Different loading, different temperature. However, the environment we're subjecting this element to is the same. It still has the same chamber compartment. So it's not really that we're completely using something different it's just there are some differences and even if you want to do like a statistical analysis like a meta-analysis you have to compare different data from different experiments and this is again this is why using data-driven analysis is a little bit you know itchy for me now because i want to know why. At least in my mind, I want to know why this column fails. So if it failed 50 years ago or now, the mechanism is going to be some new physics. It's going to be something that maybe we haven't seen before. So how can I get to that something that we haven't seen before by using the same old methods that we have been using for 50 years? By now we would have figured out something new. So maybe if we use something, a new method, maybe we can see a little bit different, and maybe that little piece of difference would open up a new experiment for us or a new research area for us that we can apply and use. That's interesting. And for me personally, I really like Dixian. I'm in the world of smoke control, and I really love how he perceived that these CFDs could lead to, let's say, more capable algorithms that would predict the smoke behavior in a compartment, giving you a number, the time for the layer to fall down or some tenability criteria to be breached. And, for example, one of my main areas of research is car parks. I engineer a lot of smoke control in car parks. And our limitation is usually that we take a car park, we do two, three, four, five CFDs in it for a certain size of the fire. And I assume if I did a sufficiently large amount of these simulations, and then I have received a new car park with a new architecture which I know I've performed one two three simulations in that car park the algorithm could technically take over and tell me what would happen in like a thousand different scenarios in that car park could you use it in like this exactly for instance now what we're doing, we have a database of, let's say, columns, 200 columns. We could come up, we can ask the algorithm to simulate a data worth of, let's think, 5,000 columns or 10,000 columns. So this way, I'm trying to capture as many interactions between the features or between the parameters as I couldn't have done using experiments. experiments. But however, I still have that baseline that at least tells the algorithm, this is the min, this is the max, this is the average of the distribution of the possible observation that I could see before. And that's the thing. Any problem that you're going to be using simulation for will be expensive. And not only expensive, you'll have to continue to do it over and over again. And, you know, once you're done with let's say with your design, you throw away this, you know, maybe you clear your desks and throw it away. But if you have this, let's say, on an annual basis, and let's say you design 50 structures, or like 50 cases, the simulation that you have is very valuable information, because if you accumulate them by 5 or 6 or 7 years, you'll have a very, very good database that you can teach an algorithm. Maybe figure out something that we haven't seen before. Maybe come up with some kind of a faster approach to solve the smoke problem or to figure out at least what could be, and this would be interesting to me, what would be a severe case for this working structure without having to pass it or doing many many you know cfds before i may be wrong but do you exactly know which one would be a severe case right now of hand it is expert judgment and you use design fire there's a thing because if you could use this technique to expand the number of investigated cases you can start talking about risk and probabilities of investigated cases you can start talking about risk and probabilities like a fire of this probability is giving you these consequences with this confidence and the fire of this probability is giving you these consequences at these intervals and then you could and what's beautiful you could ask the ai please test any smoke exhaust capacity from this amount of CFMs to this amount of CFMs. And then it will tell you, okay, if you increase the ventilation twice, you decrease your probabilities by this amount. And if you increase it sevenfold, it doesn't change much from the previous case. So you start to get much more detailed outcome of your analysis than you would have from investigating multiple points, even if you are the best CFD engineer in the world. Because it's not the tool that limits you, it's the capabilities of running multiple parallel cases that essentially limit it. And there was a solution to that. There exist solutions to that. There was a PhD student of Bart Merci, now Dr. Bart van Weyenbergen, who was doing his PhD on response surface technique. It's a statistical technique where you can map certain inputs to certain outputs, multidimensional surfaces. And from that, you can, by running, let's say, 10 CFDs or 20 CFDs, you can predict the outcome of multiple CFDs. But it still requires you to solve for a certain geometry. And here, machine learning, maybe you could use results from different buildings to enhance your knowledge about this particular building. I mean, it's amazing because already the response surface seemed like magic. And this is magic plus. If that happens, it's going to be amazing. I agree. And I really wish it happened. It would very much. So if I wanted it to happen, what should I do now? Should I go learn coding? What's the first step? And let's assume I don't know anything about coding. I don't know Python. I don't know anything about coding i don't know python i don't know r i don't know anything but i just love this where should i what should i do with myself i mean to be honest i learned everything on youtube they have five minute tutorials for every kind of algorithm for everything so you can easily learn from them the good news is at this moment we don't really develop algorithms so there are many many codes like if you go to scikit there is like the codes are already there so you can just copy paste them add your data run it and then you know you can fine-tune a few parameters you should be good to go it's not it's not really that complex once you start to go maybe into explainability confidence trust then you have to have some kind of a very good background when it comes to math or calculus because there at that point it's not just as simple as applying an algorithm it's more on the development side once if you want to figure out causality or for instance cause and effect this is at least what i'm trying to do if you want to figure out cause and effect then you have to have much more higher advancements for coding so the bottom part i mean this is what I tell my students. I'm not really expecting you to create a new algorithm. If you can do that, that'd be great. However, the algorithms we have now can't solve many, many, many problems. And all what you really have to do is two things. One, understand how the algorithm works and its assumptions, its limitations. And know how to apply it. You don't really need to code it by hand because the codes are already, they're available online. You can just copy paste them from there. You have to find your data and apply it and then you will see if you apply it. I mean, I did this experiment in two of my papers. I took five or six algorithms and I applied them by default values. I just copy pasted them on our data. Of course, I mean mean you get 95 to get 90 percent with very very cheap resources which tells me that you could basically apply the same algorithm for different problems and you are gonna get very good results too not all the time but at least for the most of the time because these algorithms are extremely powerful that's a relief in a way you know and i i had matt uh bonner as well in here and he told the same thing that there are algorithms that exist and you can apply them xenian said the same you're the third person to tell me the same so i must build my brave and and just try i guess that's how i learned programming, actually. Just keep trying and do as many mistakes as you can, and eventually it will work out. I'm going to be teaching a new course next fall on machine learning. I'll send you links for my lecture so you can attend that. Oh, really? That's so cool. I would appreciate that. For the end, I'm usually referring to resources, and you have your webpage that is very rich in resources. So I will also link to that. And you had a paper in Fire Technology about different types of machine learning that can be used in Fire. You had this Engineer's Guide to AI and Automation in Construction, which was a very interesting case study, and it was a really nice paper. What else should I refer the audience to to read up on this? I really feel for FIRE, there is going to be the mechanistic review paper is a very good one for a beginner. I know I sent out Professor Rain like a very short letter. It's going to be published very, very soon. So that would be a compliment to that one. Once it is, I'll send you a link for that. Engineer's Guide is one of my best, at least when I think about it, this was the highlight of 2021 for me. That paper is the one I'm very... I really like it. Even the title, I spent a lot of time on the title because I felt it would be something very close to my heart. There is a third paper would be mapping function. So I think it's after mapping function. So this is where we're trying to use more of a cause and effect kind of machine learning, or how can we arrive at that cause and effect without having to hassle with coding. We can actually figure out pathways between different algorithms and come up with a function or mathematical expression that can convey to us some kind of a formula, or at least can give us, because if you think of the output of an algorithm, it's a number. For us engineers, we're trained in formulas. We see that, for instance, this is the formula that you can apply to get an output. Machine learning gives you a number, and hence, this is the hesitation. We can't see hesitation. We can't see why. We can't see how it works. So in mapping function, it's a way that it can translate algorithmic logic from a black box into a function that we can see. And if you can see it, you can see the interaction between the parameters. You're likely to feel much more comfortable applying a function as opposed to applying a complete black box that we don't know why it does. more comfortable applying a function as opposed to applying a complete lightbox that we don't know why it does okay that's really really good and some external uh resources like maybe a youtube channel or something that you can recommend yeah i can send you these i have them in my bookmarks i'll send you i'll send you a link fantastic i'll put it in the show notes and i i hope someone will find it useful and i really i I really appreciate you sharing this knowledge. Okay, Nasser, that was a great talk. And I've learned something about AI today. And maybe I'm one step closer to understanding how it can be applied in my field. And I guess there's many ads buzzing now. How can this be implemented in their field? Okay, thank you for joining us in the Fireside Show. and I hope you had a great time. I had a lot. Thank you very much. I appreciate you reaching out and I appreciate your show. Very good. I mean, I always watch the shows when you post them on Twitter. It's a very interesting, insightful. I like that you not just do one thing. It's like different components within the fire part. So it's much more informative this way. I appreciate that very much. Thank you so much. Cheers, man. Bye-bye. And that's it. Wow, what a discussion that was. Maybe I just should open some Python right now and then start digging into that. I'm really excited about this world of fire science and the possibilities it brings. MZ has used AI in so many different aspects of fire engineering. Like literally go to his webpage and check out his papers, the variety of topics where this method was used and considered useful. It's just amazing how wide this technology is. Of course, there are caveats. You need to worry about the data quality. You need to worry about what the algorithms have not seen. I hope you've picked up these things from our discussion that technology is powerful, but just as powerful as the algorithm and as powerful as the data that fuels it. And by far, most importantly, as powerful as the person who's using that. So if you just don't know what you're doing and you drop machine learning on that, well, you're going to have machine learned, no idea what you're doing. But if you know what you're doing and you know what you're looking for, it's just a complex problem to dig into that. Well, then machine learning and artificial intelligence may be your best future friend. Now, this talk today, I think it's a part of a mini-series in the podcast. If you remember, I had an episode with Xinyan Huang from Hong Kong Polytechnic University, with whom I have discussed artificial intelligence and its potential use for smoke control and fire engineering at large. So you definitely, definitely should check that episode if you've missed that one. And I had an episode with Matt Bonner, my friend from Imperial College London, who has also used machine learning algorithms to investigate database of facade fires that we have built together. And it was also quite interesting to see how well the artificial intelligence has carried the task that took such a long time. So I'm really, really happy to have this in the podcast portfolio. I think these three episodes go together very well. And yeah, if you haven't heard them, absolutely after this one, you need to tune in to Ksenia's episode and Matt's episode. I'm going to drop the links in the show notes. And yeah, that's it for today. I hope you've enjoyed it as much as I did. As usual, next episode will be waiting for you here next Wednesday. Looking forward to that. And yeah, see you around. Thank you for listening. This was the Fire Science Show. Thank you for listening and see you soon.