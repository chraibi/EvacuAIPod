 Hello everybody, welcome to the Fire Science Show. Today I am excited. Well, I'm always excited for fire science, but today I am double or triple excited because we're gonna go as fire science-ish as fire science can get. I have invited a guest PhD student from University of Edinburgh, David Morissette. He's about to submit his PhD thesis. Good luck, David. And David has researched a very interesting subject that is the uncertainty in the measurements of fire. Not that many people try to touch this difficult and challenging subject. And it's not just developing estimations of how uncertain the measurements of the most basic parameters like time to ignition or heat release rate are. It's not about finding what's the standard deviation and average, etc. It's about understanding where do those deviations come in from and what do they mean for fire engineering. This is an extremely interesting talk because it touches the fabric of what we are doing as fire safety engineers. From this episode, you'll gain so much understanding about what a design fire is, what's a fire experiment, and how does the fire experiment translate into a design fire, what goes into it, and why those fires come in phases and actually taking into account the physical phenomena and phase transitions that happen during fires. Why is that critical for our understanding of fires and being able to model them? In this episode, we'll go from David's first experiments on PMMA into his extremely interesting experiment series on upholstered furniture. his extremely interesting experiment series on upholstered furniture. And over all those cases, we will discuss uncertainty, fires, how do they grow, how do they develop, what's happening during the fire, and how do we quantify that. I am sure this will be a goldmine for practitioners, especially those who are dealing with modeling fires. So let's not hype this anymore. Let's spin the intro and jump straight into the episode. Welcome to the Firesize Show. My name is Wojciech Wigrzy≈Ñski, and I will be your host. This podcast is brought to you in collaboration with OFR Consultants. OFR is the UK's leading fire risk consultancy. Its globally established team has developed a reputation for preeminent fire engineering expertise with colleagues working across the world to help protect people, property, and environment. Established in the UK in 2016 as a startup business of two highly experienced fire engineering consultants, the business has grown phenomenally in just seven years with offices across the country in seven locations from Edinburgh to Bath and now employing more than a hundred professionals. Colleagues are on a mission to continually explore the challenges that fire creates for clients and society, applying the best research, experience, and diligence for effective tailored fire safety solutions. In 2024, OFR will grow its team once more and is always keen to hear from industry professionals who would like to collaborate on fire safety futures this year, get in touch at OFRConsultants.com. Hello, everybody. Welcome to the Fire Science Show. I'm here today with David Morissette from the University of Edinburgh. Hey, David, good to have you. Thanks, Porcek. Thanks for having me. Man, I'm a big fan. Thanks for coming to the podcast. You're touching some of my most favorite things in the fire science, which is the studies on flammability, design fires and burning real items. So I cannot wait to have this conversation. But first, I need to ask you where this all have started. So when did you figure out that going so deep into the studies on how things burn is the pathway for you in the fire science? Absolutely. Well, thanks again for having me. I'm a longtime listener of the show, so it's really exciting to be here with you today. So you're doing great work with this. Thanks, man. Anyway, so as you said, we've been doing a little bit of work, sort of looking at this idea of what is statistical variation, what is statistical uncertainty when we're looking at fire science, right? So I'm a bit of an experimentalist, sort of that's what I do with my PhD studies at the University of Edinburgh. And so I guess before we start talking about what is statistical variation and how many trials do you have to do? I think an interesting question to start with is, why do we do repeat trials? So this idea of, you know, if I do an experiment, usually don't just do it once, right? Or actually in fire science, we're not really doing repeat trials. Why should we do repeat trials and how important that is, actually? Sure. But I mean, if you read any, especially in the flammability literature, if you read, you know, experiments on cone calorimetry or if you read standardized testing procedures, there's a specified amount of repeats. Right. But this idea of doing repeats actually comes down to a simple idea. Right. The reason we do repeats is this idea of capturing statistical variation, right? So that's the mechanism by which we do that. We just specify a certain amount of repeats. So for most procedures, we'll generally take a few trials, and we'll usually take an average of those results. And then we hope that those trials that we've captured actually sort of capture the statistical variation that you might expect for that experiment. Effectively, it's a consequence of just like fear of the unknown, right? We're hoping that we didn't just get a one-off sleep. And so that's the whole idea of doing repeats. But if we just take, you know, n number of trials, that's an implicit strategy for sort of mitigating your statistical variation. So things that we were trying to look into is how can we do this explicitly and like very intentionally say, I have a target uncertainty of blank, right? And I want to hit that so that I can then report that. And so people who use my data know that this was the level of certainty I have in my data. It's a very powerful sort of tool. But anyway, like you said, there's some studies on that. But let's talk a little bit about the origin story of that. So I did my undergrad in California. I was a mechanical engineering student at Cal Poly San Luis Obispo. And we took this one lab course that I'll always remember because they handed you a box of like 100 resistors, you know, the electrical components, right? And they asked you to take resistance measurements of 100 resistors, right? And then we did some very like rudimentary statistics on it, sort of like the mean, what's your confidence intervals, things of that sort. Now, obviously, I was like a second year undergrad. That was pretty boring in my mind at the time. But what's really interesting is something that kind of stuck with me is this idea that I could take a resistor out of this box, right? And by the end of this exercise, I could say with 95% confidence, I can tell you where this falls in the range of your mean plus or minus a certain value. And then I started doing some research at Cal Poly with Rick Emberley, and he's still a professor at Cal Poly. And as part of my master's thesis research, I was looking at ignition of PMMA in the cone calorimeter. And so one question I started sort of dabbling with is, okay, how many trials should I do? So I opened up the ISO standard, the ACM standard. And so I used to do, you know, three trials or whatever. Three, okay. So that's sort of what the standard procedure is. And so I was just cracking all of my experiments and I would say, get lunch with some of my friends that were doing biomedical research or something like that. And we'd be talking about our experiments and someone would say, all right, so what's your sample size, for your study? I'm like, oh yeah, we're doing three repeats. And people would always, and they'd start three repeats. And people would always laugh hysterically, right? Because they're running hundreds of experiments for a single condition. I mean, in the world of real science, three must be a really silly number, because if you just went with, oh, I just do one, then the reason would be, he doesn't care about uncertainty at all. But if you tell those people you're doing three, it's like, you're kind of trying, but you're not really there. Exactly, right? I can imagine. It must be funny. But come on, PMMA is something repeatable, right? I mean, sure, right? But in terms of when we say, like, what is repeatable? Like, how long is a piece of string, right? I mean, it's a question that depends on the context, right? So, yeah, so, I mean, I'm doing three trials, but the more I talk to these people, I had that sort of ringing question in my head, going back to those resistors, right? And this idea of, well, at what point do I have enough data? How many trials is enough, right? So that stuck with me for a little while. And something I should say, too, is obviously because the cone calorimeter has been around for a while, there's been extensive studies on repeatability and reproducibility. And so obviously there is some degree of a basis for which we have an idea of how repeatable certain experiments are. But what I couldn't find is I couldn't find a study that just did a huge number of trials for a single case to just sort of explore the idea of what happens if you have this big data set. And so if you fast forward a little bit more, back in 2020, I had the chance to go to the University of Edinburgh. So I was just visiting to do some experiments while during my master's at the time. And so I got to meet my now PhD supervisors, Angus Law and Rory Hadden. And while we were there, we were just in the pub one evening after working in the lab. And we were just chatting about doing fire experiments. And this idea of how many trials should I do came up. And so we started talking and I was asking, so how many trials do you think would be enough? And we got to this question that said, well, what if you had 100 data points for something really simple like Black QMA and the Cohen-Coward image, right? And so then we're chatting around the table and like, you know, that's reasonable. Let's give that a try. And what was funny too is across from me was was Rory who I know has been on the show a few times right and so I was like you know well 100 trials Cohen's pretty straightforward I bet you can smash that into maybe about a week you know you know and so he kind of gave me a little laugh he's like he basically made a little bet that I couldn't do 100 like good trials in a week and what I have to do I had to go in on Monday morning and with 100 slabs of DNA.. But your student, he cannot do it. It's like the most brilliant supervisor strategy I've heard about. I'm going to implement that. Anyway, so yeah, absolutely. But yeah, so we ended up with the first 100 data sets, 100 data points, just black PMMA in the cone calorimeter. And we were looking at time to ignition, mass loss rate, heat release rate, sort of all the different data that you get out of the cone because you get quite a bit of data out of it. And for people that are familiar with that kind of experiment, it is straightforward, but there's a lot you can actually discern from that information. And so once we looked at the 100 data points, we started knowing some interesting trends, and so we thought we'd expand it a little bit more. And so we ended up doing 100 experiments at three different heat fluxes, so we did 20, 40, 60 kilowatts per meter squared. And this turned into an interesting little research project that we published a paper on in FireSafe's journal. I'll link to the paper in the show notes. However, it's kind of interesting. I'm looking at the scatterplot of the time to ignition that you've got and the histograms you've got. And, you know, one thing that hits me, that strikes me, that you've said that normally you would use like three samples. Like you just go with three. Let's say your sample one, two, three, and you get some value. Those dots are not that far apart. But there are quite some outliers. There's quite a scatter, maybe not a massive scatter. It's not like the things are all over the place. They go into some sort of natural distribution, more or less. But I can imagine picking three of those, you know, and having completely different final outcome than three others. And that's a good observation, right? Because even for, you know, our sort of quintessential black PMMA in the cone calorimeter, like how like quintessential fire science can you get? quintessential black PMMA in the cone calorimeter, like how like quintessential fire science can you get? I mean, if you look at the, like you said, like look at the time to ignition for 20 kilowatts per meter squared. If you look at the actual peak to peak sort of like fluctuations, you're getting upwards of 20% error, right? So if you took three data points, any three random data points, there was a chance, however small, you know, actually, you know, if you run the numbers, it's not as small as you think. You could get those three points that are off by 20%, which throws your mean value off by quite a bit. And so this idea of there is a possibility if you're only limiting yourself to three that you could be off from the true mean. And this has implications to all sorts of things, right? Because sure, for engineers listening to the podcast, time to ignition of PMOM cone calorimeter isn't necessarily something that is going to propagate itself into engineering design. But what might is things like the parameters that we calculate. You know, what is the heat flux? What is the thermal inertia of these materials? And a lot of these things are obviously based on these experiments. Right. So if we don't account for this potential fluctuations, then how do we account for that when we start propagating that into our parameters? potential fluctuations, then how do we account for that when we start propagating that into our parameters? Going like even more fundamental, how uniform were the properties of the PMMA over the samples that you had? Was this the same slab that you've cut into smaller pieces, like the same density, same thermal bulk and everything? Absolutely. So we ordered it from a single continuous sheet of PMMA, right? So that was our attempt to sort of get as repeatable in terms of the properties as you could get. But now that all starts to go out the window if you start looking at things like timber, right? Or any other sort of materials we're going to have in homogenous properties or other sort of inconsistencies. There's actually a really interesting master's thesis that just came out, a Cal Poly by a student named Jacob David, who looked at basically the same sort of things, looking at hundreds of cone tests, but with timber, right? And so that's something we're trying to look at right now. Yeah, so anyway, so going sort of back to our main sort of data set on PMMA, so these fluctuations that you see in like the 20 kilowatt per meter squared case, for example, could be upwards of 20%, right? But as you start to increase heat flux, right, you start seeing for time to ignition, obviously, your nominal value for time to ignition goes down, but so does your fluctuations. So based on the fact that there might be those three outliers, you know, for the 20 case, and so, you know, maybe three isn't enough, right? But all of a sudden, when you go up to, say, 40 or 60, now the fluctuations are going down quite a bit. Maybe you get to a point where maybe three, four, five trials, maybe that is enough. And so we got to this question that was basically, how do we discern a threshold? Because, I mean, a funny thing that people come up to me at conferences and stuff, and they'll be like, so do you think we need to run 100 experiments? Sort of like, you know, what I'm trying to do. And I hope that no one reads the paper, and I hope that's not the outcome that people think reading it. Because, I mean, we created the data set so that we can run sort of the analysis on it. But the reality is, is you don't need to necessarily do 100 experiments. But the point is not the number. Right. The point goes back to sort of that original question I sort of asked in the intro. And it's like, why do we do repeats? Right. We get to this point that we want to capture statistical variation. We want to know our confidence in our data set. And so because of that, we started playing around with this idea of optimizing your uncertainty. And so you can read the paper for a little more detail, but we basically took this formulation of what we considered the distance from your true mean value to what we'll call our 95% confidence interval. And that scatter we refer to as our statistical uncertainty. Basically, with the data set that I have captured, how well am I capturing the true mean of sort of my data behavior? And so this behavior follows what we would say is a 1 over the square root of n behavior, so n being the number of trials. So as I collect more data, it's a nonlinear process. If we think of that progression instead as sort of what we'll call the marginal gain in certainty. So if you take sort of a derivative with respect to n for every additional trial I'm taking, how much certainty am I getting back? You can take that function, you can kind of optimize it. What you can do is if you plot that function, what you start to realize is going from, say, three to five trials brings you so much more certainty. Basically, any case that you're looking at. But to go from five to ten will give you less. Well, the jump from one to three is already a massive one. So I would say that the reason for having three is already, there's a good reason for having at least three repeats. But again, adding repeats adds more. It's not that every repeat adds the same as the previous ones. At some point, the returns start to diminish, right? Absolutely. So exactly. There's this sort of law of diminishing returns. So there's a certain point at which you're no longer, because the reality is, is we have to optimize our finite time and resources. You know, for every experiment I'm doing, I'm dedicating resources to doing that. That could have gone to a new experimental configuration. So with this idea of sort of what's our marginal gain, we started seeing that for most scenarios, you bottom that out 10 to 15 trials. And the last thing I want is to start claiming a new magic number, right? Because that changes, obviously, depending on your scenarios. scenarios. But somewhere between three going up to about 15 is where you start to optimize these balance of your variation and the sort of this addition of how much certainty am I getting back with each additional trial. Well, with the higher heat flux, those returns are less and less valuable. Perhaps if you calculate it by percent, they're still there. But in terms of the value that you're measuring, the returns are smaller and smaller. Now, another question that I have is that we put some kind of artificial ways of measuring this. Like, we love to measure fires as a function of time. It's kind of artificial because that's a process. It's not a thing that you can put on a time scale and measure how much time is that. It's a process. It has to develop. It's not something that is an explicit property of this material. The same way with like heat release rate. It's also an artificial construct. The mass loss rate. It's an outcome of a process. I wonder like how did those different measures fit into this idea of of uncertainty and can we even be certain of heat release rate or is it like inherently uncertain that's a great question right so because everything that right now we're keeping things in a very small box talking about this pmma study right because we're taking single metric right of like time to ignition time to ignition yeah i mean time to ignition i love Like, it's perhaps the least float. Actually, perhaps the least float of them all would be the total heat release. If you burn a sample completely to zero, you should get the least uncertainty out of that because all your material has reacted and your variability is within the efficiency of combustion to some extent, right? variabilities within the efficiency of combustion to some extent, right? So the time-to-ignition is also something that, to me, talks like, okay, this is a material that starts in ambient quiescent conditions and is subdued to a very specified, steady heat flux. And eventually, the processes in the material cause it to ignite. For me, it's like, okay, the material starts in the same spot. You know, we always start from the same zero, same ambience, same heat flux. So the process in the material should be the same each time, more or less the same each time. I absolutely agree with you, right? And something that I think we can sort of move to was the next sort of work package we did with this idea is, this is cool, everything on the cone scale is interesting, but the exact thing that you're talking about is things like heat release rate, right? We're looking at time-resolved information. How do we apply these kinds of ideas to that, right? Because these are the kinds of data that are very familiar with practicing engineers, right? This is what engineers need. This is what engineers need, right? This is what engineers need. This is what engineers need, right? If you're developing a design fire, if you're putting this into your FDS model, into your sprinkler calcs or your, you know, whatever, your ceiling jet correlations, you need a heat release input, right? And you need to understand what are the magnitudes, what are your transient aspects of your curve and all those things. So absolutely. So the next logical step was to scale this up to a complex fuel package. Let's come back to the idea of total heat release later. Remind me to come back to that because that's an interesting point that comes into play later. Let's start by talking about the cliffhanger. Keeping you on your toes. But time results heat release rate is obviously a natural place to start. When you think of a fuel package that you want to first thing that comes to mind, what do you think of a fuel package that you want to like, first thing comes to mind, what do you think of? I see upholstery. You see upholstery, exactly. Yeah, it's like a couch, a couch armchair, mist armchair. That's what we have in buildings mostly. So exactly, right? So when I was thinking about this, the first thing that came to mind was, you know, upholstered couches, upholstered chairs, right? So I wanted to go as close to that as possible. And so what we ended up doing is I applied for an SFP Foundation research grant, one of the student research grants, and they were generous enough to contribute to the project. And so I went and procured basically 50 identical upholstered chairs. And so the idea was I wanted to get as close to the real thing as possible. I wanted an actual, you know, upholstered chairs and so the idea was i wanted to get as close to the real thing as possible i wanted an actual you know upholstered chair and this is sort of your typical one-seater no armrests polyurethane foam um with a wood frame that's kind of deal and there's some interesting discussion before we did this about what kind of what you referred to there is like the nist armchair right so nist has this really slick setup where they have the steel frame for like an upholstered chair, and they can seal it with their own homemade materials, highly controlled materials. And so listeners haven't read their research. That's really interesting stuff on upholstered furniture fires. And there's a good podcast episode on the NIST calorimetry database, so you should definitely go check that out after this one. There you go. Exactly. And while that was an idea, and I think what they've done with that is excellent, we really wanted to sort of go to the ultimate idea of not realism, because what is real when it comes to a design fire? But we wanted to increase the complexity and actually take a manufactured upholstered chair. Anyway, so that's what we ended up doing. And so our core sort of data set was 25 repeats of this upholstered chair in our furniture calorimeter under the same conditions, and we ran it 25 times under those identical conditions. And the idea was to start looking at the variability. And so our ignition source was, we had this upholstered chair, and we had just a small Bunsen burner, because there's a million different ways you can try to ignite these chairs. But we wanted a very sort of, your minimum input to sort of kick this thing off to go to actually ignite and burn. So we used a small sort of, you know, point-setting kilowatt Bunsen burner underneath the chair in the same location. And so before we go into the day too much, too, I should also give a shout-out to the students who helped me out with that. So I want to give a shout-out to Johnny Reek, who's a fellow PhD student at the University of Edinburgh, and my friend Ian Oshwang, who were both essential in helping me run all these experiments, because 25 repeat hood scale experiments is not a one-man job. I can imagine, yeah. And so the results, and for anyone who wants to see this, we recently published a paper in Fire Technology. Now, obviously, the results are a little more complex than just looking at a scatter plot of time to ignition data for this stuff. And so the degree of complexity was to the nth degree now. Now we're looking at time-resolved transient burning rates, right? And so one question you have to ask is, okay, what does a heat release rate of a complex fuel package even really represent? There are variations. What's causing that? The heat release rate that you're seeing is some combination of the rate at which the flame is spreading over the fuel package, the rate at which it's actually burning in situ, what is the heat of combustion of the resulting pyrolysis gases. You know, all of these sort of different parameters play a role in the heat release rate. To clear out one thing, it was a chair put under a hood or was it like a room corner apparatus? What was the setup? That's a great point of clarification. So we were just burning these in an open furniture calorimeter. So literally you're in an open space. No feedback. Something actually changed later is we actually did run some experiments in a room corner with the same chair. And I can get to that. Click under two. Okay. I put it in here number two. in a room corner with the same chair. And I can get to that. Cliffhanger 2, okay. Cliffhanger 2, we can go back to that at the end. But yeah, so these chairs obviously show a highly sort of transient behavior. So each one shows this very clear, once you ignite it, there's a clear growth rate, there's a growth through the heat release rate, there's a decay rate, and eventually the chair burns out. And there's one figure in that paper that I love, which is just all 25 heat release rate curves just were smacked on top of each other. I'm sure, I think you might remember seeing it in SFP Berlin. It's what I've been referring to as our spaghetti plot of just all the curves on top of one another. And so you end up with that. If you look at that, you end up with a substantial degree of fluctuation between the trials. And if you sort of take any case, you can sort of see each individual curve going up and down throughout the plot. But if you take that sort of central region of the data, you have a peak heat release rate around 300 kilowatts or something like that. But if you take that central region, at any given time, you have a scatter of almost 200 kilowatts in time. So the fluctuation is, if you're looking on a time basis, is very large. And I think one of the most important observations, if there's anything to take away from these experiments, is that if you look at this on a time basis, taking an average in time is kind of misleading, right? Because if we take any one of these individual curves, you can sort of see this sort of general trend of an increase to a peak heat release rate, a decay, and it sort of makes sense when you look at curves individually. And if you take two individual curves curves you can start to see similar trends in those behaviors but if you take an average in time then all of a sudden you get the output no longer represents any one of the individual input curves right and there's a very distinct reason for that that's sort of one thing to take out of this is if you're looking at these complex fuel packages these very realistic fuel packages, I can think of at least a dozen, and you must be able to, of studies where people are taking complex fuels like car fires, right, compartment fires. And if you're drawing an average through these, it's something to sort of keep in mind that there's a degree of sort of complexity in the transience of these. To give the listeners some reference frame, however, I highly recommend reading the fire technology paper or at least skimming through the fire technology paper. The link is in the show notes and it's on open access so everyone can access it and it's really great paper. It's worth to just go to like figure three and look at the heat release plots. The thing is that some of those courses of the fire you've got would be fitting to more or less medium growth curve. Some of them would be closer to a slow growth curve. I mean, the peak heat release, I'm not sure if it's difficult to read spaghetti, of course, but the peak heat release rate ends up at like 200-ish kilowatts. But it can happen as well as in like third minute of your test, as much as in like 10th minute of your test, right? And we are applying this to a transient problem of human evacuation in a building. I've now made the jump in my mind to practice as an engineer. I would take a curve like this. I would put it into my CFD. And I would say, okay, in this case, you know, my occupants have seven minutes to escape. And the other person says, okay, I've burned the same chair. And in my case, they have three. And who's right? Of course, the guy with the seven minutes wins because this design works and the other guy's design is magically incorrect, right? Even though they are conducting the exact same experiment in the same exact apparatus with exact same material ignition and everything. That's crazy. And I think one thing that's really important on that note is talk about, yes, we have these variations. And like you said, you can go anywhere from a medium to a slow growth in these curves. But why is that the case? I think that's something that we really spend a lot of time articulating in both the paper and presentation senses. But what's really important is if you look at these experiments, they look very different on a time basis. But each one of them follows something like seven key events that occur in every single experiment. And if you can identify those events, then all of a sudden you can contextualize the heat release rate. So, I mean, I won't go into all of them in detail, but I'll just pick a few, for example. I'll list them. I see the plot in front of my eyes. So there's a phase of horizontal spread, a phase of upward spread. There's a phase of in-depth burning, foam burnout, then a mechanical collapse of a chair, and then you enter a long smoldering phase. So it's kind of, if you look at the chair, it's like you're setting up the fire to the bottoms. Eventually the fire spreads to the rest part of the chair. Then it burns completely. And then eventually different stages at which the fuel runs away. So now going to your favorite phases. That's a perfect summary, right? So you notice all of these things happening in all of the trials. Every single trial follows this. They all occur at different times. So they go from that sort of horizontal spread where the ignition ignites the seat cushion, and then that seat cushion spreads to the backrest. That time period is characteristic of a certain kind of fire growth. And then the time in which the flames go off the backrest is characteristic of a certain kind of fire growth. On a time basis, they're all occurring at different times. But if you actually align, instead of aligning the data to the time, you start aligning it to these events. So, you know, line up all the curves so that, you know, they're in the same, in sync when the backrest ignites or when the backrest collapse at the end. Then all of a sudden you get a curve that, an average curve that actually represents the data that you get. So that's, again, in more detail in the paper. But by using these events, you create an average that actually represents each of the individual trials instead of sort of a smear across all the time result data. And then you apply some... Because I also see the time is uncertain in this plot. So you're not saying me explicitly when the spread changes from horizontal to upward. It's 192 plus minus 90. Yeah, exactly. Eventually, you have to pick up some average transition time and apply that to your curve, more or less. True. But I mean, the interesting bit of presenting the data that way is that you can effectively probabilistically recreate these events, right? Because if you sort of appreciate that the events drive the heat release rate, then what you can do is if I can predict when these events occur, I can start to predict the behaviors in the heat release rate plus or minus some degree of error, right? And so that's something that we haven't done copious amounts of work on yet. But this idea that you could probabilistically recreate complex fuel packages gives us a, you know, that's a new input that you could use as perhaps a design fire type scenario. And instead of starting to say that I'm going to take any one of these individual curves, I can say, okay, now I at least know what's driving the process, right? And so if you can appreciate that, then all of a sudden you can start to account for things that you haven't yet observed in your experiments. So once you go from just viewing it as a time based problem, so plotting heat release rate versus time, into viewing it now as a phase-based problem. Is the uncertainty in the heat release rate less? Let's start with that. Are you capable of grasping it better? Yes, that's a great question. Absolutely. So what we saw is once we started using this sort of event-based system, our ability to predict the uncertainty on our heat release rates went down dramatically. Now you actually had a pretty tight error bar on your heat release rate at any given event relative to that set of time. And so, of course, we're doing all this for this one very specific upholstered chair that we happened to procure for these experiments. But this idea can be applied to any complex experiment, right? So if we're looking at compartment fires or if we're looking at anything else where, you know, stochastic variations are causing fluctuations in our results by just simply pointing a camera at it and being like, oh, yeah, well, of course, because at this time, this transition always occurred. Really helps contextualize a lot of these variations that we see. that we see. So quantifying what's my uncertainty on my heat release rate becomes a question of less about what's my scattering time, what is my certainty in predicting this fire behavior. I love this because you're touching something that I have intuitively felt for a long time. I think we've even discussed this with Mike's viewpoint on the car park episode that cars burn in phases. It's not a thing you set a fire to and you get an outcome. And for me, there was always this critical events in the vehicle fires, like either the windows broke or the fuel tank has collapsed. If you look at any heat release rate curve of a vehicle fire, I would say with large certainty, I'm not sure if it's the appropriate word in this episode, but I think you can be quite certain that if you see a peak heat release rate on that plot, it's either the windows broke or the fuel tank broke and some fuel is released, whatever the fuel is of that car. And we're now also making a paper on that with my student Bartosz. And it's something, once we started going one by one on those experiments, it's something that we can confirm that it is like this indeed. And also, I should also give a shout out to those NIST experiments that we talked about. What they noticed was the acceleration to this giant heat release rate for their upholstered furniture fires occurred when the bottom barrier of their fabric cushion failed, right? Which they created a giant coal fire. So, I mean, other people are also, you know, are recognizing this. And I think that is a very powerful thing to say these key observable events then drive this process, right? So the practical takeout of that is actually that you can now create quite a reliable design fire, to be honest. Because if you care less about how quickly stuff can happen, you can focus on how big the fire in its different phases could be, you know? But this is a really interesting transition to sort of one of the questions you asked earlier, which was, was our experiment in an open hood or was it in a corner? Yeah, okay. I mean, we were able to sort of, I don't want to say mitigate, we were able to pretty reasonably characterize the uncertainty for this particular fuel package in this scenario. So now the leap that you have to make when you're saying, but now I'm going to use this heat release rate to then be my design fire, is that you have to assume that the fire that's going to occur in your space is the exact fire that's following the exact events in the exact same scenario. Now, in some cases, that might be a reasonable assumption, right? A single burning item in a compartment with minimal interactions with other items around it can probably be reasonably approximated under furniture calorimetry. But what happens to, I think you mentioned this earlier, if you put it in a room corner, what happens evolves now you're in the corner of a room or what what if i ignite it in a different location what if i you know put a throw pillow on it right yeah there's all these different flexes you gotta start thinking about for your actual fire that you're modeling so we did a series of experiments that um we briefly talked about in an sfpe extra article that was online where we put a few of these in a room, like a corner configuration made of gypsum wall boards. And we looked at the heat release rate differences. And what was really interesting is we noticed that, because if you're reading the literature, we know that if you put something in a room, like a corner configuration, you're definitely going to increase your flame heights. You're going to, you know, there's going to be less entrainment reaching it, so you're going to affect the temperature distribution in your plume. And some sources might suggest that it increases your heat release rate. By a factor of four. Some sources will say by a factor of four, exactly. But what's really interesting, it's all about, in terms of affecting your heat release rate, it's all about what are the events and the drivers that are then driving this heat release rate process, right? So there are some scenarios in which if you have that increased flame height and you have different degrees of radiation out to your field surface, that might drive the process differently. But for our scenario, what we found was the heat release rates for all of these wall corner configuration-type experiments fell within the scatter of our baseline configuration. So the heat release rate wasn't all that different. You got slightly faster growth rates and things like that. But effectively, if you're looking at this as a like-for-like comparison, they fell within the uncertainty margin of the baseline. But we also ran a bunch of experiments with heat flux gauges pointed at the chairs. What's really interesting is we noticed that there was a substantially larger heat flux, though, in the wall corner configuration. And so you from which makes sense, right, because you have taller flame heights. You now have a gypsum wall that's heating up. So that's going to radiate to some degree. But from a so that's cool experimentally. But thinking from an engineer's perspective. OK, so not I can't always just say that my heat release rate is going to increase in the corner. But something that you might be able to say is, well, we are definitely going to get more radiation from this though, right? Because taking that chair into a wall corner configuration took it from like 5 to maybe 10 kilowatts per meter squared, about a meter away, to 15 plus, right? So now you're pyrolyzing fuel. Now there's a potential for a secondary ignition. And so that's a really interesting sort of step change in a design fire type process. But again, connecting it to your PMMA studies, you're inherently reducing the ascendancy because you're exposing it to higher heat flux, right? Although PMMA was time to ignition, here's a burning rate. But I guess mechanisms could be the same. You eventually reach high enough heat flags where you just consume the entire thing at once, and you very quickly move from the spread phases into burning in depth phase where you have the peak, right? Yeah, absolutely. But it's an interesting sort of difference there, and we saw also, not to go into too many more details, but we saw very different results if you ignite the chair in a different location, for example. Okay. In certain locations, you'll get much faster growth to peak your release rate, which I mean, if that's, for example, if your growth rate is sort of the only part you care about. And a lot of things like sprinkler calculations, like I used to do when I was, you know, if I was like, you know, working as an intern in fire engineering firms over in the U.S., if we were running sprinkler calcs or DTAC models, all we cared about normally was the growth rate. And that gave you your time to your sprinkler activation. But we noticed the growth rates were way faster as you ignite it in certain locations versus others. So I guess as engineers, how do we deal with that? How do we know that the ignition source that's going to match our sign fire scenario is represented by the experimental data we're using. But there must be some repetable, like you said the size was the same. So I assume the peak heat release rate eventually was a function of the entire object burning at its maximum capacity, whatever that is. So that sounds like something not that much time reliant to me. So have you observed like the peak heat release rate overall being certain to a degree? That's something that we've been looking into, and that's a question I would like to answer. Because the idea of, I guess, first in order to answer that, you have to think about what is the peak heat release rate, right? And that's something that a lot of times I scratch your heads about. Is there a physical basis for what a peak heat release rate should be? Right. Because if you sort of do a thought experiment, let's take a couch first. If you ignite one side of the couch and you have horizontal flame spread all the way across that couch, as the flame is spreading, you're also burning within the depth of the couch. So flame spread is slow enough to go from one side of the couch to the other. There is a potential you'll start burning out the other side of the couch. Now, if you're in a room environment and you have like a smoke layer, and all of a sudden now you're preheating the couch a little bit, you might get faster rates of flame spread, which means you have the entire couch involved. But between scenario A, in which you have burnout and then the flame is still spreading, versus the entire couch burning, you're going to have a very different peak heat release rate, right? Because the peak heat release rate is this complex intersection between your mass burning rate, the area of the item that's flaming, and this idea of like, what's your effective heat of combustion? So we noticed some similarities because for a lot of these scenarios, the progression was the same. We ignited the seat cushion and the seat cushion was still burning by the time that the backrest ignited and it all started burning out around the same time. But that's not always like a guaranteed condition. So the peak heat release rates that we saw were all actually pretty reasonably, you know, within, I don't know, something like 50 kilowatts or something to that effect. We talked about it a little bit more on the paper. But this idea that has to be paired with this idea of, well, is a peak heat release rate an intrinsic property of a fuel package? And I think the answer is no, right? It depends on the scenarios in which it's burning. There's also the aspect of chemical composition as your materials are exposed to radiation and then to the combustion themselves. As you said, the heat penetrates into the depth of the solid material. And as you know from gravimetric experiments, there are those peak releases of gases and stuff. Eventually, you are past the peak and it's not as intense. So I guess that the theoretical peak heat release rate would be if you could capture the moment in your timeline where you get, you know, you the moment in your timeline where you get, you know, the most of your material into the peak mass loss from the TGA. You know, if you could create those conditions where everything reaches its peak of generating volatiles at the same time in the most unified manner, every material in your chair reaches their peak TGA the same moment, then you would have the 30. And it couldn't go any higher. Like the chemistry would not allow you to go any higher. Of course, in reality, there are many transients to that. One, the frame spread that you said, the heat flux, the feedback loops, even how long it was spiralizing again, because perhaps it was not spiralizing at its highest speed, but already has spiralized all the volatile matter that was there to combust. Like think about exposed timber that eventually charred off and now you have a solid material char layer and no more gases from it that easily ignites it. Perhaps if you somehow recreated those critical conditions in which it gasifies the quickest, you could get some crazy peak heat release rates. But then again, to what extent this, it would be an artificial construct. Like it would not represent reality because in reality, you'd never reach those conditions. And I mean, that's a really interesting point. And I think that one thing that we, that's actually a natural progression from that is the conversation you had with Lucas Arnold not that long ago, right? And talking about if modelers, for example, are trying to match, you know, an experimental condition, it depends on which one they're trying to match, right? So, like, are they trying to match the couch in which you've already burnt out half the couch and the flame's still spreading? Or is it the case in which you actually have constant flame spread? And what are the physical processes that dictate whether or not those two different scenarios occur? And are the experimentalists quantifying those things actually in agreement with the modelers? And it becomes really interesting sort of discussions that you see at the MacFP workshop, for example, through IFSS. So I think that's a really interesting problem, right? At the end of the day, we do have to figure out all these intricacies. really interesting problem, right? At the end of the day, we do have to figure out all these intricacies. Now, using my brain of an engineer, if I had to create a design fire from your concept of phases and your observations, I would simplify your phases into phases of growth and phases of steady burning. And I would honestly just take the averages of the heat release rate values, but apply them to the minimum times you got. So I would take the average fire, but the soonest I got, though I'm probably destroying the total heat release rate with this approach. Like you have to forget about physics sometimes in fire engineering. But I think that would give me a reasonable design fire because I think using the quickest fires, this is the worst case scenario. If your system works on that, if the fire grows slower, then you're capturing that as well. This is the thing that we've been spinning our wheels on thinking about quite a bit because this is something I ask every time we present this work. I like to ask the audience, I show them the spaghetti plot. I'm like, all right, so what do you choose? What's your design for it that you're taking from this? Because, I mean, my engineering brain also goes to the reality that you could envelope the whole curves, right? You could say I'm taking the absolute extreme, forget the total heat release. I know I'm breaking physics, but that's the most quote-unquote conservative way to go. But then you have to start asking the question of why am I even using real data? Am I trying to match a realistic sort of condition based on physics, or am I trying to create some sort of artificial engineering solution, which the alpha T squared type phi occurs, which are based on growth rates, based on real experimental data. But you are you are losing the precision for the sake of a sort of more general analysis. And so it's so when you're looking at these data, what do you do? And I think it also a really important thing to consider before saying this is the curve I'm going to take is saying the reason reason that we separated the growth phase into two different curves, for example, is because there's two different physical mechanisms driving that heat release rate. Early phase, there's horizontal spread, and for the later phase, there's upward spread. Both of those are in completely different time scales and grow at completely different rates. So if you want to drive an alpha T squared curve through that, you can do so, but you have to do so knowing that you're ignoring two different competing phenomenon, which is okay if you acknowledge that, so to speak, because you can't account for every complexity. But that's one way to sort of slice it. The alpha T squared framework is largely based on something like a radially growing fire anyway. So alpha T squared for the first half sounds pretty good. But then maybe you start looking at more like an exponential spread for the upward spread rate. So now all of a sudden you're doing so, eyes wide open to linking these to physical mechanisms driving the problem. I've already claimed in the podcast that for me, design fires are not representation of real fires, and I don't think they've ever been. It's more like a test that I apply to my model, like CFD model of a building, CFD model of a compartment, that's my model. Design fire is my test and I never thought this being a representation of a real fire, what's a real fire in the building. But for me, if I do a hundred shops and each of them I apply 2.5 megawatts design fire, you know, I have a pretty good overall idea of how conditions in these shops vary in this 2.5 megawatt fire. And based on that, I can do my engineering assumptions. And this is the type of fire engineering that I'm very comfortable with, even if my design fire is completely artificial. But if you go into fire engineering where you would take one random curve from one random paper that just fits your agenda and you put that into your CFD model where you have completely changed the domain, the feedbacks, the environment in which the fire is placed and you say this is the outcome of what would happen if a real fire real car burned in my real car park this is the the representation of reality this is fire engineering that i am very unhappy with because like my mentor it's something that really stuck with me my mentor professor chernets once asked me, do you want to be roughly correct or precisely wrong? And I'm like, it's like, this is the same case. Like, do I prefer to apply a design fire, an artificial concept into my car park and just have the outcome I understand and can interpret? Or I want to put a very precise piece of measurement that completely does not make sense in this setting, and base my judgment on something I don't even trust it being real. Like, I immediately understand that the moment I took, you know, this curve from laboratory and put it into my building, I've already made the decision to, you know, forget about physics, right? This is a challenging question for fire safety engineering. What design fire really is? Yeah, absolutely. The hard part is we need to, we still need to do our jobs. As fire engineers, we still need to be able to do the analysis and to put fire safety systems through the paces. But on the flip side of that, we also are in a field of safety, right? So our margins of errors have substantial consequences to people, property, and the environment if things go wrong, right? So accounting for some degree of variability, I think, is important. If nothing else is taken from this discussion, I guess, is the idea of if your inputs are based upon real data or even just sort of a rule of thumb that people are taking, appreciating the idea that actually we haven't done enough to show that that is right all the time, right? And understanding the times at which this, you know, this fluctuations might actually alter the outputs of your analysis is kind of a critical thing to consider for the sake of, you designing for safety because i mean yes you're right we can do all these amazing probabilistic recreations and do all this cool stuff with the data from our chairs but like is that actually gonna be the way to develop design fires maybe there is a way through which that becomes you know a way to to do reliable design fires but the concept of appreciating you know what you don't know about the data that goes into these design fires is something that I think every person can at least take away and sort of keep in the back of their head. Fantastic. We could conclude the episode in here, but there's still a one cliffhanger to be resolved, the total heat release rate. So was that certain? So there's two things that we looked at. We looked at the total heat release and the CO yield. Those are two things that I was hoping to look at. Because looking back in the cone data, plucking 100 trials of time to ignition, you can run some pretty simple statistics on it. I wanted something to pull from it and be like, all right, let's pull a yield and let's pull a total heat release. And the total heat release was, as you would have guessed, extremely reliable. So we were able to say, you know, it was within these error bars, and if anything, it actually correlated really well to the effective heat of combustion times the mass of all your things falling in the chair. We were able to show there was a physical basis for that total heat release. So it turns out, even through all these complexities and all that stuff, the time-resolved heat release rate, the total heat release actually did come back sort of as a consistency that you can kind of see with like the time to ignition data for PMMA. But what was interesting is we noticed something sort of of the opposite effect for the CO yield. So I would encourage people to look at the paper to sort of see this more deep. But if you actually look at the time resolved yield of CO over the course of the experiment, you'll actually notice that it's highly transient, right? So in the beginning, you have quite a large CO yield, and then you have a period in which you have a very low CO yield, and then you have a period at the end, again, where the CO yields are kind of off the charts. And what's really interesting is this looks a bit daunting when you first look at it, because you're like, oh my gosh. Turns out, you know, if you were to pick a CO yield from the back of the SFP handbook, you know, for polyurethane foam, you're sort of at the lower limit of anything that we measured in the whole experiment. Right. So to take a constant yield would really underestimate the total CO generation. And it makes sense when you think about it, because in the beginning, you have tons of pyrolysis gases and not much flaming going on. Right. So if you can imagine this chair, I put the pilot underneath it. In the early phases, the heat release rate is growing slowly, but there's lots of areas which are undergoing pyrolysis and they're giving off gases, but those aren't burning. And what we know from sort of combustion chemistry is these pyrolysis gases have a lot of carbon monoxide, lots of CO in them, right? But once those gases ignite, the flame sheet provides this sort of, you know, this sufficient temperatures for which the CO can be oxidized into CO2, right? So then our CO yield goes down substantially during flaming. But then as the flame sheet starts to break down towards the end of the experiment, your CO yield starts to creep back up. So in terms of modeling applications of trying to understand the, you know, how do I apply a CO yield, if you're looking at real fuel packages, undergo these phases, then understanding the phases are really important in sort of being able to accurately model what is my CO generation. And that was sort of the, while the total heat release was a nice sort of closed bow on top of everything, the CO opened up all sorts of questions that require some further research. I'm not sure why I'm saying total hit-release rate. It's just hit-release rate is so embedded in my head. I simply cannot say hit-release without adding rate at the end. And you know what's driving me crazy? That I know so many people. I would just take your conclusions in here and apply at the same time the maximum burning rate and the maximum CO generation, because that's conservative. Like, you know, ignoring that there's a very physical distinction between version one, where your pyrolysis gazes burn, and version two, where they don't. Having leading to completely two separate pathways of outcome. I know people, a lot of people, who just merge them conservative. Let's do this. Anyway, David, fantastic fire science. It's really exciting how much you can do with simple experiments. People strive for creating the world's largest fire and stuff like that, the most complex settings. And I know more and more people who find the beauty in really understanding the simple mechanisms in fire simple as like a artificial concept because we all know that when you go in depth into fire science the deeper you go the harder it gets so I really appreciate this this kind of science and I hope to talk to you soon again in the Fire Science Show. Thanks for coming. No, thank you so much for having me. This has been a wonderful chat. Thank you again. And that's it. I hope you've enjoyed it. I hope I did not overhype this. For me, it was a really great conversation and a really important episode of the Fire Science Show. You know, I've done my trials on design fire episodes, and I hope they, to some extent, are in line with what David was presenting us in this discussion. David has a lot of firsthand experience in testing those fires and measuring those fires and observing them and figuring out where the discrepancies and ascendance come into those fires. So I'm quite happy that my observations are in line with what he's explaining. Of course, I didn't go that deep into this. Kudos to David for his hard work. David has also asked me to give a shout out to Professor Glenn Torncroft, who is the professor who put him into the resistor lab that he mentioned as a story in the episode. And actually, Professor Torncroft is also a co-author of his PMMA paper. That's really nice that it sparked the interest in measuring uncertainty and went so far that they've written a paper together on the properties of PMMA. Fantastic. I love this. I love this story in a way. Back to the practical takes from this podcast episode gosh there's so many first of all the phases of fire how by slicing the fire into phases you can distinguish them independently and figure out some statistical properties of each of the phases this is a really far-reaching conclusion that we can really put into practice if we get more data on common fires this is the future guys this is how we're gonna do our common fires. This is the future, guys. This is how we're going to do our design fires in the future. Another thing, I did not really capture that and follow in the podcast episode itself. It really struck me when I was editing the podcast that David mentioned the ability to use this knowledge for probabilistic fire definition. And this is a beautiful concept. This could really work out because if you know the phases, if you know the probability of transitioning from phase to phase, you could really go on with very good probabilistic design fires. So I love it. And I'm looking forward for more work from Adam Rowe and David's team. I'm crossing fingers for his PhD. And I am looking forward for all the fire science he's going to put out. Once he is a doctor at Adam Rowe. A lot of possibilities and I'm really keen to work with them on topics like this in the future. So that's it for this episode. I am overly excited. I'm sorry, apologies, but I love fire science. I love this. This is the exact thing that is my passion and yeah, you can hear it. Anyways, next week, another interesting topic in the world of fire science you for listening and see you soon.