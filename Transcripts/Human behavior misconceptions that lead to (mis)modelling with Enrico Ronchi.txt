 Hello and welcome to Firesense Show Session 37. Great to have you here again. My today's guest is Professor Enrico Ronchi from Lund University and you may remember him from Episode 16 where I've tapped into his mind and Professor Ruggiero Lovreglio's mind to understand what the future has for evacuation modeling. the future has for evacuation modeling. I've invited Enrico after a passionate discussion on Twitter about protecting evacuation modeling fields from bad science and how badly some models are used and how it creates a parallel literature scheme. And it's quite funny that just today in the morning, there was a really nice webinar by Human Behavior Group of IFSS, where the terminology used in evacuation science was discussed. And a lot of this is so relatable to my today's discussion. So in a way, the webinar in the morning has shaped the discussion in the afternoon. And that makes me very happy because I think it gave the talk another dimension. And I hope you like how we venture with Enrico through some concepts that are common in evacuation modeling and maybe they should not. And some concepts that are not common and maybe they should be a little more. So yeah, whether if you are an engineer or researcher, I think there's a lot of useful things to take away from this episode. I hope you enjoy. So let's not prolong this and let's jump into the episode. Welcome to the Firesize Show. My name is Wojciech Wigrzy≈Ñski and I will be your host. Hello today, I'm here with Professor Enrico Ronchi from Lund University. Hello Enrico, great to have you back in the show. Hi. Hi, Wojciech. Very nice to be back. Please don't panic. Oy, oy, oy, oy, oy, oy. The P word, the P word. No, no. Yes, it's just 15 seconds into the episode and we've already used the forbidden word. And it's an important part of this episode. Just earlier this morning, I saw a great webinar in Human Behavior in Fire work group of IFSS with Anne Templeton and Milad Haghani, who did discuss the P-word in depth. And you were moderator of that. It was great. Will it be available online? Yes, we have actually a YouTube channel. So we record all our webinars. And yeah, so I will share around the link to subscribe to our YouTube channel. So we record all our webinars. And yeah, so I will share around the link to subscribe to our YouTube channel. That's so cool. So two for a price of one, you end up listening to this episode to go and immediately learn something about how bad we use terminology in evacuation modeling. But it has consequences far beyond the, let's say, cleanliness of the language. It's not just using the terminology because the vocabulary says so. It has a deeper meaning that leads to misconceptions and issues with that. Do you think it's really such an important thing to have workshops on that and really battle? I think in our subfield of evocation modeling and Jungbjorn Windfire, we really flag this as a very big problem because we noticed that there is an even increasing literature in modeling papers that try to model something that has very little link to the real world. And if we look at the studies, for instance, that both Anne and Mila Dagani have been doing into the use of terminology, this seems to have spread more and more following up these seminal papers which make an incorrect use of those words. So, for instance, there is, let's say, which make an incorrect use of those words. So, for instance, there is, let's say, one of the key papers in our field is the paper from Helbing and some of his colleagues published in Nature in the middle 90s, which presents the social force model. Probably that's one of the most common modeling approaches in evocation simulations. And he talks about modeling panic. And the thing is that actually the model is a beautiful model mathematically because it's very simple and it's very flexible in representing people movement, but it is actually not modeling panic. And what happened is that people literally take the words as they are in the paper and they start making modeling efforts, which let's say, derail from the original purpose of that paper, which was actually about modeling people movement. And we see a lot of modeling efforts which don't make too much physical sense. And it's really a pity because as community, we should try to intervene to do something into stopping this waste of energies. I would argue that probably this is not as uncommon as we think, that we have people that maybe come from slightly different domains from ours that approach our domain of fire and evacuation modeling, and basically they are very good at maths probably. They're very good into coding and implementing models, but they probably did not spend enough time reading the literature of the explanation of what they are meant to model. But they just look at the modeling literature. unfortunately, very poor for the fire engineers and for any users of evacuation models, because they mostly attempt to model something that doesn't happen in real life or happen very rarely, or that it is not the assumptions that we use in design. So this disconnection between the real world behavior and the real world design and the modeling world, unfortunately, creates a lot of waste in the research community. So there needs to be efforts to try to avoid this somehow. And, I mean, that was the motivation for one of the motivation for our groups on human fires of the IFSS was, indeed, to try to put ourselves together, hit some hot topics that we thought could be interesting, both within the community, but also outside the community, so that if someone is approaching our field, at least can have a general overview, let's say, on the common mistakes and the common problems that we see when new people approach this field. Okay, let's try and figure out what really the issue is. Okay, let's try and figure out what really the issue is. So in my understanding, you can use the term panic or whatever other way you would like to name it. But you can use this term as a proxy of a very complex human behavior. You say, okay, this group of people or this person has panicked, which means there is a new rationale for their behavior. They will act like this. They will do this. And you also use the term that it's contagious. And I think that was how it was previously described. It can spread from person to person. So suddenly you have a fairly easy way to model behavior of large groups of people. And in reality, you evacuation scientists looking through real cases of fires, looking through data, carrying your experiments, you do not see this simple negative behavior. I had Erika in the podcast and she was mentioning the complexity of modeling behavior. So in one way, putting this in this panic route or this simplistic behavior route gives you the benefit of modeling behavior. And you can claim that you've modeled behavior, but in reality, you did not. You just drawn a proxy, which actually is incorrect. It's like you're building a video game, but it's nothing to do with reality because it's much easier to, let's say, model social influence as a contagion that spreads with contact, with physical contact. Like, okay, like I touch you, you become a red dot, you wear a black dot and so on. But this is not how it works. And that's the problem that we face. We face this situation in which the literature, unfortunately, is kind of split into a part of scientists that attempt to model the real world phenomena. And this, as you said, and as several other colleagues say, like Erika Kuligowski and many others, it is not easy because you can either approach this with a, let's say, more macroscopic approach or microscopic approach, try to look at each individual person. But in any case, it's not simple because you will have to factor in many variables. One of which we know is social influence, which is not based in relying on who we have around us, but also who we are in the simulation. So the actual profiling of each individual person as an agent is also important. And all these models, already there are a few very good models technically for modeling these things, but the problem is how to calibrate them, real-world scenarios, real-world data, knowing that even to get hold of this data is very difficult. And even if we have data of this kind, sometimes they either come from observational studies in which you don't have that level of detail into, let's say, what is the inner decision-making process that people do, or on the other side, the problem is when you have, like, instead stated preference studies in which actually people claim what they will do, then they have problems with validity because then you may people claim what they will do, then they have problems with validity because then you may have people claiming they will do something, but we do not know how often, how well this relates with actually what will happen in the real world. So each of these type of data carry their own uncertainties and their own limitations. So the solution is not just to make everything simple, overly simple, and just say, okay, we do a spread of contagion model and we have solved the problem and we can use this for design. The solution is to try to, first of all, as scientists to understand what are these fundamental variables that affect decision-making, but as modelers, just let's stick to what we know from the real world. So let's go ahead with the knowledge that we have. Let's not try to go one step forward and to try to model things that we don't know, because that's not going to work. That's not going to work. Any model will not be meaningful for fire engineers if it's not validated and if it's not properly calibrated. And so that's what I'm saying. There is, for instance, very interesting work that was done a couple of years ago, also by Erica, Steve Gwynn, Mike Kinsey, a couple of colleagues there, Max Kinneather. They were looking at so-called behavioral statements. So they would say, okay, we have a list of statements we can produce, which in very short sentences, they tell us what we know about human behavior and fire. So what we actually can say with a certain degree of certainty that is going to happen. So one classic example is humans tend to satisfy rather than optimize. So humans will not necessarily pick the exit that will give us the shortest time but they will pick the exit in the building that will make them arrive safely they don't care if they arrive 10 seconds before or 10 seconds later but you see instead this kind of flood of optimization models which in one sense i understand why they are there because okay you want to first know what is the condition the optimal condition in order to strive for that. But I see much more important to put efforts into understanding what brings people to comply to information that are given either by the building itself, so let's say environmental features, or by intervention of building managers, rather than putting a huge effort in studying the exact optimum evacuation strategy in a building. We need something that works. And again, if it takes 10 seconds less or 10 seconds more, to an individual person would not make a difference. So I think there is a wrong allocation of resources in research in these terms. And that probably comes from the fact that it's much easier to build an optimization model rather than doing an experiment. I think in contrast to what you say about this complex modeling of behavior and even root choice, in contrast to that, most of the currently used models would just have some shortest path algorithm to decide which path you would take. Maybe some social force model to push the agents from each other. Maybe some model of attractiveness of exits where you could physically alter how attractive one exit is from another. But from my experience in this practical modeling and I would say I used it mostly for engineering, not for science. So I've never ventured into writing my own models or I've never really went too deep beyond what my contract required me to do. So, you know, I'm an engineer. It's not my job to figure out the best way when the tools are on the market and they are advertised are the industry standard, right? So you have this contrast of what it can be with modeling behavior and what it is mostly, which is route optimization or path optimization and just movement calculations. And there's nothing wrong with movement calculations. It's just it's not modeling of complex behavior of people. It's just it's not modeling of complex behavior of people. And do you think this creates this, let's say, dualism in the world of evacuation science? I think in general terms, the key here is flexibility. This is the key word. So when you use evacuation models, and I will say many commercial model developers have understood that, it's much more important for a user to be able to control the behavior of agents in order to be able to represent different what-if scenarios than to have the most optimized algorithm for route choice, let's say, or for people movement and so on. So I think that's something that many people in the commercial evacuation modeling world have well implemented and understood. The problem is actually with researchers. And I say this with a lot of pain in my heart, but there is a full body of literature that digs a lot into modeling things that are not really needed for engineers, or they are actually starting from wrong assumptions. And that's the real problem. So that's why I appreciate a lot the efforts, for instance, that modelers do when they start with a model, and then they jump immediately into data and try to validate it with data, or all the way around, they have data-driven models in which they start with data, and then they try to build a model. That's a very good starting point for an evacuation model. But I don't see this, I mean, I see this, but I don't see it as often as I would like it to see, because there is still a part of the literature of people that have basically never done an evacuation experiment in their life, and they build models, or even even worse because that's in principle is okay that you are a specialized modeler and not into experiments but that also i've never probably read about evacuation behavior and actual experimental data and that's the problem and when that's where you see this mismatch where that's where you see these publications that unfortunately even get published in scientific journals that have very little link to the real world and unfortunately very little use to fire engineers or in general to any users of evacuation models. like exercises done often maybe, I don't know, graduate students or thesis students that have to publish something because of the pressure to publish. Maybe it's mathematically beautiful. I don't question this. Maybe some people are really good at that, maybe better than many of us. But it's very little use. It dilutes the literature in the field, and it also has very little use for engineers. And again, I'm not arguing that all literature should be linked to the practical world, but you should not mislead at least engineers. You should not lead them into doing design mistakes. And this is unfortunately what I see because this literature is so big of those kind of papers trying to model in panic or contagion or whatever other, let's say, behavior that has been deconstructed and dismounted in the literature in the field of social psychology or similar fields, that it is actually worrying at this point because we are not able to stop this. And they cite each other and they continue doing this. I can make an example. I had a nice chat at some point with an applied mathematician. And he told me something that I always suspect is a crowd modeler. And Emiliano Cristiani is from Rome. And he told me, you know, there are some people that really fall in love with certain mathematical equations. And they will try in our field to squeeze those everywhere. So they say, oh, I can use this also for crowd dynamics. And for those that instead try to link this to the real world and have a use of this, this is painful. It is painful because you see such a waste of resources. So these people could be maybe brilliant into solving certain problems of evacuation modeling, but they put instead so much emphasis on something else that is not needed. modeling, but they put instead so much emphasis on something else that is not needed. So that's what we both were saying. OK, we wish that we will have the right efforts, putting the right problems in our community. I think this dilution is not only typical for evacuation science. I see that a lot in tunneling. For example, there's a group of researchers who will determine the backlayering distance or critical velocity until the fourth significant digit. When, as an engineer, you just double it and then make it safe, you don't really care that much. And as you mentioned, this creates this parallel ecosystem. I mean, when it's just one paper that does that or one researcher who does it, it can be an error, it can be a mistake. It can be just an honest misjudgment of a scientist. But when it forms an ecosystem where this is continuously repeated, repeated, repeated, you start building on a parallel paradigm. You need to build science, you need to have paradigms. on a parallel paradigm. You need to build science, you need to have paradigms. And if in your case, modeling or assuming some certain behaviors, some certain extreme behaviors that do not have proof in real evacuation studies or real fires, if that suddenly becomes a paradigm for a part of your field, this part of field is genuinely lost. And as you mentioned, that is waste of resources, that is waste of human time, that is waste of resources, that is waste of human time, that is waste of work of sometimes brilliant scientists who are really good at what they are doing. For example, applying these mathematical problems to solve something. And we are not a big field. We don't have people to lose. We need all hands on deck solving important problems of our discipline. Do you think maybe sometimes entering this parallel can be just a conscious choice because it may be somewhat easier? I honestly don't think so. of understanding of the literature, which leads to basically sticking to a certain type of terminology and a certain type of assumptions that, let's say, if we are split and we are, let's say, a percentage of people that claim that we should model in panic, a contagion, and so on. And let's say the other part of the scientific community is trying to explain over and over and over in the last 30, 40 years. I mean, this morning when we were discussing it, that the first papers discussing panic misconceptions, you know, they come from the 70s or things like this. So it's still something that we're battling. I think there is a genuinely honest problem with the community that is not able to intervene to spread this misconception. And in our case, this is also very much fed by media and is fed by, you know, everything that you watch outside the scientific world, because that's the problem that we have as well. Maybe in other domain, modeling of fire science, it is not such a big problem. But in our domain, the thing is that when you turn on the TV or open a newspaper online, there is always this tendency to have sensational titles and so on. So they want to use this kind of extreme wording and depict this extreme behavior, even if they didn't happen, because their goal is often to sell newspapers rather than actually telling what happened. But as I said, I don't think there is an hidden agenda in a way. It's more like a genuine mistake made by a portion of the community that either comes from another world. I mentioned the world of applied mathematics, but it can be applied physics or anything that is not linked to the core literature in the field of psychology, social psychology and so on, that explained us how this mechanism of human-being fire works. So this is what I think is the problem nowadays. And to be honest with you, I'm even more worried now with the new trends of data science and machine learning and so on, because these are fantastic tools, but they often start from another premise, that we don't want to understand the fundamental mechanisms, but we want to identify patterns in what happens. And often this leads to the blind trust in the data, which is the biggest problem that we have in this case, and for which I think we should be very careful about, because there is a huge risk that we will end up now with a whole bunch of new literature in the domain of evacuation using this kind of data science approaches with machine learning and so on, which may really start from data that have problems, and instead they take them as good by default and come up with wrong conclusions. And as I say, don't get me wrong, because I'm a big enthusiast myself of this kind of data science approach and machine learning. But I just have to put down a warning there for our field specifically because there is this risk to overtrust data and start getting models that bring down conclusions that do not make too much sense just because we are not putting efforts in understanding the fundamentals of what is happening but just trying to identify patterns into what happens. So in my field using machine learning or artificial and any type of big data tool actually is let's say straightforward because i'm working with fundamental physics the conservation of energy and mass is always the same the way how i know pressure or density changes with temperature i can describe this with an equation even if i have very very complex relation between a group of variables and my outcome. It is in the physical law. And here by data, I assume you mean the common tools for evacuation modelings, which would be surveys, which would be data collected from evacuation drills and real incidents. and real incidents, it would be mathematical data describing the movement of people, but mathematical data cannot describe the behavior of people or their choices, or at least not in such a straightforward way as I can describe the motion of smoke or heat transfer through a partition in a building. So you are really, like, your data, by assumption assumption has this level of unknown. There is that you first need to understand the variability in your data set. And you first need to understand what your data represents before you drop it into a black box seeking for hidden relations. Is that what you meant? I think this is exactly what I mean. And sometimes I make this example when I talk about this. I don't know how many of you are chess players, but you probably have heard the story of AlphaZero, this super software that has been built learning from millions of chess games and improving the game and actually having one of the main grandmasters, number one in the world, Magnus Carlsen, learning from the software that use this kind of algorithm, caring just about the final outcome, not about the chess theories on everything that we know from the theory. Well, in that context, this works really well because the assumptions to start with, so the movement of the pieces are what they are. So there is no, let's say, a bishop that moves on a straight line. They move diagonally. That's it. So instead, what we see in our field is that there are a lot of aspects and assumptions that are really, really questionable sometimes. And they are really objects of debate into how and why certain behavior occur. And also they are depending on the local conditions that you have. Because even if you have a very large data set, I mean, I spoke with very brilliant researchers that work with massive data sets of pedestrian movement. Like, for instance, I think the group that has the largest collection of data sets on this is at the University of Eindhoven with Alessandro Corbett and Federico Toschi. These guys, they put these sensors in train stations. They collect millions of trajectories. So it's really fascinating what they do. But imagine, even with this kind of huge dataset, this will still be a very specific population in a very specific context, in a very specific environment. So to generalize findings, it needs another step. It needs, like, I see those two things as complementary. I'm really happy that these efforts are happening, and I think they're really good, but we cannot just rely on those models that try to identify patterns in this big data. We also need to try to dig more into understanding what are the fundamental processes and theories that can explain what we see. That's why I hope that many of the people that work with these methods probably would agree with me. I don't think that those data science methods should substitute what we've been doing so far. They should complement it. But we cannot have this as substituting our current theoretical methods, especially when it comes to those type of data for which we have a lot of uncertainty to begin with. And as I say, we may argue for hours what do we use deterministic models a lot in fire modeling, much more than in evacuation models. And, you know, there you have a deterministic model mostly because you have a physical phenomenon. But even in that physical phenomenon, there are so many things that can impact it, so many boundary conditions and all sorts of other variables that we cannot fully control. Boundary conditions and all sorts of other variables that we cannot fully control. In our subfield of evaluation, this is evident to me that we are not yet at the stage of science understanding in which we can completely control and fully understand what leads to certain behavior. So we need to be really cautious when we use data science approaches because they often take the data as they are. They don't question them. So the next step, and I think there are groups that are trying to do this, and also to try to understand better how to treat this kind of data in order to understand. Because any sensor, any device, anything can have a fault, or there can be an inner problem with the data. So that's why I think we should be very cautious with this. And maybe I don't want to sound over negative, but I think these are very good tools, but they should be seen as complementary, not as a substitution to everything that we've been doing so far regarding fundamental theories. I have a challenging thought for you. If I look back on the years of development of the tools we use to predict evacuation processes in buildings, whether you go back to like 70s, 80s, you use graphs to solve it on a piece of paper, or you use the first spreadsheets to calculate it in a computer, or you start going into computer models which were just glorified spreadsheets, or you go into models that go with movement trajectories and calculate past social forces and so on, or even you jump into machine learning, you still have the exact same constraint related to your boundary of the human decision process. Like in every single of these tools, you are constrained by the exact same limitation of not knowing the principles behind human behavior. So maybe if that's the case from the simplest tool to the most advanced or modern tool you have, maybe that's a place where you should push your head of evacuation scientists to solve. Maybe we need a standardized model of human behavior that could be just applied in research instead of trying to figure it out from scratch. What would be this, if you had the power to just solve it instantly, which path would you go and what would you take to deliver that to the community? I mean, there have been efforts in this direction, at least to understand some of the key variables and key mechanisms of human behavior in fire. And as I said, I think one exercise that all modelers should do is to take, indeed, what I mentioned before, these behavioral statements, which are a list of statements that describe what happens with humans during a fire emergency. What do they do? And try to violate their assumptions against those so to see if their assumptions hold against this. And now as a researcher and also fortunately as mentors of researchers, what I tell them, every time that you write a research paper in which you're investigating a specific population or a specific scenario and so on, try to create this behavioral statement. or a specific scenario and so on, try to create this behavioral statement. So try to create a simple statement that concisely describes what actually happened in that given situation so that a modeler can take this statement and use it as a benchmark against the assumptions that are used in the model. Because as I said, if we do this exercise, if we take this knowledge that we have nowadays and we compare it with models, we will definitely find some models that violate through their assumptions some of these statements. And fortunately, as I said, the trends that I see, especially in the commercial world, and it's a bit strange because I would have expected more the researchers to keep up with this, but actually I see it much more in the commercial world. The model developers in the vacation world are trying to actually give flexibility to their models so that at least you can make sure that you can calibrate the model in a way that doesn't violate certain assumptions that you have. But as I said, in the research world, unfortunately, it happens often that people want to reinvent the wheel from scratch and that's not probably the right approach i mean if i would have a magic stick and decide what to do with the research funding in the vacation world in the world i would basically put 90 of my money on data collection and theory and let the modeling be done by the actual model developers in companies. They probably are much better writing codes than all of us. This is at least how I would do it. So these rules could be or can be applied as some sort of sanity check if your modeling makes sense, right? Yeah, that's the idea. So, I mean, to try to benchmark the assumptions that you use against those simple rules. And, you know, there have been simple rules defined in different domains of the evacuation war. There have been these behavioral statements that come from our own community for fire safety science. But, for instance, there are a lot of efforts done in the pedestrian evacuation dynamics community, for instance, identifying rules of interactions between people in a crowd. So at a more microscopic level, see how does a lane of people get formed. There is a brilliant research in this or how actually queuing happens. So trying to extract those rules from the individual papers, boil them down in simple statements, and then benchmark the models that we have out there against those and see, does this get violated to begin with? Is this inbuilt in the defaults of the models? Can we modify the defaults so that at least we can tune the model in a way that these assumptions are not violated anymore? So this is the kind of benchmarking work that I would like to see more and more. And I mean, it's all linked to the efforts that have been done over the years in verifying and validating evacuation models. You know, this is one of my main life mission as a researcher to develop standardized testing for VMV of evacuation models. But as I said, we need to have another step of the community trying to make their findings simple enough to be directly implemented into models. I have one more challenging one. So I think in most of these approaches, we focus very much on behavior of a single agent or a single person that evacuates. agent or a single person that evacuates. But do we truly need this microscopic approach to simulate the population, you know, to see large crowd patterns, to see queuing? Is this the only way? Because we started this with the issues with papers and this is part of the science that goes in a slightly wrong way because they misunderstand some of the assumptions and they do things that put a lot of effort in researching things that make no sense, basically. So maybe if we could break the paradigm that you model an agent, maybe there are ways to skip that individual behavior and just focus on group behavior or something bigger. How do you do it in wildland interface fire do you still stick to agent or i i think this is a another very good point of discussion i had several discussion about this with different researchers that work in different domain both in the wii fire world and in the crowd evacuation world i think there is not a type of modeling approach that works for everything. I think it really depends on the problem that you need to solve. Because in some cases, it is true that you could actually use a very simple, let's say, flow calculation or a very simple macroscopic model and get a fairly decent approximation of the result that you need. In some other contexts, it's actually probably not enough, especially when the impact of an individual behavior or a small group of individuals can be huge on your total evacuation time. Then you cannot just look at the big picture. You may get this diluted and maybe, let's say, overlook certain aspects. So this discussion came up a lot, for instance, when it comes to woe fires fires because there is even a lot of people arguing that we don't even have to have pedestrian evacuation modeling within whey fire modeling because the scale of the traffic when you have scenarios with traffic evacuation is much bigger. But again, I will not answer like yes we need or no we don't. The answer is, it really depends on which scenario you're looking at and what conditions you're looking at, both for buildings and wafers. We should be able to have in the available a pool of tools and then select the most appropriate one, given the problems that we need to solve and the design issues that we have. So I would not argue, I would not recommend to just abandon microscopic modeling. That's definitely not a good idea, personally, I think. Or nor abandon macroscopic modeling because that is also useful in many instances. But more like choosing what is the right path that we need to take depending on what applications we have and have those two tracks developing in parallel so maybe microscopic models can learn something from microscopic models and vice versa because you know there have been also effort trying to combine those and using hybrid approaches and so on but that's not trivial you would have to decide when a person changes from a person into a population where it's this this sharp edge when if you would agree that at some point at the population scale, these individual decisions have less and less impact because essentially they become statistics. If you care about the movement of one million people, the decision of a single person ends up being a statistic. That's it. So finding this edge between them and applying correct approaches to each may be an important aspect of modeling overall. And I think in the same way, it's when you go from modeling agent movement into agent behavior. literally stop doing something very simple and jump into a field where more complicated things dictate the outcome and like honestly from your experience do you think in i don't know next five ten years we will have reliable behavioral models to use in our modeling like i go today i i start up computer software of my choice to model my building i pop in pre-evacuation distribution times and i run the simulations will i be able in like five to ten years be able to just put defaults human behavior model and and just run the same simulations without this presumption i think in five years no probably i don't want to be over optimistic because that is something that we have been looking around already for quite some time, and we are digging into this. But I would argue that the tools that we have today, and as mentioned, several of them, they're becoming more and more flexible in a way that if we have, as users, a solid understanding of what are the scenarios and the behaviors that we want to represent, we actually have the design tools to do our design nowadays. So what is missing here is more research towards understanding of everything that has to do with those outlier scenarios that we have much less understanding and control on because we know that in certain situations there might be a small group of people doing a certain behavior that might dramatically affect our results and you know when we design for performance by designing fire engineering we actually look at our set so we look at the actual tail of our vacation time curves. And that's where I would like to have more research, because now we can do it, but we have a quite large uncertainty in this. So what I hope and I really think that we could do in 10 years from now is to basically work more on making a smaller gap between what is the result that we expect and the uncertainty that we have. So to basically have an R set, because, you know, when we work with R set, A set, sometimes we actually, often we recommend to have these probabilistic approaches. So to make sure that in 10 years from now, this uncertainty is reduced. So our span of possible results become smaller, which means that we have more confidence in our R set when we do our design. So our span of possible results becomes smaller, which means that we have more confidence in our R set when we do our design. So this is what I see happening in 10 years from now. I don't think we're going to be able to pin down to one R set for each very specific scenario, because also that's not really how we work and how humans are. It's very, there is so many variables that can play a role that it's very hard to isolate them all, especially in a timeframe of five or 10 years from now. But we can improve our understanding of this uncertainty and decrease that uncertainty so that we actually have a better understanding of what this R set, this required safe egress time is. I just cannot stop myself from asking you for predictions for the future. And for the new listeners of the podcast, there's like a whole episode with two Italians rambling on how the future of evacuation modeling will look like. And I highly recommend that episode. It was a great discussion. And I would also love to recommend the episode with Erika Kligoski, who explains how she's modeling behavior in wildland fires. And that is truly beautiful on how these aspects of human behavior are being modeled and what it takes to actually model them. Maybe not in buildings, outside of buildings, but still, it's a similar problem. And I absolutely love how she presented it. And it really shows the complexity of the problem. And I think through resources like this, through research like that, we truly bring more people closer to understanding what is the big problem in here? What is the big picture in here? And to understand it, they have to move back to fundamentals. And by moving back to fundamentals, they move away from doing this misjudged conceptions that you can simplify a human behavior into a phenomenon of panic or whatever else. And I guess that could be like a good summary on where should we go, educate people on the complexity and what it really means to model behavior, to stop this over of bad science that in the end influences our discipline. Correct. And I think this is a good take home message. So if you are a user of models or even if you are a researcher, go back and start questioning the assumptions both that the model use and that you are using, the assumptions both that the model use and that you are using, because this is one of the key aspects that can lead to mistakes and problems in your simulation world. So I think this should be the take-home message, especially when it comes to representation of such a complex problem like human behavior in fire. Fantastic. That's a great summary. I'll link all the resources into show notes. Fantastic. That's a great summary. I'll link all the resources into show notes. And I hope we've helped some scientists to get on the correct path and others to help us fight or battle the flood of bad science. Thank you very much, Enrico, for coming to the show for the second time. It was a huge pleasure to have you here. And I have a feeling we will see each other not so far from now again, maybe touching on the R-Set concert. Oh, man, that would be fun. Thanks, Wojciech. It was very nice to be back. And thanks a lot for all the efforts and time you put into this podcast because it's really fantastic. We really needed it. Oh, thank you. I'm so happy. I didn't know we needed it until I started it, and now I just can't stop. It's a really cool experience. Thank you, Enrico. See you around. And that's it. I hope you've enjoyed this episode. And if there's one takeaway that you can take from it, I guess the sanity check approach for testing your evacuation model would be the thing to take. approach for testing your evacuation model would be the thing to take. Enrico mentioned the paper, and I found it. The paper is titled Guidance for the Model Developer on Representing Human Behavior in Aggressive Models, and it was published in the Fire Technology Journal. It's authored by Gwyn, Halsey, and Kinsey, and I'll link it in the episode description, obviously. Enrico mentioned the paper contains statements, and in fact it does, statements that are, let's say, the things that we know about the behavior of people when they evacuate. And this can be used to cross-check your analysis, whether you break these statements or not. And if you do, well, then most likely there is some error or misconception or the model is used in an incorrect way. And as we don't have yet the perfect models, as we don't have yet the ability to just easily model the evacuation behavior of people, actually cross-checking our simulations with such a sanity checker sounds like a pretty powerful tool that can be used to make sure that the modeling is of high quality. And I would recommend it to engineers and I would highly recommend it to scientists who research in this field. And as mentioned in the start of the episode, I would also highly recommend listening to the IFSS webinar. There's a channel on YouTube with these webinars. This one was the third one, and they're all great. So I hope this is another resource that is interesting to you, and I hope you reach out for that. Thank you for being here with me. Thank you for another week of podcasting. I look forward to the next week so see you again next Wednesday thank you bye this was the fire science show thank you for listening and see you soon